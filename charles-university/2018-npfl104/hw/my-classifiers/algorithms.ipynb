{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from helper import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_dict(data):\n",
    "    classes = {}\n",
    "    for row in data:\n",
    "        if (row[-1] not in classes):\n",
    "            classes[row[-1]] = []\n",
    "        classes[row[-1]].append(row)\n",
    "    return classes\n",
    "\n",
    "def mean_std(data):\n",
    "    mstd = [(np.mean(col), np.std(col)) for col in list(zip(*data))[:-1]]\n",
    "    return [(mean, std) if std != 0 else (0.0,1.0) for mean,std in mstd]\n",
    "\n",
    "def mean_std_classes(data):\n",
    "    classes = class_dict(data)\n",
    "    mstd = {}\n",
    "    for c in classes:\n",
    "        mstd[c] = mean_std(classes[c])\n",
    "    return mstd\n",
    "\n",
    "def prob(x, mean, std):\n",
    "    if std == 0.0: return 1e-6\n",
    "    return (1/(math.sqrt(2*math.pi)*std))*math.exp(-(math.pow(x-mean,2)/(2*math.pow(std,2))))\n",
    "\n",
    "def prior(train):\n",
    "    p = {}\n",
    "    for c in set(train[-1]):\n",
    "        p[c] = len([x for x in train[:,-1] if x == c]) / len(train[:,-1])\n",
    "    return p\n",
    "\n",
    "def prob_classes(mstd, priors, row):\n",
    "    p = {}\n",
    "    for c in mstd:\n",
    "        p[c] = priors[c] *np.multiply.reduce([\n",
    "            prob(x, mean, std)\n",
    "            for (mean, std), x in zip(mstd[c], row)])\n",
    "    return p\n",
    "\n",
    "def predict(mstd, priors, row):\n",
    "    probs = prob_classes(mstd, priors, row)\n",
    "    best = None, -1\n",
    "    for c in probs:\n",
    "        if best[0] is None or probs[c] > best[1]:\n",
    "            best = c, probs[c]\n",
    "    return best[0]\n",
    "\n",
    "def accuracy(train, test):\n",
    "    dist = mean_std_classes(train)\n",
    "    priors = prior(train)\n",
    "    predicted = [predict(dist, priors, row) for row in test]\n",
    "    actual = [row[-1] for row in test]\n",
    "    return sum(1 for p,a in zip(predicted, actual) if p == a) / len(test) * 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95.39999999999999\n",
      "81.04539033228917\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(train['artificial'], test['artificial']))\n",
    "print(accuracy(train['income'], test['income']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from helper import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(row, weights):\n",
    "    weighted_sum = weights[0] + np.dot(weights[1:], row[:-1])\n",
    "    return 1 if weighted_sum >= 0 else 0\n",
    "\n",
    "def train_weights(train, learn_rate, epochs):\n",
    "    weights = np.zeros_like(train[0])\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for row in train:\n",
    "            error = row[-1] - predict(row, weights)\n",
    "            weights[0] += learn_rate * error\n",
    "            for i in range(len(row)-1):\n",
    "                weights[i + 1] += learn_rate * error * row[i]\n",
    "\n",
    "    return weights\n",
    "\n",
    "def accuracy(data, weights):\n",
    "    predicted = [predict(row, weights) for row in data]\n",
    "    actual = [row[-1] for row in data]\n",
    "    return sum(1 for p,a in zip(predicted, actual) if p == a) / len(data) * 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0\n",
      "78.28757447331245\n"
     ]
    }
   ],
   "source": [
    "weights = train_weights(train['artificial'], 0.1, 5)\n",
    "print(accuracy(test['artificial'], weights))\n",
    "weights = train_weights(train['income'], 0.1, 5)\n",
    "print(accuracy(test['income'], weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import Counter\n",
    "from helper import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(x, y, length):\n",
    "    return np.add.reduce(np.abs(x[:length] - y[:length]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neighbors(train, test, k):\n",
    "    sample = train[np.random.choice(len(train), 500)]\n",
    "    distances = np.array(sorted([(x, dist(test, x, len(test))) for x in sample], key=lambda x: x[1]))[:k, 0]\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(nb):\n",
    "    pred = Counter([n[-1] for n in nb]).most_common()[0][0]\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(train, test, k):\n",
    "    nbs = np.array([neighbors(train, row, k) for row in test])\n",
    "    predicted = [prediction(nb) for nb in nbs]\n",
    "    actual = [row[-1] for row in test]\n",
    "    return sum(1 for p,a in zip(predicted, actual) if p == a) / len(test) * 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0\n",
      "76.3773723972729\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(train['artificial'], test['artificial'], 5))\n",
    "print(accuracy(train['income'], test['income'], 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_split(index, value, dataset):\n",
    "    left, right = [], []\n",
    "    for row in dataset:\n",
    "        if row[index] < value: left.append(row)\n",
    "        else: right.append(row)\n",
    "    return left, right\n",
    " \n",
    "def gini_index(groups, classes):\n",
    "    n = sum(len(group) for group in groups)\n",
    "    gini = 0\n",
    "    for group in groups:\n",
    "        if len(group) == 0:\n",
    "            continue\n",
    "        score = 0\n",
    "        for c in classes:\n",
    "            p = [row[-1] for row in group].count(c) / len(group)\n",
    "            score += p ** 2\n",
    "        gini += (1 - score) * len(group) / n\n",
    "    return gini\n",
    "\n",
    "def get_split(dataset):\n",
    "    class_values = list(set(row[-1] for row in dataset))\n",
    "    best_index, best_value, best_score, best_groups = 1e10, 1e10, 1e10, None\n",
    "    for index in range(len(dataset[0])-1):\n",
    "        for row in dataset:\n",
    "            groups = test_split(index, row[index], dataset)\n",
    "            gini = gini_index(groups, class_values)\n",
    "            if gini < best_score:\n",
    "                best_index, best_value, best_score, best_groups = index, row[index], gini, groups\n",
    "    return {'index': best_index, 'value': best_value, 'groups': best_groups}\n",
    "\n",
    "def terminal(group):\n",
    "    outcomes = [row[-1] for row in group]\n",
    "    return max(set(outcomes), key=outcomes.count)\n",
    "\n",
    "def split_node(node, max_depth, min_size, depth):\n",
    "    left, right = node['groups']\n",
    "    del node['groups']\n",
    "    if not left or not right:\n",
    "        node['left'] = node['right'] = terminal(left + right)\n",
    "        return\n",
    "    if depth >= max_depth:\n",
    "        node['left'], node['right'] = terminal(left), terminal(right)\n",
    "        return\n",
    "    if len(left) <= min_size:\n",
    "        node['left'] = terminal(left)\n",
    "    else:\n",
    "        node['left'] = get_split(left)\n",
    "        split_node(node['left'], max_depth, min_size, depth+1)\n",
    "    if len(right) <= min_size:\n",
    "        node['right'] = terminal(right)\n",
    "    else:\n",
    "        node['right'] = get_split(right)\n",
    "        split_node(node['right'], max_depth, min_size, depth+1)\n",
    "\n",
    "def predict(node, row):\n",
    "    if row[node['index']] < node['value']:\n",
    "        return predict(node['left'], row) if isinstance(node['left'], dict) else node['left']\n",
    "    else:\n",
    "        return predict(node['right'], row) if isinstance(node['right'], dict) else node['right']\n",
    "\n",
    "def accuracy(train, test):\n",
    "    tree = get_split(train)\n",
    "    split_node(tree, 3, 5, 1)\n",
    "    predicted = [predict(tree, row) for row in test]\n",
    "    actual = [row[-1] for row in test]\n",
    "    return sum(1 for p,a in zip(predicted, actual) if p == a) / len(test) * 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0\n",
      "83.56980529451508\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(train['artificial'], test['artificial']))\n",
    "print(accuracy(train['income'][:500], test['income']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataset):\n",
    "    model.fit(train[dataset][:,:-1], train[dataset][:,-1])\n",
    "    target_pred = model.predict(test[dataset][:,:-1])\n",
    "    return accuracy_score(test[dataset][:,-1], target_pred, normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(GaussianNB(), 'artificial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(DecisionTreeClassifier(), 'artificial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyper/Documents/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(Perceptron(), 'artificial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(KNeighborsClassifier(), 'artificial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7957127940544193"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(GaussianNB(), 'income')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8098396904367053"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(DecisionTreeClassifier(), 'income')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyper/Documents/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7833056937534549"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(Perceptron(), 'income')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7769793010257355"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(KNeighborsClassifier(), 'income')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
