# Mar 05

[Slides](https://ufal.mff.cuni.cz/~straka/courses/npfl114/1718/slides/?02),
[Recording](https://slideslive.com/38906313/deep-learning-lecture-2-training-neural-networks)

- Capacity, overfitting, underfitting, regularization [Section 5.2 of DLB]
- Hyperparameters and validation sets [Section 5.3 of DLB]
- Maximum Likelihood Estimation [Section 5.5 of DLB]
- Neural network training (this topic is treated in detail withing the [lecture NAIL002](https://is.cuni.cz/studium/eng/predmety/index.php?do=predmet&kod=NAIL002))
  - Gradient Descent and Stochastic Gradient Descent [Sections 4.3 and 5.9 of DLB]
  - Backpropagation algorithm [Section 6.5 to 6.5.3 of DLB, especially Algorithms 6.2 and 6.3; *note that Algorithms 6.5 and 6.6 are used in practice*]
  - SGD algorithm [Section 8.3.1 and Algorithm 8.1 of DLB]
  - SGD with Momentum algorithm [Section 8.3.2 and Algorithm 8.2 of DLB]
  - SGD with Nestorov Momentum algorithm [Section 8.3.3 and Algorithm 8.3 of DLB]
  - Optimization algorithms with adaptive gradients
    - AdaGrad algorithm [Section 8.5.1 and Algorithm 8.4 of DLB]
    - RMSProp algorithm [Section 8.5.2 and Algorithm 8.5 of DLB]
    - Adam algorithm [Section 8.5.3 and Algorithm 8.7 of DLB]
