# Apr 23

[Slides](https://ufal.mff.cuni.cz/~straka/courses/npfl114/1718/slides/?09),
[Recording](https://slideslive.com/38907422/deep-learning-lecture-9-recurrent-neural-networks-iii-machine-translation)

- Highway Networks [[Rupesh Kumar Srivastava, Klaus Greff, JÃ¼rgen Schmidhuber: **Training Very Deep Networks**](https://arxiv.org/abs/1507.06228)]
- Variational Dropout [[Yarin Gal, Zoubin Ghahramani: **A Theoretically Grounded Application of Dropout in Recurrent Neural Networks**](https://arxiv.org/abs/1512.05287)]
- Layer Normalization [[Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton: **Layer Normalization**](https://arxiv.org/abs/1607.06450)]
- Neural Machine Translation using Encoder-Decoder or Sequence-to-Sequence architecture [[Ilya Sutskever, Oriol Vinyals, Quoc V. Le: **Sequence to Sequence Learning with Neural Networks**](https://arxiv.org/abs/1409.3215) and [Kyunghyun Cho et al.: **Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation**](https://arxiv.org/abs/1406.1078)]
- Using Attention mechanism in Neural Machine Translation [[Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio: **Neural Machine Translation by Jointly Learning to Align and Translate**](https://arxiv.org/abs/1409.0473)]
- Translating Subword Units [[Rico Sennrich, Barry Haddow, Alexandra Birch: **Neural Machine Translation of Rare Words with Subword Units**](https://arxiv.org/abs/1508.07909)]
- _Google NMT [[Yonghui Wu et al.: Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation](https://arxiv.org/abs/1609.08144)]_
- _Translating without RNNs with attention only [[Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin: **Attention Is All You Need**}](https://arxiv.org/abs/1706.03762)]_
