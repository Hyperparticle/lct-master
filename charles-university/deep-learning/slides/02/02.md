class: title
## NPFL114, Lecture 02

# Training Neural Networks

![:pdf 26%,,padding-right:64px](../res/mff.pdf)
![:image 33%,,padding-left:64px](../res/ufal.svg)

.author[
Milan Straka
]

---
# Machine Learning Basics

We usually have a **training set**, which is assumed to consist of examples
generated independently from a **data generating distribution**.

The goal of optimization is to match the training set as well as possible.

--

However, the main goal of machine learning is to perform well on _previously
unseen_ data, so called **generalization error** or **test error**. We typically
estimate the generalization error using a **test set** of examples independent
of the training set.

---
# Machine Learning Basics

Challenges in machine learning:
- _underfitting_

- _overfitting_

--

![:pdf 100%](underfitting_overfitting.pdf)

---
# Machine Learning Basics

We control whether a model underfits or overfits by modifying its _capacity_.
(representational capacity, effective capacity).

--

![:pdf 90%,center](generalization_error.pdf)

--

The **No free lunch theorem** (Wolpert, 1996) states that averaging over
_all possible_ data distributions, every classification algorithm achieves
the same _overall_ error when processing unseen examples. In a sense, no machine
learning algorithm is _universally_ better than any other.

---
# Machine Learning Basics

Any change in a machine learning algorithm that is designed to _reduce
generalization error_ but not necessarily its training error is called
**regularization**.

--

$L\_2$ regularization (also called weighted decay) penalizes models
with large weights (i.e., penalty of $||→θ||^2$).

![:pdf 90%,center](regularization.pdf)

---
# Machine Learning Basics

_Hyperparameters_ are not adapted by learning algorithm itself.

Usually a **validation set** or **development set** is used to
estimate the&nbsp;generalization error, allowing to update hyperparameters accordingly.

---
# Loss Function

A model is usually trained in order to minimize the _loss_ on the training data.

--

Assuming that a model computes $f(→x;→θ)$ using parameters $→θ$,
the _mean square error_ is computed as $$∑\_i \left(f(→x^{(i)}; →θ) - y^{(i)}\right)^2.$$

--

A common principle used to design loss functions is the _maximum likelihood
principle_.

---
# Maximum Likelihood Estimation

Let $\mathbb X = \\{→x^{(1)}, →x^{(2)}, \ldots, →x^{(m)}\\}$ be training data drawn
independently from the data-generating distribution $p\_\textrm{data}$. We denote
the empirical data distribution as $\hat p\_\textrm{data}$.

Let $p\_\textrm{model}(→x; →θ)$ be a family of distributions. The
*maximum likelihood estimation* of parameters $→θ$ is:

$$\begin{aligned}
→θ\_\mathrm{ML} &= \argmax\_→θ p\_\textrm{model}(\mathbb X; →θ) \\\
                &= \argmax\_→θ ∏\nolimits\_{i=1}^m p\_\textrm{model}(→x^{(i)}; →θ) \\\
                &= \argmin\_→θ ∑\nolimits\_{i=1}^m -\log p\_\textrm{model}(→x^{(i)}; →θ) \\\
                &= \argmin\_→θ \mathbb E\_{⁇→x ∼ \hat p\_\textrm{data}} [-\log p\_\textrm{model}(→x; →θ)] \\\
                &= \argmin\_→θ H(\hat p\_\textrm{data}, p\_\textrm{model}(→x; →θ)) \\\
                &= \argmin\_→θ D\_\textrm{KL}(\hat p\_\textrm{data}||p\_\textrm{model}(→x; →θ)) \color{gray} + H(\hat p\_\textrm{data})
\end{aligned}$$

---
# Maximum Likelihood Estimation

Easily generalized to situations where our goal is predict $y$ given $→x$.
$$\begin{aligned}
→θ\_\mathrm{ML} &= \argmax\_→θ p\_\textrm{model}(\mathbb Y | \mathbb X; →θ) \\\
                &= \argmax\_→θ ∏\nolimits\_{i=1}^m p\_\textrm{model}(y^{(i)} | →x^{(i)}; →θ) \\\
                &= \argmin\_→θ ∑\nolimits\_{i=1}^m -\log p\_\textrm{model}(y^{(i)} | →x^{(i)}; →θ) \\\
\end{aligned}$$

The resulting _loss function_ is called _negative log likelihood_, or
_cross-entropy_ or _Kullback-Leibler divegence_.

---
# Mean Square Error as MLE

Assume our goal is to perform a regression, i.e., to predict $p(y | →x)$ for $y ∈ ℝ$.

Let $\hat y(→x; →θ)$ give the prediction of mean of $y$.

We define $p(y | →x)$ as $\N(y; \hat y(→x; →θ), σ^2)$ for a given fixed $σ$.
Then:
$$\begin{aligned}
\argmax\_→θ p(y | →x; →θ) =& \argmin\_→θ ∑\_{i=1}^m -\log p(y^{(i)} | →x^{(i)} ; →θ) \\\
                          =& \argmin\_→θ ∑\_{i=1}^m -\log \sqrt{\frac{1}{2πσ^2}}
                            e ^ {\normalsize -\frac{(y^{(i)} - \hat y(→x^{(i)}; →θ))^2}{2σ^2}} \\\
                          =& -m \log σ - \frac{m}{2} \log 2π \\\
                           & - \argmin\_→θ ∑\_{i=1}^m -\frac{(y^{(i)} - \hat y(→x^{(i)}; →θ))^2}{2σ^2} \\\
                          =& \argmin\_→θ ∑\_{i=1}^m -\frac{||y^{(i)} - \hat y(→x^{(i)}; →θ)||^2}{2σ^2}
\end{aligned}$$

---
# Gradient Descent

Let a model compute $f(→x;→θ)$ using parameters $→θ$. In order to compute $J(→θ)
= \argmin\_→θ 𝔼\_{(→x, y)∼\hat p\_\textrm{data}} L(f(→x; →θ), y)$, we may use _gradient descent_:
$$→θ ← →θ - α∇\_→θJ(→θ)$$


## Gradient Descent

We use all training data to compute $J(→θ)$.

## Online (or Stochastic) Gradient Descent

We estimate the expectation in $J(→θ)$ using a single randomly sampled example
from the training data. Such an estimate is unbiased, but very noisy.

## Minibatch SGD

The minibatch SGD is a trade-off between gradient descent and SGD – the
expectation in $J(→θ)$ is estimated using $m$ random independent examples from
the training data.

---
# Gradient Descent

![:pdf 100%](gradient_descent.pdf)

---
class: middle, center, full
# Gradient Descent

![:image 100%](nn_loss.jpg)

---
# Backpropagation

Assume we want to compute partial derivatives of a given loss function $J$ and
let $\frac{∂J}{∂z}$ be known.

![:pdf 45%,center](chain_rule.pdf)

$$\begin{gathered}
\frac{∂J}{∂y\_i} = \frac{∂J}{∂z} \frac{∂z}{∂y\_i} = \frac{∂J}{∂z} \frac{∂g(→y)}{∂y\_i} \\\
\frac{∂J}{∂→x\_i} = \frac{∂J}{∂z} \frac{∂z}{∂y\_i} \frac{∂y\_i}{∂→x\_i} = \frac{∂J}{∂z} \frac{∂g(→y)}{∂y\_i} \frac{∂f(→x\_i)}{∂→x\_i}
\end{gathered}$$

---
class: middle, center
# Backpropagation Example

![:pdf 80%](net.pdf)

---
class: middle, center
# Backpropagation Example

![:pdf 80%](net-forward.pdf)

---
class: middle, center, full
# Backpropagation Example

![:pdf 98%](net-backward.pdf)

This is meant to be frightening – you do **not** do this manually when training.

---
class: middle
# Backpropagation Algorithm

#### Forward Propagation
.algorithm[
**Inputs**: Network with nodes $u^{(1)}, u^{(2)}, \ldots, u^{(n)}$ numbered in
topological order, each node being computed as $u^{(i)} = f^{(i)}(\mathbb A^{(i)})$
for $\mathbb A^{(i)}$ composed of values of the predecessors $P(u^{(i)})$ of
$u^{(i)}$. <br>
**Outputs**: Value of $u^{(n)}$.
- For $i = 1, \ldots, n$:
    - $\mathbb A^{(i)} ← \lbrace u^{(j)} | j ∈ P(u^{(i)})\rbrace$
    - $u^{(i)} ← f^{(i)}(\mathbb A^{(i)})$
- Return $u^{(n)}$
]

---
class: middle
# Backpropagation Algorithm

#### Simple Variant of Backpropagation
.algorithm[
**Inputs**: The network as in the Forward propagation algorithm.<br>
**Outputs**: Partial derivatives $g^{(i)} = \frac{∂u^{(n)}}{∂u^{(i)}}$ of $u^{(n)}$ with respect to all $u^{(i)}$.
- Run forward propagation to compute all $u^{(i)}$
- $g^{(n)} = 1$
- For $i = n-1, \ldots, 1$:
    - $g^{(i)} ← ∑\_{j:i∈P(u^{(j)})} g^{(j)} \frac{∂u^{(j)}}{∂u^{(i)}}$
- Return $→g$
]

In practice, we do not usually represent networks as collections of scalar
nodes; instead we represent them as collections of tensor functions – most
usually functions $f: ℝ^n → ℝ^m$. Then $\frac{∂f(→x)}{∂→x}$ is a Jacobian.
However, the backpropagation algorithm is analogous.

---
class: center, middle
# Neural Network Architecture à la '80s

![:dot 65%](digraph G { rankdir=LR; splines=line;
  x3 [ label=<x<sub><font point-size="10">3</font></sub>>]
  x4 [ label=<x<sub><font point-size="10">4</font></sub>>]
  x1 [ label=<x<sub><font point-size="10">1</font></sub>>]
  x2 [ label=<x<sub><font point-size="10">2</font></sub>>]
  h3 [ label=<h<sub><font point-size="10">3</font></sub>>]
  h4 [ label=<h<sub><font point-size="10">4</font></sub>>]
  h1 [ label=<h<sub><font point-size="10">1</font></sub>>]
  h2 [ label=<h<sub><font point-size="10">2</font></sub>>]
  y1 [ label=<y<sub><font point-size="10">1</font></sub>>]
  y2 [ label=<y<sub><font point-size="10">2</font></sub>>]
  {x1, x2, x3, x4} -> {h1, h2, h3, h4} -> {y1, y2}
  lx [ label=<Input<br/>layer>; shape=none ]
  la [ label=<Hidden<br/>layer>; shape=none ]
  ly [ label=<Output<br/>layer>; shape=none ]
  lx -> la -> ly [ style=invis ]
})

---
# Neural Network Activation Functions

## Hidden Layers Derivatives
- $σ$: $$\frac{dσ(x)}{dx} = σ(x) \cdot (1-σ(x))$$
- $\tanh$: $$\frac{d\tanh(x)}{dx} = 1 - \tanh(x)^2$$
- ReLU:
  $$ \frac{d\ReLU(x)}{dx} = \begin{cases} 1 &\text{if } x > 0 \\\ \textrm{NaN} &\text{if }x = 0 \\\ 0 &\text{if } x < 0 \end{cases}$$

---
class: middle
# Stochastic Gradient Descent

#### Stochastic Gradient Descent (SGD) Algorithm
.algorithm[
**Inputs**: NN computing function $f(→x; →θ)$ with initial value of parameters $→θ$.<br>
**Inputs**: Learning rate $α$.<br>
**Outputs**: Updated parameters $→θ$.
- Repeat until stopping criterion is met:
    - Sample a minibatch of $m$ training examples $(→x^{(i)}, y^{(i)})$
    - $→g ← \frac{1}{m} ∇\_{→θ} ∑\_i L(f(→x^{(i)}; →θ), y^{(i)})$
    - $→θ ← →θ - α→g$
]

---
# SGD With Momentum

#### SGD With Momentum
.algorithm[
**Inputs**: NN computing function $f(→x; →θ)$ with initial value of parameters $→θ$.<br>
**Inputs**: Learning rate $α$, momentum $β$.<br>
**Outputs**: Updated parameters $→θ$.
- Repeat until stopping criterion is met:
    - Sample a minibatch of $m$ training examples $(→x^{(i)}, y^{(i)})$
    - $→g ← \frac{1}{m} ∇\_{→θ} ∑\_i L(f(→x^{(i)}; →θ), y^{(i)})$
    - $→v ← β→v - α→g$
    - $→θ ← →θ + →v$
]
![:pdf 30%,center](momentum.pdf)

---
# SGD With Nesterov Momentum

#### SGD With Nesterov Momentum
.algorithm[
**Inputs**: NN computing function $f(→x; →θ)$ with initial value of parameters $→θ$.<br>
**Inputs**: Learning rate $α$, momentum $β$.<br>
**Outputs**: Updated parameters $→θ$.
- Repeat until stopping criterion is met:
    - Sample a minibatch of $m$ training examples $(→x^{(i)}, y^{(i)})$
    - $→θ ← →θ + β→v$
    - $→g ← \frac{1}{m} ∇\_{→θ} ∑\_i L(f(→x^{(i)}; →θ), y^{(i)})$
    - $→v ← β→v - α→g$
    - $→θ ← →θ - α→g$
]
![:image 70%,center](nesterov.jpg)

---
class: middle
# Algorithms with Adaptive Learning Rates

#### AdaGrad (2011)
.algorithm[
**Inputs**: NN computing function $f(→x; →θ)$ with initial value of parameters $→θ$.<br>
**Inputs**: Learning rate $α$, constant $ε$ (usually $10^{-8}$).<br>
**Outputs**: Updated parameters $→θ$.
- Repeat until stopping criterion is met:
    - Sample a minibatch of $m$ training examples $(→x^{(i)}, y^{(i)})$
    - $→g ← \frac{1}{m} ∇\_{→θ} ∑\_i L(f(→x^{(i)}; →θ), y^{(i)})$
    - $→r ← →r + →g^2$
    - $→θ ← →θ - \frac{α}{\sqrt{→r + ε}}→g$
]

---
class: middle
# Algorithms with Adaptive Learning Rates

#### RMSProp (2012)
.algorithm[
**Inputs**: NN computing function $f(→x; →θ)$ with initial value of parameters $→θ$.<br>
**Inputs**: Learning rate $α$, momentum $β$, constant $ε$ (usually $10^{-8}$).<br>
**Outputs**: Updated parameters $→θ$.
- Repeat until stopping criterion is met:
    - Sample a minibatch of $m$ training examples $(→x^{(i)}, y^{(i)})$
    - $→g ← \frac{1}{m} ∇\_{→θ} ∑\_i L(f(→x^{(i)}; →θ), y^{(i)})$
    - $→r ← β→r + (1-β)→g^2$
    - $→θ ← →θ - \frac{α}{\sqrt{→r + ε}}→g$
]

---
# Algorithms with Adaptive Learning Rates

#### Adam (2014)
.algorithm[
**Inputs**: NN computing function $f(→x; →θ)$ with initial value of parameters $→θ$.<br>
**Inputs**: Learning rate $α$ (default 0.001), constant $ε$ (usually $10^{-8}$).<br>
**Inputs**: Momentum $β\_1$ (default 0.9), momentum $β\_2$ (default 0.999).<br>
**Outputs**: Updated parameters $→θ$.
- $→s ← 0$, $→r ← 0$, $t ← 0$
- Repeat until stopping criterion is met:
    - Sample a minibatch of $m$ training examples $(→x^{(i)}, y^{(i)})$
    - $→g ← \frac{1}{m} ∇\_{→θ} ∑\_i L(f(→x^{(i)}; →θ), y^{(i)})$
    - $t ← t + 1$
    - $→s ← β\_1→s + (1-β\_1)→g$ &nbsp; &nbsp; _(biased first moment estimate)_
    - $→r ← β\_2→r + (1-β\_2)→g^2$ &nbsp;_(biased second moment estimate)_
    - $\hat→s ← →s / (1 - β\_1^t)$
    - $\hat→r ← →r / (1 - β\_2^t)$
    - $→θ ← →θ - \frac{α}{\sqrt{\hat→r + ε}}\hat→s$
]

---
# Algorithms with Adaptive Learning Rates

#### Adam (2014)
.algorithm[
**Inputs**: NN computing function $f(→x; →θ)$ with initial value of parameters $→θ$.<br>
**Inputs**: Learning rate $α$ (default 0.001), constant $ε$ (usually $10^{-8}$).<br>
**Inputs**: Momentum $β\_1$ (default 0.9), momentum $β\_2$ (default 0.999).<br>
**Outputs**: Updated parameters $→θ$.
- $→s ← 0$, $→r ← 0$, $t ← 0$
- Repeat until stopping criterion is met:
    - Sample a minibatch of $m$ training examples $(→x^{(i)}, y^{(i)})$
    - $→g ← \frac{1}{m} ∇\_{→θ} ∑\_i L(f(→x^{(i)}; →θ), y^{(i)})$
    - $t ← t + 1$
    - $→s ← β\_1→s + (1-β\_1)→g$ &nbsp; &nbsp; _(biased first moment estimate)_
    - $→r ← β\_2→r + (1-β\_2)→g^2$ &nbsp;_(biased second moment estimate)_
    - $α\_t ← α \sqrt{1 - β\_2^t} / (1-β\_1^t)$
    - $→θ ← →θ - \frac{α\_t}{\sqrt{→r + ε}}→s$
]

---
# Adam Bias Correction

After $t$ steps, we have $$→r\_t = (1 - β\_2) ∑\_{i=1}^t β\_2^{t-i}→g\_i^2.$$

Assuming that the second moment $𝔼[→g\_i^2]$ is stationary, we have
$$\begin{aligned}
𝔼[→r\_t] &=  𝔼\left[(1 - β\_2) ∑\_{i=1}^t β\_2^{t-i}→g\_i^2\right] \\\
         &=  𝔼[→g\_t^2] \cdot (1 - β\_2) ∑\_{i=1}^t β\_2^{t-i} \\\
         &=  𝔼[→g\_t^2] \cdot (1 - β\_2^t)
\end{aligned}$$

and analogously for correction of $→s$.

---
# Adaptive Optimizers Animations

![:image 68%,center](optimizers-1.gif)

---
# Adaptive Optimizers Animations

![:image 85%,center](optimizers-2.gif)

---
# Adaptive Optimizers Animations

![:image 85%,center](optimizers-3.gif)

---
# Adaptive Optimizers Animations

![:image 85%,center](optimizers-4.gif)
