class: title
## NPFL114, Lecture 03

# Training Neural Networks II

![:pdf 26%,,padding-right:64px](../res/mff.pdf)
![:image 33%,,padding-left:64px](../res/ufal.svg)

.author[
Milan Straka
]

---
# Putting It All Together

Let us have a dataset with a training, validation and test sets, each containing
examples $(→x, y)$. Depending on $y$, consider one of the following output
activation functions:
$$\begin{cases}
  \textrm{none} & \textrm{ if } y ∈ ℝ\\\
  σ & \textrm{ if } y \textrm{ is a probability of an outcome}\\\
  \softmax & \textrm{ if } y \textrm{ is a gold class}
\end{cases}$$

If $→x ∈ ℝ^d$, we can use a neural network with an input layer of size $d$,
hidden layer of size $h$ with a non-linear activation function, and an output
layer of size $o$ (either 1 or number of classification classes) with the
mentioned output function.

---
layout: true
# Putting It All Together
![:dot 35%,center](digraph G { rankdir=LR; splines=line;
  x3 [ label=<x<sub><font point-size="10">3</font></sub>>]
  x4 [ label=<x<sub><font point-size="10">4</font></sub>>]
  x1 [ label=<x<sub><font point-size="10">1</font></sub>>]
  x2 [ label=<x<sub><font point-size="10">2</font></sub>>]
  h3 [ label=<h<sub><font point-size="10">3</font></sub>>]
  h4 [ label=<h<sub><font point-size="10">4</font></sub>>]
  h1 [ label=<h<sub><font point-size="10">1</font></sub>>]
  h2 [ label=<h<sub><font point-size="10">2</font></sub>>]
  o1 [ label=<o<sub><font point-size="10">1</font></sub>>]
  o2 [ label=<o<sub><font point-size="10">2</font></sub>>]
  {x1, x2, x3, x4} -> {h1, h2, h3, h4} -> {o1, o2}
  lx [ label=<Input<br/>layer>; shape=none ]
  la [ label=<Hidden<br/>layer>; shape=none ]
  lo [ label=<Output<br/>layer>; shape=none ]
  lx -> la -> lo [ style=invis ]
})

---

We have $$h\_i = f^{(1)}\left(∑\_j ⇉W^{(1)}\_{i,j} x\_j + b^{(1)}\_i\right)$$
where $⇉W^{(1)} ∈ ℝ^\{h×d}$ is a matrix of weights, $→b^{(1)} ∈ ℝ^h$ is a vector of biases,
and $f^{(1)}$ is an activation function.

---

Similarly $$o\_i = f^{(2)}\left(∑\_j ⇉W^{(2)}\_{i,j} h\_j + b^{(2)}\_i\right)$$
with $⇉W^{(2)} ∈ ℝ^\{o×h}$ another matrix of weights, $→b^{(2)} ∈ ℝ^o$ another vector of biases,
and $f^{(2)}$ being an output activation function.

---
layout: false
# Putting It All Together

The parameters of the model are therefore $⇉W^{(1)}, ⇉W^{(2)}, →b^{(1)}, →b^{(2)}$ of total size
$d×h + h×o + h + o$.

To train the network, we repeatedly sample $m$ training examples and perform
SGD (or any its adaptive variant), updating the parameters to minimize the
loss.

--

$$θ\_i ← θ\_i - α\frac{∂L}{∂θ\_i}$$

--

We set the hyperparameters (size of the hidden layer, hidden layer activation
function, learning rate, …) using performance on the validation set and evaluate
generalization error on the test set.

---
# Practical Issues

- Processing all input in _batches_.

--

- Vector representation of the network.

--

  Considering $h\_i = f^{(1)}\left(∑\_j ⇉W^{(1)}\_{i,j} x\_j + b^{(1)}\_i\right)$,
  we can write $$→h = f^{(1)}\left(⇉W^{(1)} →x + →b^{(1)}\right)$$
  $$→o = f^{(2)}\left(⇉W^{(2)} →h + →b^{(2)}\right) = f^{(2)}\left(⇉W^{(2)} \left(f^{(1)}\left(⇉W^{(1)} →x + →b^{(1)}\right)\right) + →b^{(2)}\right)$$

--

  The derivatives $$\frac{∂f^{(1)}\left(⇉W^{(1)} →x + →b^{(1)}\right)}{∂⇉W^{(1)}}$$
  are then matrices (called _Jacobians_).

---
class: center
# Computation Graph

![:dot 30%](digraph G { rankdir=LR; splines=line;
  x3 [ label=<x<sub><font point-size="10">3</font></sub>>]
  x4 [ label=<x<sub><font point-size="10">4</font></sub>>]
  x1 [ label=<x<sub><font point-size="10">1</font></sub>>]
  x2 [ label=<x<sub><font point-size="10">2</font></sub>>]
  h3 [ label=<h<sub><font point-size="10">3</font></sub>>]
  h4 [ label=<h<sub><font point-size="10">4</font></sub>>]
  h1 [ label=<h<sub><font point-size="10">1</font></sub>>]
  h2 [ label=<h<sub><font point-size="10">2</font></sub>>]
  o1 [ label=<o<sub><font point-size="10">1</font></sub>>]
  o2 [ label=<o<sub><font point-size="10">2</font></sub>>]
  {x1, x2, x3, x4} -> {h1, h2, h3, h4} -> {o1, o2}
  lx [ label=<Input<br/>layer>; shape=none ]
  la [ label=<Hidden<br/>layer>; shape=none ]
  lo [ label=<Output<br/>layer>; shape=none ]
  lx -> la -> lo [ style=invis ]
})

$↓$

![:dot 90%](digraph G { rankdir=LR; splines=line;
  x [ label=<<b>x</b>> ]
  W1 [ label=<<b>W</b><sup><font point-size="10">1</font></sup>> ]
  xW1 [ label=<*> ]; {x, W1} -> xW1
  b1 [ label=<<b>b</b><sup><font point-size="10">1</font></sup>> ]
  xW1b1 [ label=<+> ]; {xW1, b1} -> xW1b1
  h [ label=<f<sup><font point-size="10">1</font></sup>> ]; xW1b1 -> h
  W2 [ label=<<b>W</b><sup><font point-size="10">2</font></sup>> ]
  hW2 [ label=<*> ]; {h, W2} -> hW2
  b2 [ label=<<b>b</b><sup><font point-size="10">2</font></sup>> ]
  hW2b2 [ label=<+> ]; {hW2, b2} -> hW2b2
  o [ label=<f<sup><font point-size="10">2</font></sup>> ]; hW2b2 -> o
  y [ label="y" ];
  L [ label="L" ]; {o, y} -> L
})

---
# Neural Networks Web Browser Demos

- [TensorFlow Playground](http://playground.tensorflow.org/)

- [Sketch RNN Demo](https://magenta.tensorflow.org/assets/sketch_rnn_demo/index.html)

- [DeepLearn.js Demos](https://deeplearnjs.org/index.html#demos)
  - [DeepLearn.js Image Style Transfer Demo](https://reiinakano.github.io/fast-style-transfer-deeplearnjs/)

---
class: middle, tablebig
# High Level Overview

|                 | Classical ('90s) | Deep Learning |
|-----------------|------------------|---------------|
|Architecture     | $\vdots\,\,\vdots\,\,\vdots$ | $\vdots\,\,\vdots\,\,\vdots\,\,\vdots\,\,\vdots\,\,\vdots\,\,\vdots\,\,\vdots\,\,\vdots\,\,\vdots$ &nbsp; CNN, RNN, VAE, GAN, …
|Activation func. | $\tanh, σ$    | $\tanh$, ReLU, PReLU, ELU, SELU, Swish, …
|Output function  | none, $σ$     | none, $σ$, $\softmax$
|Loss function    | MSE           | NLL (or cross-entropy or KL-divergence)
|Optimalization   | SGD, momentum | SGD, RMSProp, Adam, …
|Regularization   | L2, L1        | L2, Dropout, BatchNorm, LayerNorm, …

---
# MLE Loss of Softmax

![:dot 50%,center](digraph G { rankdir=LR; splines=line; compound=true; node [style=invis]
  subgraph cluster_1 { s1 s2 s3 s4 }
  z1 -> s1 [label=<z<sub><font point-size="8">1</font></sub>>; lhead=cluster_1]
  z2 -> s2 [label=<z<sub><font point-size="8">2</font></sub>>; lhead=cluster_1]
  z3 -> s3 [label=<z<sub><font point-size="8">3</font></sub>>; lhead=cluster_1]
  z4 -> s4 [label=<z<sub><font point-size="8">4</font></sub>>; lhead=cluster_1]
  s1 -> o1 [label=<o<sub><font point-size="8">1</font></sub>>; ltail=cluster_1]
  s2 -> o2 [label=<o<sub><font point-size="8">2</font></sub>>; ltail=cluster_1]
  s3 -> o3 [label=<o<sub><font point-size="8">3</font></sub>>; ltail=cluster_1]
  s4 -> o4 [label=<o<sub><font point-size="8">4</font></sub>>; ltail=cluster_1]
  lz; ls [label="Softmax"; shape=none; style=normal]; lz -> ls [style=invis]
})

Let us have a softmax output layer with
$$o\_i = \frac{e^{z\_i}}{∑\_j e^{z\_j}}.$$

---
# MLE Loss of Softmax

Consider now the MLE estimation. The loss $L(\softmax(→z), \textit{gold})$ for
gold class index $\textit{gold}$ is then $$L(\softmax(→z), \textit{gold}) = - \log o\_\textit{gold}.$$

The derivation of the loss with respect to $→z$ is then
$$\begin{aligned}
\frac{∂L}{∂z\_i} =& \frac{∂}{∂z\_i} \left[-\log \frac{e^{z\_\textit{gold}}}{∑\_j e^{z\_j}}\right] \\\
                 =& -\frac{∂z\_\textit{gold}}{∂z\_i} + \frac{∂\log(∑\_j e^{z\_j})}{∂z\_i} \\\
                 =& -[\textit{gold} = i] + \frac{1}{∑\_j e^{z\_j}} e^{z\_i} \\\
                 =& -[\textit{gold} = i] + o\_i.
\end{aligned}$$

Therefore, $\frac{∂L}{∂→z} = - →1\_\textit{gold} + →o$, where $→1\_\textit{gold}$
is 1 at index $\textit{gold}$ and 0 otherwise.

---
# MLE Loss of Softmax and Sigmoid

In the previous case, the gold distribution was _sparse_, with only one
probability being 1.

In the case of general gold distribution $→g$, we have
$$L(\softmax(→z), →g) = -∑\_i g\_i \log o\_i.$$ Adapting the previous procedure,
we obtain
$$\frac{∂L}{∂→z} = -→g + →o.$$

--

## Sigmoid

Analogously, for $o = σ(z)$ we get $\frac{∂L}{∂z} = -g + o$,
where $g$ is the target gold probability.

---
# Regularization

As already mentioned, regularization is any change in the machine learning
algorithm that is designed to reduce generalization error but not necessarily
its training error.

Regularization is usually needed only if training error and generalization error
are different. That is often not the case if we process each training example
only once. Generally the more training data, the better generalization
performance.

- Early stopping

- L2, L1 regularization

- Dataset augmentation

- Ensembling

- Dropout

---
class: middle
# Regularization – Early Stopping

![:pdf 100%](early_stopping.pdf)

---
# L2 Regularization

We prefer models with parameters small under L2 metric.

The L2 regularization, also called _weight decay_, _Tikhonov regularization_ or
_ridge regression_ therefore minimizes
$$\tilde J(→θ; \mathbb X) = J(→θ; \mathbb X) + λ ||→θ||\_2^2$$
for a suitable (usually very small) $λ$.

During the parameter update of SGD, we get
$$θ\_i ← θ\_i - α\frac{∂J}{∂θ\_i} - 2αλθ\_i$$

---
class: center,middle
# L2 Regularization

![:pdf 60%](l2_regularization.pdf)

---
# L2 Regularization as MAP

Another way to arrive at L2 regularization is to utilize Bayesian inference.

With MLE we have
$$→θ\_\mathrm{MLE} = \argmax\_→θ p(\mathbb X; →θ).$$

Instead, we may want to maximize _maximum a posteriori (MAP)_ point estimate:
$$→θ\_\mathrm{MAP} = \argmax\_→θ p(→θ; \mathbb X)$$

Using Bayes' theorem $\left[p(→θ; \mathbb X) = p(\mathbb X; →θ) p(→θ) / p(\mathbb X)\right]$, we get
$$→θ\_\mathrm{MAP} = \argmax\_→θ p(\mathbb X; →θ)p(→θ).$$

---
# L2 Regularization as MAP

The $p(→θ)$ are prior probabilities of the parameter values (our _preference_).

One possibility for such a prior is $\N(→θ; 0, σ^2)$.

Then
$$\begin{aligned}
→θ\_\mathrm{MAP} &= \argmax\_→θ p(\mathbb X; →θ)p(→θ) \\\
                 &= \argmax\_→θ ∏\nolimits\_{i=1}^m p(→x^{(i)}; →θ)p(→θ) \\\
                 &= \argmin\_→θ ∑\nolimits\_{i=1}^m -\log p(→x^{(i)}; →θ) - \log p(→θ) \\\
\end{aligned}$$

By substituting the probability of the Gaussian prior, we get
$$→θ\_\mathrm{MAP} = \argmin\_→θ ∑\_{i=1}^m -\log p(→x^{(i)}; →θ) + \frac{1}{2} \log(2πσ^2) + \frac{→θ^2}{2σ^2}$$

---
# L1 Regularization

Similar to L2 regularization, but we prefer low L1 metric of parameters. We
therefore minimize
$$\tilde J(→θ; \mathbb X) = J(→θ; \mathbb X) + λ ||→θ||\_1$$

The corresponding SGD update is then $$θ\_i ← θ\_i - α\frac{∂J}{∂θ\_i} - αλ.$$

---
# Regularization – Dataset Augmentation

For some data, it is cheap to generate slightly modified examples.

- Image processing: translations, horizontal flips, scaling, rotations, color
  adjustments, …

- Speech recognition

- More difficult for discrete domains like text.

---
# Regularization – Dataset Augmentation

- Noise injection to input examples

- Label smoothing

-  Mixup (25 Oct 2017)

![:pdf 60%,center](mixup.pdf)

---
# Regularization – Ensembling

Ensembling (also called _model averaging_ or _bagging_) is a general technique
for reducing generalization error by combining several models. The models are
usually combined by averaging their outputs (either distributions or output
values in case of regression).

---
# Regularization – Ensembling

- Generate different datasets by sampling with replacement.

![:pdf 80%,center](ensembling.pdf)

- Use random different initialization.

- Average models from last hours/days of training.

---
# Regularization – Dropout

How to design good universal features?

- In reproduction, evolution is achieved using gene swapping. The genes must not
  be just good with combination with other genes, they need to be universally
  good.

--

Idea of _dropout_ by (Srivastava et al., 2014), in preprint since 2012.

When applying dropout to a layer, we drop each neuron independently with
a probability of $p$ (usually called _dropout rate_). To the rest of the
network, the dropped neurons have value of zero.

Dropout is performed only when training, during inference no nodes are
dropped. However, in that case we need to _scale the activations down_
by a factor of $1-p$ to account for more neurons than usual.

Alternatively, we might _scale the activations up_ during training by a factor
of $1/(1-p)$.

---
class: middle, center
# Regularization – Dropout as Ensembling

![:pdf 70%](dropout_ensembling.pdf)
