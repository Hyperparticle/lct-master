class: title
## NPFL114, Lecture 04

# Convolutional Networks

![:pdf 26%,,padding-right:64px](../res/mff.pdf)
![:image 33%,,padding-left:64px](../res/ufal.svg)

.author[
Milan Straka
]

---
class: middle
# Dropout Implementation

```python
def dropout(input, rate=0.5, training=False):
    def do_inference():
        return tf.identity(input)

    def do_train():
        random_noise = tf.random_uniform(tf.shape(input))
        mask = tf.cast(tf.less(random_noise, rate),
                       tf.float32)
        return input * mask / (1 - rate)

    if training == True:
        return do_train()
    if training == False:
        return do_inference()
    return tf.cond(training, do_train, do_inference)
```

---
class: middle
# Dropout Effect

![:pdf 100%](dropout_features.pdf)

---
class: middle
# Dropout Effect

![:pdf 100%](dropout_activations.pdf)

---
# Convergence

The training process might or might not converge. Even if it does, it might
converge slowly or quickly.

There are _many_ factors influencing convergence and its speed, we now discuss
three of them.

---
class: middle, full
# Convergence – Saturating Non-linearities

![:image 100%](tanh.png)

---
# Convergence – Parameter Initialization

Neural networks usually need random initialization to _break symmetry_.

- Biases are usually initialized to a constant value, usually 0.

--

- Weights are usually initialized to small random values, either with uniform or
  normal distribution.
   - The scale matters for deep networks!

   - Originally, people used $U\left[-\frac{1}{\sqrt n}, \frac{1}{\sqrt n}\right]$ distribution.

--

   - Xavier Glorot and Yoshua Bengio, 2010:
     _Understanding the difficulty of training deep feedforward neural networks_.

     The authors theoretically and experimentally show that a suitable way to
     initialize a $ℝ^{n×m}$ matrix is $$U\left[-\sqrt{\frac{6}{m+n}}, \sqrt{\frac{6}{m+n}}\right].$$

---
class: middle, center
# Convergence – Parameter Initialization

![:pdf 85%](glorot_activations.pdf)

---
class: middle, center
# Convergence – Parameter Initialization

![:pdf 85%](glorot_gradients.pdf)

---
class: middle
# Convergence – Gradient Clipping

![:pdf 100%](exploding_gradient.pdf)

---
# Convergence – Gradient Clipping

![:pdf 80%,center](gradient_clipping.pdf)

Using a given maximum norm, we may clip the gradient.
$$g ← \begin{cases}
  g & \textrm{ if }||g|| ≤ c \\\
  c \frac{g}{||g||} & \textrm{ if }||g|| > c
\end{cases}$$

The clipping can be per weight, per matrix or for the gradient as a whole.

---
class: middle, center
# Going Deeper

# Going Deeper

---
# Convolutional Networks

Consider data with some structure (temporal data, speech, images, …).

Unlike densely connected layers, we might want:
- Sparse (local) interactions
- Parameter sharing (equal response everywhere)
- Shift invariance

---
class: middle
# Convolutional Networks

![:image 100%](convolution.png)

---
# Convolution Operation

For a functions $x$ and $w$, _convolution_ $x \* w$ is defined as
$$(x \* w)(t) = ∫x(a)w(t - a)\d a.$$

For vectors, we have
$$(→x \* →w)\_t = ∑\_i x\_i w\_{t-i}.$$

Convolution operation can be generalized to two dimensions by
$$(⇉I \* ⇉K)\_{i, j} = ∑\_{m, n} ⇉I\_{m, n} ⇉K\_{i-m, j-n}.$$

Closely related is _cross-corellation_, where $K$ is flipped:
$$S\_{i, j} = ∑\_{m, n} ⇉I\_{i+m, j+n} ⇉K\_{m, n}.$$

---
class: center
# Convolution

![:pdf 74%](convolution_all.pdf)

---
# Convolution Layer

The $K$ is usually called a _kernel_ or a _filter_, and we generally apply
several of them at the same time.

Consider an input image with $C$ channels. The convolution layer with
$F$ filters of width $W$, height $H$ and stride $S$ produces an output with $F$ channels
kernels of total size $W × H × C × F$ and is computed as
$$(⇉I \* ⇉K)\_{i, j, k} = ∑\_{m, n, o} ⇉I\_{i\cdot S + m, j\cdot S + n, o} ⇉K\_{m, n, o, k}.$$

--

There are multiple padding schemes, most common are:
- `valid`: we only use valid pixels, which causes the result to be smaller
  than the input
- `same`: we pad original image with zero pixels so that the result is exactly
  the size of the input

---
# Convolution Layer

There are two prevalent image formats:
- `NHWC` or `channels_last`: The dimensions of the 4-dimensional image tensor
  are batch, height, width, and channels.

  The original TensorFlow format, faster on CPU.

--

- `NCHW` or `channels_first`: The dimensions of the 4-dimensional image tensor
  are batch, channel, height, and width.

  Usual GPU format (used by CUDA and nearly all frameworks); on TensorFlow, not
  all CPU kernels are available with this layout.

---
# Pooling

Pooling is an operation similar to convolution, but we perform a fixed operation
instead of multiplying by a kernel.

- Max pooling: minor translation invariance
- Average pooling

![:pdf 80%,center](pooling.pdf)

---
# High-level CNN Architecture

We repeatedly use the following block:
1. Convolution operation
2. Non-linear activation (usually ReLU)
3. Pooling

![:image 100%](cnn.jpg)

---
class: middle
# AlexNet – 2012 (16.4% error)

![:pdf 100%](alexnet.pdf)

---
# AlexNet – 2012 (16.4% error)

Training details:
- 2 GPUs for 5-6 days

- SGD with batch size 128, momentum 0.9, weight decay 0.0005

- initial learning rate 0.01, manually divided by 10 when validation error rate
  stopped improving

- dropout with rate 0.5 on fully-connected layers

- data augmentation using translations and horizontal reflections (choosing random
  $224 × 224$ patches from $256 × 256$ images)
  - during inference, 10 patches are used (four corner patches and a center
    patch, as well as their reflections)

---
class: middle, center
# AlexNet – ReLU vs tanh

![:pdf 85%](relu_vs_tanh.pdf)

---
class: middle
# LeNet – 1998

![:pdf 100%](lenet.pdf)

Achieved 0.8% test error on MNIST.

---
class: middle
# Similarities in V1 and CNNs

![:pdf 100%](gabor_functions.pdf)
The primary visual cortex recognizes Gabor functions.

---
class: middle
# Similarities in V1 and CNNs

![:pdf 100%](first_convolutional_layer.pdf)
Similar functions are recognized in the first layer of a CNN.

---
class: middle, center
# CNNs as Regularizers – Deep Prior

![:image 70%](deep_prior_superresolution.jpg)

---
class: middle
# CNNs as Regularizers – Deep Prior

![:image 100%](deep_prior_inpainting.jpg)

---
class: middle
# CNNs as Regularizers – Deep Prior

![:image 100%](deep_prior_inpainting_diversity.jpg)

---
class: middle
# CNNs as Regularizers – Deep Prior

![:image 100%](deep_prior_inpainting_architecture.jpg)

[Deep Prior paper website with supplementary material](https://dmitryulyanov.github.io/deep_image_prior)
