class: title
## NPFL114, Lecture 05

# Convolutional Networks II

![:pdf 26%,,padding-right:64px](../res/mff.pdf)
![:image 33%,,padding-left:64px](../res/ufal.svg)

.author[
Milan Straka
]

---
# Convolution Layer

The $K$ is usually called a _kernel_ or a _filter_, and we generally apply
several of them at the same time.

Consider an input image with $C$ channels. The convolution layer with
$F$ filters of width $W$, height $H$ and stride $S$ produces an output with $F$ channels
kernels of total size $W √ó H √ó C √ó F$ and is computed as
$$(‚áâI \* ‚áâK)\_{i, j, k} = ‚àë\_{m, n, o} ‚áâI\_{i\cdot S + m, j\cdot S + n, o} ‚áâK\_{m, n, o, k}.$$

--

There are multiple padding schemes, most common are:
- `valid`: we only use valid pixels, which causes the result to me smaller
- `same`: we pad original image with zero pixels so that the result is exactly
  the size of the input

---
# Convolution Layer

There are two prevalent image formats:
- `NHWC` or `channels_last`: The dimensions of the 4-dimensional image tensor
  are batch, height, width, and channels.

  The original TensorFlow format, faster on CPU.

--

- `NCHW` or `channels_first`: The dimensions of the 4-dimensional image tensor
  are batch, channel, height, and width.

  Usual GPU format (used by CUDA and nearly all frameworks); on TensorFlow, not
  all CPU kernels are available with this layout.

---
# Pooling

Pooling is an operation similar to convolution, but we perform a fixed operation
instead of multiplying by a kernel.

- Max pooling: minor translation invariance
- Average pooling

![:pdf 80%,center](../04/pooling.pdf)

---
class: center
# VGG ‚Äì 2014 (6.8% error)

![:pdf 62%](vgg_architecture.pdf)
![:pdf 45%](vgg_parameters.pdf)

---
class: middle
# Inception (GoogLeNet) ‚Äì 2014 (6.7% error)

![:pdf 100%](inception_block.pdf)

---
class: middle
# Inception (GoogLeNet) ‚Äì 2014 (6.7% error)

![:pdf 100%](inception_block_reduction.pdf)

---
class: middle
# Inception (GoogLeNet) ‚Äì 2014 (6.7% error)

![:pdf 100%](inception_architecture.pdf)

---
# Inception (GoogLeNet) ‚Äì 2014 (6.7% error)

![:pdf 14.5%,center](inception_graph.pdf)

Also note the two auxiliary classifiers (they have weight 0.3).

---
# Batch Normalization

_Internal covariate shift_ refers to the change in the distributions
of hidden node activations due to the updates of network parameters
during training.

Let $‚Üíx = (x\_1, \ldots, x\_d)$ be $d$-dimensional input. We would like to
normalize each dimension as $$\hat x\_i = \frac{x\_i - ùîº[x\_i]}{\sqrt{\Var[x\_i]}}.$$
Furthermore, it may be advantageous to learn suitable scale $Œ≥\_i$ and shift $Œ≤\_i$ to
produce normalized value $$y\_i = Œ≥\_i\hat x\_i + Œ≤\_i.$$

---
# Batch Normalization

Consider a mini-batch of $m$ examples $(‚Üíx^{(1)}, \ldots, ‚Üíx^{(m)})$.

_Batch normalizing transform_ of the mini-batch is the following transformation.

.algorithm[
**Inputs**: Mini-batch $(‚Üíx^{(1)}, \ldots, ‚Üíx^{(m)})$, $Œµ ‚àà ‚Ñù$<br>
**Outputs**: Normalized batch $(‚Üíy^{(1)}, \ldots, ‚Üíy^{(m)})$
- $‚ÜíŒº ‚Üê \frac{1}{m} ‚àë\_{i = 1}^m ‚Üíx^{(i)}$
- $‚ÜíœÉ^2 ‚Üê \frac{1}{m} ‚àë\_{i = 1}^m (‚Üíx^{(i)} - Œº)^2$
- $\hat‚Üíx^{(i)} ‚Üê (‚Üíx^{(i)} - ‚ÜíŒº) / \sqrt{œÉ^2 + Œµ}$
- $‚Üíy^{(i)} ‚Üê ‚ÜíŒ≥ \hat‚Üíx^{(i)} + ‚ÜíŒ≤$
]

--

Batch normalization is commonly added just before a nonlinearity. Therefore, we
replace $f(‚áâW‚Üíx + ‚Üíb)$ by $f(\textit{BN}(‚áâW‚Üíx))$.

--

During inference, $‚ÜíŒº$ and $‚ÜíœÉ^2$ are fixed. They are either precomputed
after training on the whole training data, or an exponential moving average is
updated during training.

---
class: middle
# Inception with BatchNorm (4.8% error)

![:pdf 100%](inception_batchnorm.pdf)

---
class: middle, center
# Inception v2 and v3 ‚Äì 2015 (3.6% error)

![:image 55%](inception3_conv5.png)
![:image 35%](inception3_conv3.png)

---
class: middle
# Inception v2 and v3 ‚Äì 2015 (3.6% error)

![:pdf 32%](inception3_inception_a.pdf)
![:pdf 32%](inception3_inception_b.pdf)
![:pdf 32%](inception3_inception_c.pdf)

---
class: middle, center
# Inception v2 and v3 ‚Äì 2015 (3.6% error)

![:pdf 70%](inception3_architecture.pdf)

---
class: middle, center
# Inception v2 and v3 ‚Äì 2015 (3.6% error)

![:pdf 70%](inception3_ablation.pdf)

---
class: middle
# ResNet ‚Äì 2015 (3.6% error)

![:pdf 100%](resnet_depth_effect.pdf)

---
class: middle
# ResNet ‚Äì 2015 (3.6% error)

![:pdf 100%](resnet_block.pdf)

---
class: middle
# ResNet ‚Äì 2015 (3.6% error)

![:pdf 100%](resnet_block_reduced.pdf)

---
class: middle
# ResNet ‚Äì 2015 (3.6% error)

![:pdf 100%](resnet_architecture.pdf)

---
class: center
# ResNet ‚Äì 2015 (3.6% error)

![:pdf 30%](resnet_overall.pdf)

---
class: middle
# ResNet ‚Äì 2015 (3.6% error)

![:pdf 100%](resnet_residuals.pdf)

---
class: middle, full
# ResNet ‚Äì 2015 (3.6% error)

![:image 100%](../02/nn_loss.jpg)

---
class: middle, full
# WideNet ‚Äì 2016

![:pdf 100%](widenet_block.pdf)

---
class: middle
# ResNeXt ‚Äì 2016

![:pdf 100%](resnext_block.pdf)

---
class: center, middle
# ResNeXt ‚Äì 2016

![:pdf 70%](resnext_architecture.pdf)

---
class: middle, full
# ResNeXt ‚Äì 2016

![:pdf 100%](resnext_training.pdf)

---
class: center, middle
# NasNet ‚Äì 2017

![:pdf 47%](nasnet_overall.pdf)

---
class: middle
# NasNet ‚Äì 2017

![:pdf 100%](nasnet_rnn_controller.pdf)

---
class: middle
# NasNet ‚Äì 2017

![:pdf 100%](nasnet_blocks.pdf)

---
class: middle
# NasNet ‚Äì 2017

![:pdf 100%](nasnet_performance.pdf)

---
class: middle
# NasNet ‚Äì 2017

![:pdf 100%](../01/nas_net.pdf)

---
class: middle, center
# Beyond Image Classification

# Beyond Image Classification

---
# Beyond Image Classification

- Transfer learning

--

- Convolutions can be applied to other dimensions than 2:

--

    - 1D: speech, text (e.g., text classification)

--
    - 3D: the dimensions can be either spacial (e.g., 3D object recognition)
      or temporal-spacial (e.g., videos)

---
# Beyond Image Classification

- Object detection (including location)
![:pdf 100%](object_detection.pdf)

--
- Image segmentation
![:pdf 100%](image_segmentation.pdf)

--
- Human pose estimation
![:pdf 100%](human_pose_estimation.pdf)

---
# Fast R-CNN

- Start with a network pre-trained on ImageNet (VGG-16 is used in the original
  paper).

--

## RoI Pooling
- Crucial for fast performance.
- The last max-pool layer ($14√ó14 ‚Üí 7√ó7$ in VGG) is replaced by a RoI pooling
  layer, producing output of the same size. For each output sub-window we
  max-pool the corresponding values in the output layer.
- Two sibling layers are added, one predicting $K+1$ categories and the other
  one predicting 4 bounding box parameters for each of $K$ categories.

---
class: middle
# Fast R-CNN

![:image 100%](fast_rcnn.jpg)

---
# Fast R-CNN

The bounding box is parametrized as follows. Let $x\_r, y\_r, w\_r, h\_r$ be
center coordinates and width and height of the RoI, and let $x, y, w, h$ be
parameters of the bounding box. We represent them as follows:
$$\begin{aligned}
t\_x &= (x - x\_r)/w\_r, & t\_y &= (y - y\_r)/h\_r \\\
t\_w &= \log (w/w\_r), & t\_h &= \log (h/h\_r)
\end{aligned}$$

--

Usually a $\textrm{smooth}\_{L\_1}$ loss is employed for bounding box parameters
$$\textrm{smooth}\_{L\_1}(x) = \begin{cases}
  0.5x^2    & \textrm{if }|x| < 1 \\\
  |x| - 0.5 & \textrm{otherwise}
\end{cases}$$

--

The complete loss is then
$$L(\hat c, \hat t,c, t) = L\_\textrm{cls}(\hat c, c) + Œª[c ‚â• 1]
  ‚àë\_{i ‚àà \lbrace \mathrm{x, y, w, h}\rbrace} \textrm{smooth}\_{L\_1}(\hat t\_i - t\_i).$$

---
# Fast R-CNN

##Intersection over union
For two bounding boxes (or two masks) the _intersection over union_ (_IoU_)
is a ration of the intersection of the boxes (or masks) and the union
of the boxes (or masks).

--

##Choosing RoIs for training
During training, we use $2$ images with $64$ RoIs each. The RoIs are selected
so that $25\%$ have intersection over union (IoU) overlap with ground-truth
boxes at least 0.5; the others are chosen to have the IoU in range $\[0.1, 0.5)$.

--

##Choosing RoIs during inference
Single object can be found in multiple RoIs. To choose the most salient one,
we perform _non-maximum suppression_ -- we ignore RoIs which have an overlap
with a higher scoring larger than a given threshold (usually, 0.3 is used).
