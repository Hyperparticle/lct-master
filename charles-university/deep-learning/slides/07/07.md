class: title
## NPFL114, Lecture 07

# Object Detection & Segmentation, Recurrent Neural Networks

![:pdf 26%,,padding-right:64px](../res/mff.pdf)
![:image 33%,,padding-left:64px](../res/ufal.svg)

.author[
Milan Straka
]

---
# Object Detection

- Object detection (including location)
![:pdf 100%](../05/object_detection.pdf)

--
- Image segmentation
![:pdf 100%](../05/image_segmentation.pdf)

--
- Human pose estimation
![:pdf 100%](../05/human_pose_estimation.pdf)

---
# Fast R-CNN

- Start with a network pre-trained on ImageNet (VGG-16 is used in the original
  paper).

--

## RoI Pooling
- Crucial for fast performance.
- The last max-pool layer ($14×14 → 7×7$ in VGG) is replaced by a RoI pooling
  layer, producing output of the same size. For each output sub-window we
  max-pool the corresponding values in the output layer.
- Two sibling layers are added, one predicting $K+1$ categories and the other
  one predicting 4 bounding box parameters for each of $K$ categories.

---
class: middle
# Fast R-CNN

![:image 100%](../05/fast_rcnn.jpg)

---
# Fast R-CNN

The bounding box is parametrized as follows. Let $x\_r, y\_r, w\_r, h\_r$ be
center coordinates and width and height of the RoI, and let $x, y, w, h$ be
parameters of the bounding box. We represent them as follows:
$$\begin{aligned}
t\_x &= (x - x\_r)/w\_r, & t\_y &= (y - y\_r)/h\_r \\\
t\_w &= \log (w/w\_r), & t\_h &= \log (h/h\_r)
\end{aligned}$$

--

Usually a $\textrm{smooth}\_{L\_1}$ loss is employed for bounding box parameters:
$$\textrm{smooth}\_{L\_1}(x) = \begin{cases}
  0.5x^2    & \textrm{if }|x| < 1 \\\
  |x| - 0.5 & \textrm{otherwise}
\end{cases}$$

--

The complete loss is then
$$L(\hat c, \hat t,c, t) = L\_\textrm{cls}(\hat c, c) + λ[c ≥ 1]
  ∑\_{i ∈ \lbrace \mathrm{x, y, w, h}\rbrace} \textrm{smooth}\_{L\_1}(\hat t\_i - t\_i).$$

---
# Fast R-CNN

##Intersection over union
For two bounding boxes (or two masks) the _intersection over union_ (_IoU_)
is a ration of the intersection of the boxes (or masks) and the union
of the boxes (or masks).

--

##Choosing RoIs for training
During training, we use $2$ images with $64$ RoIs each. The RoIs are selected
so that $25\%$ have intersection over union (IoU) overlap with ground-truth
boxes at least 0.5; the others are chosen to have the IoU in range $\[0.1, 0.5)$.

--

##Choosing RoIs during inference
Single object can be found in multiple RoIs. To choose the most salient one,
we perform _non-maximum suppression_ — we ignore RoIs which have an overlap
with a higher scoring larger than a given threshold (usually, 0.3 is used).

---
# Object Detection Evaluation

##Average Precision
Evaluation is performed using _Average Precision_ (_AP_).

We assume all bounding boxes (or masks) produced by a system have confidence
values which can be used to rank them. Then, for a single class, we take the
boxes (or masks) in the order of the ranks and generate precision/recall curve,
considering a bounding box correct if it has IoU at least 0.5 with any
ground-truth box.
We define _AP_ as an average of precisions for recall levels $0, 0.1, 0.2,
\ldots, 1$.

--

![:pdf 38%,,margin-left:11%](precision_recall_person.pdf)
![:pdf 38%](precision_recall_bottle.pdf)

---
# Faster R-CNN

For Fast R-CNN, the most time consuming part is generating the RoIs.

Therefore, Faster R-CNN jointly generates _regions of interest_ using
a _region proposal network_ and performs object detection.

--

![:image 50%,center](faster_rcnn_architecture.jpg)

---
# Faster R-CNN

The region proposals are generated using a $3×3$ sliding window, with
3 different scales ($128^2$, $256^2$ and $512^2$) and 3
aspect ratios ($1:1$, $1:2$, $2:1$).

![:pdf 100%](faster_rcnn_rpn.pdf)

---
class: middle
# Faster R-CNN

![:pdf 100%](faster_rcnn_performance.pdf)

---
# Mask R-CNN

"Straightforward" extension of Faster R-CNN able to produce image segmentation
(i.e., masks for every object).

![:pdf 100%,,margin-top:100px](../05/image_segmentation.pdf)

---
class: middle
# Mask R-CNN

![:image 100%](mask_rcnn_architecture.jpg)

---
# Mask R-CNN

##RoIAlign

More precise alignment if required for the RoI in order to predict the masks.
Therefore, instead of max-pooling used in the RoI pooling, RoIAlign with
bilinear interpolation is used instead.

![:pdf 50%,center](mask_rcnn_roialign.pdf)

---
# Mask R-CNN

Masks are predicted in a third branch of the object detector.

- Usually higher resolution is needed ($14×14$ instead of $7×7$).
- The masks are predicted for each class separately.
- The masks are predicted using convolutions instead of fully connected layers.

![:pdf 100%](mask_rcnn_heads.pdf)

---
class: middle, full
# Mask R-CNN

![:pdf 100%](mask_rcnn_ablation.pdf)

---
# Mask R-CNN – Human Pose Estimation

![:pdf 100%](../05/human_pose_estimation.pdf)

--

- Testing applicability of Mask R-CNN architecture.

- Keypoints (e.g., left shoulder, right elbow, …) are detected
  as independent one-hot masks of size $56×56$ with $\softmax$ output function.

--

![:pdf 100%](mask_rcnn_hpe_performance.pdf)

---
# Normalization

##Batch Normalization

Neuron value is normalized across the minibatch, and in case of CNN also across
all positions.

--

##Layer Normalization

Neuron value is normalized across the layer.

--

![:pdf 100%,,margin-top:50px](normalizations.pdf)

---
# Group Normalization

Group Normalization is analogous to Layer normalization, but the channels are
normalized in groups (by default, $G=32$).

![:pdf 100%](normalizations.pdf)

--

![:pdf 45%,center](group_norm.pdf)

---
class: middle, center, full
# Group Normalization

![:pdf 97%](group_norm_vs_batch_norm.pdf)

---
class: middle, center
# Group Normalization

![:pdf 80%](group_norm_coco.pdf)

---
class: middle, center
# Recurrent Neural Networks

# Recurrent Neural Networks

---
# Recurrent Neural Networks

## Single RNN cell

![:pdf 25%,center](rnn_cell.pdf)

--

## Unrolled RNN cells

![:pdf 90%,center](rnn_cell_unrolled.pdf)

---
# Basic RNN Cell

![:pdf 70%,center,margin-bottom:20px](rnn_cell_basic.pdf)

--

Given an input $→x^{(t)}$ and previous state $→s^{(t-1)}$, the new state is computed as
$$→s^{(t)} = f(→s^{(t-1)}, →x^{(t)}; →θ).$$

--

One of the simplest possibilities is
$$→s^{(t)} = \tanh(⇉U→s^{(t-1)} + ⇉V→x^{(t)} + →b).$$

---
# Basic RNN Cell

Basic RNN cells suffer a lot from vanishing/exploding gradients (_the challenge
of long-term dependencies_).

--

If we simplify the recurrence of states to $$→s^{(t)} = ⇉U→s^{(t-1)},$$
we get $$→s^{(t)} = ⇉U^t→s^{(0)}.$$

--

If $U$ has eigenvalue decomposition of $⇉U = ⇉Q ⇉Λ ⇉Q^{-1}$, we get
$$→s^{(t)} = ⇉Q ⇉Λ^t ⇉Q^{-1} →s^{(0)}.$$

--

Several more complex RNN cell variants have been proposed, which alleviate
this issue to some degree, namely **LSTM** and **GRU**. They will be introduced
in the next lecture.

---
# Basic RNN Applications

## Sequence Element Classification

Use outputs for individual elements.

![:pdf 70%,center,margin-top:20px;margin-bottom:20px](rnn_cell_unrolled.pdf)

--

## Sequence Representation

Use state after processing the whole sequence (alternatively, take output of the
last element).

---
# Basic RNN Applications

## Sequence Prediction

During training, predict next sequence element.

![:pdf 70%,center,margin-top:20px;margin-bottom:20px](sequence_prediction_training.pdf)

--

During inference, use predicted elements as further inputs.

![:pdf 70%,center,margin-top:20px;margin-bottom:20px](sequence_prediction_inference.pdf)

---
# TF – Functional vs Object Layers

So far, we have been using functional layers interface.

```python
tf.layers.dense(inputs, units, activation=None, ...)
```

Generally, each functional call creates a new layer (although it is possible to
employ `name` and `reuse` parameters and share layers of the same name).

--

TensorFlow also supports (newer) object interface.
```python
layer = tf.layers.Dense(units, activation=None, ...)

y1, y2 = layer(x1), layer(x2)
```

---
# TF – RNN Cells

RNN cells are provided only through the object interface.

- `tf.nn.rnn_cell.RNNCell`: abstract superclass
- `tf.nn.rnn_cell.BasicRNNCell`: the basic RNN cell described earlier
- `tf.nn.rnn_cell.{BasicLSTMCell,LSTMCell}`: the LSTM cells
- `tf.nn.rnn_cell.GRUCell`: the GRU cell
- various wrappers, and also additional types in `tf.contrib.rnn`

--

```python
rnn_cell = tf.nn.rnn_cell.BasicRNNCell(units,
                                       activation=None, ...)

output, state = rnn_cell(input, state)

rnn_cell.zero_state(batch_size, dtype)
```

---
# TF – Processing Sequences with RNN

## tf.nn.static\_rnn
```python
outputs, state = tf.nn.static_rnn(cell,
                                  inputs,
                                  initial_state=None,
                                  ...)
```

--

The `inputs` is a list of tensors, each of shape _(batchsize, inputsize)_,
and analogously for `outputs`. The `state` is the last state.

The length of `inputs` and `outputs` must be statically known.

---
# TF – Processing Sequences with RNN

## tf.nn.dynamic\_rnn
```python
outputs, state = tf.nn.dynamic_rnn(cell,
                                   inputs,
                                   initial_state=None,
                                   time_major=False,
                                   ...)
```

--

- If `time_major` is `False` (default for consistency with other operations),
  then `inputs` is a tensor with shape _(batchsize, maxtime, inputsize)_,
  and analogously for `outputs`.
- If `time_major` is `True` (faster variant), then `inputs` is a tensor with shape
  _(maxtime, batchsize, inputsize)_, and analogously for `outputs`.

The `state` is again the last state.

The length of the sequences do not need to be known statically (a `while` cycle
is added to the computational graph).
