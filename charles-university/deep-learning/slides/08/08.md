class: title
## NPFL114, Lecture 08

# Recurrent Neural Networks II,<br>Word Embeddings

![:pdf 26%,,padding-right:64px](../res/mff.pdf)
![:image 33%,,padding-left:64px](../res/ufal.svg)

.author[
Milan Straka
]

---
# Recurrent Neural Networks

## Single RNN cell

![:pdf 25%,center](../07/rnn_cell.pdf)

## Unrolled RNN cells

![:pdf 90%,center](../07/rnn_cell_unrolled.pdf)

---
# Challenge of Long-term Dependencies

Consider a RNN cell which given an input $→x^{(t)}$ and previous state
$→s^{(t-1)}$ computes the new state as
$$→s^{(t)} = f(→s^{(t-1)}, →x^{(t)}; →θ).$$

--

RNN cells generally suffer a lot from vanishing/exploding gradients (_the
challenge of long-term dependencies_), a problem attributed to the fact
that _same_ function is iteratively applied many times.

---
# Long Short-Term Memory

Hochreiter & Schmidhuber (1997) suggested that to enforce
_constant error flow_, we would like
$$f' = →1.$$

They propose to achieve that by a _constant error carrousel_.

![:pdf 90%,center](lstm_cec.pdf)

---
# Long Short-Term Memory

They also propose an _input_ and _output_ gates which control the flow
of information into and out of the carrousel (_memory cell_ $→c\_t$).

![:pdf 70%,center](lstm_input_output_gates.pdf)

$$\begin{aligned}
  →i\_t & ← σ(⇉W^i →x\_t + ⇉V^i →h\_{t-1} + →b^i) \\\
  →o\_t & ← σ(⇉W^o →x\_t + ⇉V^o →h\_{t-1} + →b^o) \\\
  →c\_t & ← →c\_{t-1} + →i\_t \cdot \tanh(⇉W^y →x\_t + ⇉V^y →h\_{t-1} + →b^y) \\\
  →h\_t & ← →o\_t \cdot \tanh(→c\_t)
\end{aligned}$$

---
# Long Short-Term Memory

Later in Gers, Schmidhuber & Cummins (1999) a possibility to _forget_
information from memory cell $→c\_t$ was added.

![:pdf 50%,center](lstm_input_output_forget_gates.pdf)

$$\begin{aligned}
  →i\_t & ← σ(⇉W^i →x\_t + ⇉V^i →h\_{t-1} + →b^i) \\\
  →f\_t & ← σ(⇉W^f →x\_t + ⇉V^f →h\_{t-1} + →b^f) \\\
  →o\_t & ← σ(⇉W^o →x\_t + ⇉V^o →h\_{t-1} + →b^o) \\\
  →c\_t & ← →f\_t \cdot →c\_{t-1} + →i\_t \cdot \tanh(⇉W^y →x\_t + ⇉V^y →h\_{t-1} + →b^y) \\\
  →h\_t & ← →o\_t \cdot \tanh(→c\_t)
\end{aligned}$$

---
# Gated Recurrent Unit

_Gated recurrent unit (GRU)_ was proposed by Cho et al. (2014) as
a simplification of LSTM. The main differences are
- no memory cell
- forgetting and updating tied together

![:pdf 70%,center](gru.pdf)

---
# Gated Recurrent Unit

![:pdf 70%,center](gru.pdf)

$$\begin{aligned}
  →r\_t & ← σ(⇉W^r →x\_t + ⇉V^r →h\_{t-1} + →b^r) \\\
  →u\_t & ← σ(⇉W^u →x\_t + ⇉V^u →h\_{t-1} + →b^u) \\\
  \hat→h\_t & ← \tanh(⇉W^h →x\_t + ⇉V^h (→r\_t \cdot →h\_{t-1}) + →b^h) \\\
  →h\_t & ← →u\_t \cdot →h\_{t-1} + (1 - →u\_t) \cdot \hat→h\_t
\end{aligned}$$

---
class: middle, center
# Word Embeddings

# Word Embeddings

---
# Word Embeddings

One-hot encoding considers all words to be independent of each other.

However, words are not independent – some are more similar than others.

Ideally, we would like some kind of similarity in the space of the word
representations.

--

## Distributed Representation
The idea behind distributed representation is that objects can
be represented using a set of common underlying factors.

--

We therefore represent words as fixed-size _embeddings_ into $ℝ^d$ space,
with the vector elements playing role of the common underlying factors.

---
# Unsupervised Word Embeddings

The embeddings can be trained for each task separately.

--

However, a method of precomputing word embeddings have been proposed, based on
_distributional hypothesis_:

**Words that are used in the same contexts tend to have similar meanings**.

The distributional hypothesis is usually attributed to Firth (1957).

---
# Word2Vec

Mikolov et al. (2013) proposed two very simple architectures for precomputing
word embeddings, together with a C multi-threaded implementation `word2vec`.

![:pdf 70%,center](word2vec.pdf)

---
class: middle
# Word2Vec

![:pdf 100%](word2vec_composability.pdf)

---
# Word2Vec – SkipGram Model

![:pdf 70%,center,margin:30px 0](word2vec.pdf)

Considering input word $w\_i$ and output $w\_o$, the Skip-gram model defines
$$p(w\_o | w\_i) ≝ \frac{e^{⇉W\_{w\_o}^\top ⇉V\_{w\_i}}}{∑\_w e^{⇉W\_w^\top ⇉V\_{w\_i}}}.$$

---
# Word2Vec – Hierarchical Softmax

Instead of a large softmax, we construct a binary tree over the words, with
a sigmoid classifier for each node.

If word $w$ corresponds to a path $n\_1, n\_2, \ldots, n\_L$, we define
$$p\_\textrm{HS}(w | w\_i) ≝ ∏\_{j=1}^{L-1} σ(\textrm{[+1 if }n\_{j+1}\textrm{  is right child else -1]} \cdot ⇉W\_{n\_j}^\top ⇉V\_{w\_i}).$$

---
# Word2Vec – Negative Sampling

Instead of a large softmax, we could train individual sigmoids for all words.

We could also only sample the _negative examples_ instead of training all of
them.

This gives rise to the following _negative sampling_ objective:
$$l\_\textrm{NEG}(w\_o, w\_i) ≝ \log σ(⇉W\_{w\_o}^\top ⇉V\_{w\_i}) + ∑\_{j=1}^k 𝔼\_{w\_j ∼ P(w)} \log σ(-⇉W\_{w\_j}^\top ⇉V\_{w\_i}).$$

--

For $P(w)$, both uniform and unigram distribution $U(w)$ work, but $$U(w)^{3/4}$$
outperforms them significantly.

---
class: middle, center
# Basic NLP Processing

# Basic NLP Processing

---
class: center
# Bidirectional RNN

![:pdf 80%](bidirectional_rnn.pdf)

---
class: middle, center
# Word Embeddings for Unknown Words

# Word Embeddings for Unknown Words

---
class: center
# Recurrent Character-level WEs

![:pdf 62%](cle_rnn.pdf)

---
class: middle
# Recurrent Character-level WEs

![:pdf 100%](cle_rnn_examples.pdf)

---
class: middle
# Recurrent Character-level WEs

![:image 100%](cle_rnn_gru.jpg)

---
class: center
# Convolutional Character-level WEs

![:pdf 75%](cle_cnn.pdf)

---
class: middle
# Convolutional Character-level WEs

![:pdf 100%](cle_cnn_examples.pdf)

---
# Character N-grams

Another simple idea appeared simultaneously in three nearly simultaneous
publications as [Charagram](https://arxiv.org/abs/1607.02789), [Subword Information](https://arxiv.org/abs/1607.04606) or [SubGram](http://link.springer.com/chapter/10.1007/978-3-319-45510-5_21).

A word embedding is a sum of the word embedding plus embeddings of its character
_n_-grams. Such embedding can be pretrained using same algorithms as `word2vec`.

--

The implementation can be
- dictionary based: only some number of frequent character _n_-grams is kept
- hash-based: character _n_-grams are hashed into $K$ buckets
  (usually $K ∼ 10^6$ is used)

---
class: middle
# Charagram WEs

![:pdf 100%](cle_charagram_examples.pdf)

---
class: center
# Charagram WEs

![:pdf 66%](cle_charagram_ngrams.pdf)

---
# Character-level WE Implementation

## Training

- Generate unique words per batch.

- Process the unique words in the batch.

- Copy the resulting embeddings suitably in the batch.

--

## Inference

- We can cache character-level word embeddings during inference.

---
# TF – RNN and Variable Length Sequences

## tf.nn.dynamic\_rnn
```python
outputs, state = tf.nn.dynamic_rnn(cell,
                                   inputs,
                                   dtype=None,
                                   initial_state=None,
*                                  sequence_length=None,
                                   ...)
```

--

The `sequence_length` parameter allows sequences of different length. If used,
the `outputs` past the sequence lengths are zeros and the `state` is the state
after processing last element of each sequence.

---
# TF – RNN and Variable Length Sequences

## Losses
```python
outputs, state = tf.nn.sparse_softmax_cross_entropy(
  labels,
  logits,
* weights=1.0,
  ...)
```
--

The `weights` can be used to mask some of the labels and logits.

## Generating the weights
```python
tf.sequence_masks(lengths, maxlen=None, dtype=tf.bool)

weights = tf.sequence_masks(lengths, dtype=tf.float32)
```

---
# TF – Bidirectional RNNs

## tf.nn.bidirectional\_dynamic\_rnn
```python
(outputs_fwd, outputs_bwd), (state_fwd, state_bwd) = \
  tf.nn.bidirectional_dynamic_rnn(cell_fwd,
                                  cell_bwd,
                                  inputs,
                                  dtype=None,
                                  initial_state_fwd=None,
                                  initial_state_bwd=None,
                                  sequence_length=None,
                                  time_major=False,
                                  ...)
```
