class: title
## NPFL114, Lecture 11

# Sequence Prediction,<br> Reinforcement Learning

![:pdf 26%,,padding-right:64px](../res/mff.pdf)
![:image 33%,,padding-left:64px](../res/ufal.svg)

.author[
Milan Straka
]

---
class: middle, center
# Structured Prediction

# Structured Prediction

---
# Structured Prediction

Consider generating a sequence of $y\_1, \ldots, y\_N ∈ Y^N$ given input
$→x\_1, \ldots, →x\_N$.

--
Predicting each sequence element models the distribution $P(y\_i | ⇉X)$.

--
There are two problems with the above approach:
1. There may be dependencies among the $y\_i$ themselves.

--

2. Even if we consider dependencies, if we compute $\softmax$ for each $y\_i$
   individually, we will hit the _label bias problem_.

![:dot 65%,center](digraph G { rankdir=LR; splines=line;
  0 -> 1 [label="x"]
  1 -> 2 [label="b"]
  2 -> 3 [label="c"]
  0 -> 4 [label="a"]
  4 -> 5 [label="b"]
  5 -> 3 [label="c"]
})

---
# Seq2seq Beam Search Decoding

So far we described only greedy decoding in a sequence to sequence decoder.

However, such decoding might not find the most probable output sequence.

We might consider a _beam search_, where we iteratively compute some fixed
number (a _beam size_) of best output sequences.

---
# Conditional Random Fields

Let $G = (V, E)$ be a graph such that $Y$ is indexed by vertices of $G$.
Then $(⇉X, →y)$ is a conditional markov field, if the random variables $→y$
conditioned on $⇉X$ obey the Markov property with respect to the graph, i.e.,
$$P(y\_i | ⇉X, y\_j, i ≠j) = P(y\_i | ⇉X, y\_j, (i, j) ∈ E).$$

--
Usually we assume that dependencies of $→y$, conditioned on $⇉X$, form a chain.

---
# CRF Output Layer

For a sequence of $(→x\_1, \ldots, →x\_N)$ and $(y\_1, \ldots, y\_N)$, we
define a score as
$$s(⇉X, →y; →θ, ⇉A) = ∑\_{i=1}^N \big(⇉A\_{y\_{i-1}, y\_i} + f\_→θ(y\_i | ⇉X)\big).$$

--
We then define
$$p(→y | ⇉X) = \softmax\_{→z ∈ Y^N}\big(s(⇉X, →z)\big)\_{→y},$$
so that
$$\log p(→y | ⇉X) = s(⇉X, →y) - \operatorname{logadd}\_{→z ∈ Y^N}(s(⇉X, →z)).$$

---
# CRF Output Layer

We can compute $p(→y | ⇉X)$ efficiently using dynamic programming. If we denote
$α\_t(k)$ as probability of all sentences with $t$ elements with the last $y$
being $k$.

We can then show that
$$\begin{aligned}
α\_t(k) &= \operatorname{logadd}\_{→z}(s(⇉X, →z)) \\\
        &= f\_→θ(y\_t=k | ⇉X\_{1:t}) + \operatorname{logadd}\_i(α\_{t-1}(i) + ⇉A\_{i, k}).
\end{aligned}$$

For efficient implementation, we use the fact that
$$\ln(a+b) = \ln a + \ln (1 + e^{\ln b - \ln a}).$$

---
# Connectionist Temporal Classification

Let us again consider generating a sequence of $y\_1, \ldots, y\_M$ given input
$→x\_1, \ldots, →x\_N$, but this time $M ≤ N$ and there is no explicit alignment
of $→x$ and $y$ in the gold data.

--
![:pdf 100%,,margin-top:100px](ctc_example.pdf)

---
# Connectionist Temporal Classification

We enlarge the set of output labels by a – (_blank_) and perform a classification for every
input element to produce an _extended labeling_. We then post-process it by the
following rules:
1. We remove neighboring symbols.
2. We remove the –.

--

Because the explicit alignment of inputs and labels is not known, we consider
_all possible_ alignments.

---
# Connectionist Temporal Classification

Let us denote the probability of label $l$ at time $t$ as $p\_l^t$.

We now consider a modified label sequence by inserting a – to the beginning,
to the end and between every pair of labels. Using this modified labeling
we define
$$α\_t(s) ≝ ∑\_{\textrm{labeling }→π\textrm{ with labels of }→π\_{1:t} = →y\_{1:s}} ∏\_{t'=1}^t p\_{→π\_{t'}}^{t'}.$$

--
Similarly to the CRF, we can use dynamic programming to compute $α$ in
polynomial time.

![:pdf 37%,center](ctc_computation.pdf)

---
# CTC Decoding

Unlike CRF, we cannot easily perform the decoding in an optimal way. We
therefore either use greedy decoding, or a beam search.

![:pdf 100%](ctc_decoding.pdf)

---
class: middle, center
# Reinforcement Learning

# Reinforcement Learning

---
# Reinforcement Learning

![:pdf 85%, center](diagram.pdf)

--

A _Markov decision process_ is a quadruple $(\mathcal S, \mathcal A, P, γ)$,
where:
- $\mathcal S$ is a set of states,
- $\mathcal A$ is a set of actions,
- $P(S\_{t+1} = s', R\_{t+1} = r | S\_t = s, A\_t = a)$ is a probability that
  action $a ∈ \mathcal A$ will lead from state $s ∈ \mathcal S$ to $s'
  ∈ \mathcal S$, producing a _reward_ $r ∈ ℝ$,
- $γ ∈ [0, 1]$ is a _discount factor_.

--

Let a _return_ $G\_t$ be $G\_t ≝ ∑\_{k=0}^\infty γ^k R\_{t + 1 + k}$.

---
class: middle
# K-armed Bandits

![:pdf 100%](k-armed_bandits.pdf)

---
# K-armed Bandits

Let $q\_\*(a)$ be the real _value_ of an action $a$:
$$q\_\*(a) = 𝔼[R\_{t+1} | A\_t = a].$$

--

Denoting $Q\_t(a)$ our estimated value of action $a$ at time $t$, we would like
$Q\_t(a)$ to converge to $q\_\*(a)$.

--

A natural way to estimate $Q\_t(a)$ is
$$Q\_t(a) ≝ \frac{\textrm{sum of rewards when action }a\textrm{ is taken}}{\textrm{number of times action }a\textrm{ was taken}}.$$

---
# K-armed Bandits

Following the definition of $Q\_t(a)$, we could choose a _greedy action_ $A\_t$ as
$$A\_t(a) ≝ \argmax_a Q\_t(a).$$

--
## Exploitation versus Exploration

Choosing a greedy action is _exploitation_ of current estimates. We however also
need to _explore_ the space of actions to improve our estimates.

--

An _$ε$-greedy_ method follows the greedy action with probability $1-ε$, and
chooses a uniformly random action with probability $ε$.

---
class: center
# K-armed Bandits

![:pdf 72%](e_greedy.pdf)

---
# K-armed Bandits

## Incremental Implementation

Let $Q\_n$ be an estimate using $n$ rewards $R\_1, \ldots, R\_n$.

$$\begin{aligned}
Q\_n &= \frac{1}{n} ∑\_{i=1}^n R\_i \\\
     &= \frac{1}{n} (R\_n + \frac{n-1}{n-1} ∑\_{i=1}^{n-1} R\_i) \\\
     &= \frac{1}{n} (R\_n + (n-1) Q\_{n-1}) \\\
     &= \frac{1}{n} (R\_n + n Q\_{n-1} - Q\_{n-1}) \\\
     &= Q\_{n-1} + \frac{1}{n}(R\_n - Q\_{n-1})
\end{aligned}$$

---
# K-armed Bandits

## Non-stationary Problems

Analogously to the solution obtained for a stationary problem, we consider
$$Q\_{n+1} = Q\_n + α(R\_{n+1} - Q\_n).$$

---
# Policies

A _policy_ $π$ computes a distribution of actions in a given state, i.e.,<br>
$π(a | s)$ corresponds to a probability of performing an action $a$ in state
$s$.

--

## Value Function
To evaluate a quality of policy, we define _value function_ $v\_π(s)$, or more
explicitly _state-value function_, as
$$v\_π(s) ≝ 𝔼\_π[G\_t | S\_t = s].$$

--
An _action-value function_ for policy $π$ is defined analogously as
$$q\_π(s, a) ≝ 𝔼\_π[G\_t | S\_t = s, A\_t = a].$$

--
It follows that
$$q\_π(s, a) = 𝔼\_π[R\_{t+1} + γv\_π(S_{t+1}) | S\_t = s, A\_t = a].$$

---
# Optimal Policy

As value functions define an partial ordering of policies ($π' ≥ π$ if
and only if for all states $s$, $v\_{π'}(s) ≥ v\_π(s)$), it can be proven
that there always exists an _optimal policy_ $π\_\*$, which is better or equal
to all other policies.

Intuitively, $π\_\*(s) = \argmax\nolimits\_a q\_\*(s, a)$.

--
## Policy Improvement Theorem

Let $π$ and $π'$ be any pair of deterministic policies, such that
$q\_π(s, π'(s)) ≥ v\_π(s)$. Then $π' ≥ π$, i.e., for all states $s$, $v\_{π'}(s) ≥ v\_π(s)$.

---
class: middle
# Monte Carlo Control

![:pdf 100%](monte_carlo.pdf)

---
# Temporal Difference Methods

![:pdf 100%](td_example.pdf)

--

![:pdf 100%](td_example_update.pdf)

---
# Temporal Difference Methods

A straightforward modification of Monte Carlo algorithm with constant-step
update and temporal difference is given by
$$Q(S\_t, A\_T) ← Q(S\_t, A\_t) + α[R\_{t+1} + γ Q(S\_{t+1}, A\_{t+1}) -Q(S\_t, A\_t)]$$
and is called _Sarsa_ ($S\_t, A\_t, R\_{t+1}, S\_{t+1}, A\_{t+1}$).

--
![:pdf 100%](sarsa.pdf)

---
# Q-learning

_Q-learning_ is another TD control algorithm by (Watkins, 1989), defined by
$$Q(S\_t, A\_T) ← Q(S\_t, A\_t) + α[R\_{t+1} + γ \max\_a Q(S\_{t+1}, a) -Q(S\_t, A\_t)].$$

--
![:pdf 100%](q_learning.pdf)

---
class: center
# Sarsa vs Q-learning

![:pdf 65%](sarsa_vs_q.pdf)

---
class: middle
# Double Q-learning

![:pdf 100%](double_q_example.pdf)

---
class: middle
# Double Q-learning

![:pdf 100%](double_q.pdf)
