<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<!-- saved from url=(0059)http://ufal.mff.cuni.cz/~hajic/courses/npfl067/assign1.html -->
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-2">
    
    <link href="./Assignment_1_Description_files/homepage.css" type="text/css" rel="stylesheet">
    <title>
	    Assignment 1
   </title>
  <style type="text/css">
:root #main-content > [style="padding:10px 0 0 0 !important;"]
{ display: none !important; }</style><style id="style-1-cropbar-clipper">/* Copyright 2014 Evernote Corporation. All rights reserved. */
.en-markup-crop-options {
    top: 18px !important;
    left: 50% !important;
    margin-left: -100px !important;
    width: 200px !important;
    border: 2px rgba(255,255,255,.38) solid !important;
    border-radius: 4px !important;
}

.en-markup-crop-options div div:first-of-type {
    margin-left: 0px !important;
}
</style></head>
  <body>
    <div class="main">
      <div class="language">
      </div>

      <h1>Assignment #1: PFL067 Statistical NLP</h1>
      <h2>Exploring Entropy and Language Modeling</h2>

<p>
Instructor: <a href="http://ufal.mff.cuni.cz/~hajic">Jan Hajic</a> &lt;<a href="mailto:hajic@ufal.mff.cuni.cz">hajic@ufal.mff.cuni.cz</a>&gt;<br>
TA: <a href="http://ufal.mff.cuni.cz/~pecina">Pavel Pecina</a> &lt;<a href="mailto:pecina@ufal.mff.cuni.cz">pecina@ufal.mff.cuni.cz</a>&gt;
</p>

Back to <a href="http://ufal.mff.cuni.cz/~hajic/courses/npfl067/syllabus.html">syllabus</a>.

<hr width="100%">
<h2>1. Entropy of a Text</h2>

In this experiment, you will determine the conditional entropy of the
word distribution in a text given the previous word.  To do this, you
will first have to compute
<i>P(i,j)</i>, 
which is the probability that at
any position in the text you will find the word <i>i</i> 
followed immediately
by the word <i>j</i>, and 
<i>P(j|i)</i>, 
which is the probability that if word <i>i</i>
occurs in the text then word <i>j</i> will follow.  Given these
probabilities, the conditional entropy of the word distribution in a
text given the previous word can then be computed as:

<p>
</p><center>
	<img src="./Assignment_1_Description_files/a1form1.jpg" alt="entropy formula for i,j">
</center>

<p> The perplexity is then computed simply as

</p><p>
</p><center>
	<font size="+1">PX(<i>P(J|I)</i>) = 2<sup><i><b>H(J|I)</b></i></sup>
</font>
</center>

<p>
Compute this conditional entropy and perplexity for 

</p><p>
<b>TEXTEN1.txt</b>

</p><p>
This file has every word on a separate line. (Punctuation is
considered a word, as in many other cases.) The <i>i,j</i> above will
also span sentence boundaries, where i is the last word of one
sentence and j is the first word of the following sentence (but
obviously, there will be a fullstop at the end of most sentences).

</p><p>
Next, you will mess up the text and measure how this alters the
conditional entropy.  For every character in the text, mess it up with
a likelihood of 10%.  If a character is chosen to be messed up, map
it into a randomly chosen character from the set of characters that
appear in the text.  Since there is some randomness to the outcome of
the experiment, run the experiment 10 times, each time measuring the
conditional entropy of the resulting text, and give the min, max, and
average entropy from these experiments.  Be sure to use srand to reset
the random number generator seed each time you run it.  Also, be sure
each time you are messing up the original text, and not a previously
messed up text.  Do the same experiment for mess up likelihoods of
5%, 1%, .1%, .01%, and .001%.

</p><p>Next, for every word in the text, mess it up with a likelihood of
10%.  If a word is chosen to be messed up, map it into a randomly
chosen word from the set of words that appear in the text.  Again run
the experiment 10 times, each time measuring the conditional entropy
of the resulting text, and give the min, max, and average entropy from
these experiments. Do the same experiment for mess up likelihoods of
5%, 1%, .1%, .01%, and .001%.

</p><p>Now do exactly the same for the file

</p><p>
<b>TEXTCZ1.txt</b>

</p><p>
which contains a similar amount of text in an unknown language
<em>(just FYI, that's Czech<a href="http://ufal.mff.cuni.cz/~hajic/courses/npfl067/assign1.html#czech"><sup>*</sup></a>)</em>

</p><p>Tabulate, graph and explain your results. Also try to explain the
differences between the two languages. To substantiate your
explanations, you might want to tabulate also the basic
characteristics of the two texts, such as the word count, number of
characters (total, per word), the frequency of the most frequent
words, the number of words with frequency 1, etc.

</p><p>
Attach your source code commented in such a way that it is sufficient
to read the comments to understand what you have done and how you have
done it.

</p><p>
Now assume two languages, L<sub>1</sub> and L<sub>2</sub> do not share
any vocabulary items, and that the conditional entropy as described
above of a text T<sub>1</sub> in language L<sub>1</sub> is E and that
the conditional entropy of a text T<sub>2</sub> in language
L<sub>2</sub> is also E.  Now make a new text by appending
T<sub>2</sub> to the end of T<sub>1</sub>.  Will the conditional
entropy of this new text be greater than, equal to, or less than E?
Explain. [This is a paper-and-pencil exercise of course!]


</p><h2>2. Cross-Entropy and Language Modeling</h2>

<p>
This task will show you the importance of smoothing for language
modeling, and in certain detail it lets you feel its effects.
</p><p>
First, you will have to prepare data: take the same texts as in the previous task, i.e. 

</p><p>
<b>TEXTEN1.txt</b> and
<b>TEXTCZ1.txt</b>

</p><p>
Prepare 3 datasets out of each: strip off the last 20,000 words and
call them the <b>Test Data</b>, then take off the last
40,000 words from what remains, and call them the <b>Heldout
Data</b>, and call the remaining data the <b>Training
Data</b>.

</p><p>
Here comes the coding: extract word counts from the <b>training
data</b> so that you are ready to compute unigram-, bigram- and
trigram-based probabilities from them; compute also the uniform
probability based on the vocabulary size. Remember (T being the text
size, and V the vocabulary size, i.e. the number of types - different
word forms found in the training text):

</p><p>
<font size="+1">
<i>p<sub>0</sub>(w<sub>i</sub>) = 1 / V</i> <br>

<i>p<sub>1</sub>(w<sub>i</sub>) = c<sub>1</sub>(w<sub>i</sub>) / T</i>  <br>

<i>p<sub>2</sub>(w<sub>i</sub>|w<sub>i-1</sub>) 
         = c<sub>2</sub>(w<sub>i-1</sub>,w<sub>i</sub>) / 
           c<sub>1</sub>(w<sub>i-1</sub>)  <br>
</i>
<i>p<sub>3</sub>(w<sub>i</sub>|w<sub>i-2</sub>,w<sub>i-1</sub>) 
         = c<sub>3</sub>(w<sub>i-2</sub>,w<sub>i-1</sub>,w<sub>i</sub>) / 
           c<sub>2</sub>(w<sub>i-2</sub>,w<sub>i-1</sub>)  <br>
</i>
</font>
 
</p><p>
Be careful; remember how to handle correctly the beginning and end of
the training data with respect to bigram and trigram counts. 

</p><p>
Now compute the four smoothing parameters (i.e. "coefficients",
"weights", "lambdas", "interpolation parameters" or whatever, for the
trigram, bigram, unigram and uniform distributions) from the
<b>heldout data</b> using the EM algorithm. [Then do the
same using the <b>training data</b> again: what smoothing
coefficients have you got? After answering this question, throw them
away!] Remember, the smoothed model has the following form:

</p><p>
<font size="+1">
	<i>p<sub>s</sub>(w<sub>i</sub>|w<sub>i-2</sub>,w<sub>i-1</sub>)</i> = 

	<i>l<sub>0</sub>p<sub>0</sub>(w<sub>i</sub>)</i>+

	<i>l<sub>1</sub>p<sub>1</sub>(w<sub>i</sub>)</i>+

	<i>l<sub>2</sub>p<sub>2</sub>(w<sub>i</sub>|w<sub>i-1</sub>)</i> + 

	<i>l<sub>3</sub>p<sub>3</sub>(w<sub>i</sub>|w<sub>i-2</sub>,w<sub>i-1</sub>),
</i>
</font>

</p><p>
where

</p><p>
<font size="+1">
<i>
&nbsp;&nbsp;&nbsp;&nbsp;l<sub>0</sub> + l<sub>1</sub> + l<sub>2</sub> + l<sub>3</sub> = 1.
</i>

</font>


</p><p>
And finally, compute the cross-entropy of the <b>test
data</b> using your newly built, smoothed language model. Now
tweak the smoothing parameters in the following way: add 10%, 20%,
30%, ..., 90%, 95% and 99% of the difference between the trigram
smoothing parameter and 1.0 to its value, discounting at the same the
remaining three parameters proportionally (remember, they have to sum
up to 1.0!!). Then set the trigram smoothing parameter to 90%, 80%,
70%, ... 10%, 0% of its value, boosting proportionally the other three
parameters, again to sum up to one. Compute the cross-entropy on the
<b>test data</b> for all these 22 cases (original + 11
trigram parameter increase + 10 trigram smoothing parameter
decrease). Tabulate, graph and explain what you have got. Also, try to
explain the differences between the two languages based on similar
statistics as in the Task No. 2, plus the "coverage" graph (defined as
the percentage of words in the <b>test data</b> which have been
seen in the <b>training data</b>).

</p><p>
Attach your source code commented in such a way that it is sufficient
to read the comments to understand what you have done and how you have
done it.

</p><hr width="100%">

<p><a name="czech"><sup>*</sup></a>
<em>If you want to see the accents correctly,
select ISO Latin 2 coding (charset=iso-8859-2) for viewing, but your
programs obviously will (should) work in any case (supposing they are 8-bit
clean). For those linguistically minded &amp; wishing to learn more about
the language, look <a href="http://www.czech-language.cz/">here</a>. We will be using
texts in this language often, albeit you will never be <u>required</u>
to learn it.)</em>










</p></div></body></html>