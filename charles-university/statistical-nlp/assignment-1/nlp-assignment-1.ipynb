{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Assignment #1: PFL067 Statistical NLP\n",
    "\n",
    "## Exploring Entropy and Language Modeling\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1. Entropy of a Text\n",
    "\n",
    "In this experiment, you will determine the conditional entropy of the word distribution in a text given the previous word. To do this, you will first have to compute P(i,j), which is the probability that at any position in the text you will find the word i followed immediately by the word j, and P(j|i), which is the probability that if word i occurs in the text then word j will follow. Given these probabilities, the conditional entropy of the word distribution in a text given the previous word can then be computed as:\n",
    "\n",
    "$$H(J|I) = -\\sum_{i \\in I, j \\in J} P(i,j) \\log_2 P(j|i)$$\n",
    "\n",
    "The perplexity is then computed simply as\n",
    "\n",
    "$$P_X(P(J|I)) = 2^{H(J|I)}$$\n",
    "\n",
    "Compute this conditional entropy and perplexity for `TEXTEN1.txt`\n",
    "\n",
    "This file has every word on a separate line. (Punctuation is considered a word, as in many other cases.) The i,j above will also span sentence boundaries, where i is the last word of one sentence and j is the first word of the following sentence (but obviously, there will be a fullstop at the end of most sentences).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections as c\n",
    "import math\n",
    "import random\n",
    "from numpy.random import RandomState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "random.seed(200)\n",
    "np.random.seed(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "english = './TEXTEN1.txt'\n",
    "czech = './TEXTCZ1.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def open_text(filename):\n",
    "    \"\"\"Reads a text line by line, applies light preprocessing, and returns a dataframe of each word\"\"\"\n",
    "    with open(filename) as f:\n",
    "        content = f.readlines()\n",
    "\n",
    "    text = pd.DataFrame(content, columns=['words'])\n",
    "    text.words = text.words.apply(lambda word: word.strip().lower())\n",
    "    \n",
    "    text['wordprev'] = text.words.shift(1).fillna('<s>')\n",
    "    text['wordprev2'] = text.wordprev.shift(1).fillna('<ss>')\n",
    "    \n",
    "    text['bigrams'] = list(zip(text.wordprev, text.words))\n",
    "    text['trigrams'] = list(zip(*[text.wordprev2, text.wordprev, text.words]))\n",
    "    \n",
    "    text = text.drop(['wordprev', 'wordprev2'], axis=1)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>trigrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when</td>\n",
       "      <td>(&lt;s&gt;, when)</td>\n",
       "      <td>(&lt;ss&gt;, &lt;s&gt;, when)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>on</td>\n",
       "      <td>(when, on)</td>\n",
       "      <td>(&lt;s&gt;, when, on)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>board</td>\n",
       "      <td>(on, board)</td>\n",
       "      <td>(when, on, board)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>h</td>\n",
       "      <td>(board, h)</td>\n",
       "      <td>(on, board, h)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.</td>\n",
       "      <td>(h, .)</td>\n",
       "      <td>(board, h, .)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>m</td>\n",
       "      <td>(., m)</td>\n",
       "      <td>(h, ., m)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>.</td>\n",
       "      <td>(m, .)</td>\n",
       "      <td>(., m, .)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>s</td>\n",
       "      <td>(., s)</td>\n",
       "      <td>(m, ., s)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>.</td>\n",
       "      <td>(s, .)</td>\n",
       "      <td>(., s, .)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>beagle</td>\n",
       "      <td>(., beagle)</td>\n",
       "      <td>(s, ., beagle)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    words      bigrams           trigrams\n",
       "0    when  (<s>, when)  (<ss>, <s>, when)\n",
       "1      on   (when, on)    (<s>, when, on)\n",
       "2   board  (on, board)  (when, on, board)\n",
       "3       h   (board, h)     (on, board, h)\n",
       "4       .       (h, .)      (board, h, .)\n",
       "5       m       (., m)          (h, ., m)\n",
       "6       .       (m, .)          (., m, .)\n",
       "7       s       (., s)          (m, ., s)\n",
       "8       .       (s, .)          (., s, .)\n",
       "9  beagle  (., beagle)     (s, ., beagle)"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open_text(english)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['when', 'on', 'board', 'h', '.', 'm', '.', 's', '.', 'beagle']"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(open_text(english).words)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def language_model(text):\n",
    "    \"\"\"Counts unigrams and bigrams in a dataframe\"\"\"\n",
    "    words = list(text.words)\n",
    "    word_counts = c.Counter(words)\n",
    "    num_words = sum(word_counts.values())\n",
    "    vocabulary = sorted(list(set(word_counts.keys())))\n",
    "\n",
    "    bigrams = list(text.bigrams)\n",
    "    bigram_counts = c.Counter(bigrams)\n",
    "    num_bigrams = sum(bigram_counts.values())\n",
    "    bigram_vocabulary = sorted(list(set(bigram_counts.keys())))\n",
    "    \n",
    "    unigram_model = words, word_counts, num_words, vocabulary\n",
    "    bigram_model = bigrams, bigram_counts, num_bigrams, bigram_vocabulary\n",
    "    \n",
    "    return unigram_model, bigram_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def Pword(unigram_model, W='', alpha=0.7):\n",
    "    \"\"\"Calculates the probability a word appears in a sentence\"\"\"\n",
    "    _, word_counts, num_words, vocabulary = unigram_model\n",
    "    return (word_counts[W] + alpha) / (num_words + alpha * len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def Pbigram(bigram_model, W='', Wprev='', alpha=0.7):\n",
    "    \"\"\"Calculates the probability a bigram appears in a sentence\"\"\"\n",
    "    _, bigram_counts, num_bigrams, bigram_vocabulary = bigram_model\n",
    "    return (bigram_counts[(Wprev, W)] + alpha) / (num_bigrams + alpha * len(bigram_vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# P(A|B) = P(A,B) / P(B)\n",
    "def Pwprev(models, W='', Wprev='', alpha=0.7):\n",
    "    \"\"\"Calculates the probability a word W proceeds a word Wprev\"\"\"\n",
    "    unigram_model, bigram_model = models\n",
    "    return Pbigram(bigram_model, W=W, Wprev=Wprev, alpha=alpha) / Pword(unigram_model, W=Wprev, alpha=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def entropy(models, bigrams, alpha=0.7):\n",
    "    \"\"\"Calculates the entropy from a list of bigrams\"\"\"\n",
    "    _, bigram_model = models\n",
    "    return - sum(Pbigram(bigram_model, W=W, Wprev=Wprev, alpha=alpha) \n",
    "                 * math.log(Pwprev(models, W=W, Wprev=Wprev, alpha=alpha), 2) for Wprev,W in bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def perplexity(models, bigrams, alpha=0.7):\n",
    "    \"\"\"Calculates the perplexity from a list of bigrams\"\"\"\n",
    "    return 2 ** entropy(models, bigrams, alpha=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def stats(filename, alpha=1e-5):\n",
    "    text = open_text(filename)\n",
    "    models = language_model(text)\n",
    "    words, word_counts, num_words, vocabulary = unigram_model\n",
    "    bigrams, bigram_counts, num_bigrams, bigram_vocabulary = bigram_model\n",
    "    \n",
    "    word_count = num_words\n",
    "    char_count = len([char for word in words for char in word])\n",
    "    most_frequent_words = word_counts.most_common()[:10]\n",
    "    num_words_freq_1 = sum(1 for key in word_counts if word_counts[key] == 1)\n",
    "    \n",
    "    H = entropy(models, bigram_vocabulary, alpha=alpha)\n",
    "    P = perplexity(models, bigram_vocabulary, alpha=alpha)\n",
    "\n",
    "    return (word_count, char_count, most_frequent_words, num_words_freq_1, H, P)s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(221098,\n",
       " 972917,\n",
       " [(',', 14721),\n",
       "  ('the', 13949),\n",
       "  ('of', 9400),\n",
       "  ('.', 5645),\n",
       "  ('and', 5601),\n",
       "  ('in', 5123),\n",
       "  ('to', 4583),\n",
       "  ('a', 3286),\n",
       "  ('that', 2667),\n",
       "  ('as', 2180)],\n",
       " 3165,\n",
       " 5.332007216327324,\n",
       " 40.280431033438646)"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats(english)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Next, you will mess up the text and measure how this alters the conditional entropy. For every character in the text, mess it up with a likelihood of 10%. If a character is chosen to be messed up, map it into a randomly chosen character from the set of characters that appear in the text. Since there is some randomness to the outcome of the experiment, run the experiment 10 times, each time measuring the conditional entropy of the resulting text, and give the min, max, and average entropy from these experiments. Be sure to use srand to reset the random number generator seed each time you run it. Also, be sure each time you are messing up the original text, and not a previously messed up text. Do the same experiment for mess up likelihoods of 5%, 1%, .1%, .01%, and .001%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def charset(text):\n",
    "    words = text.words\n",
    "    return sorted(list(set(char for word in words for char in word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!', '\"', '&', \"'\", '(', ')', ',', '.', '/', '0']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "charset(open_text(english))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def vocab_list(text):\n",
    "    words = text.words\n",
    "    return sorted(list(set(word for word in words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"', '&', '&c', '&e', '(', ')', ',', '.', '000', '1']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_list(open_text(english))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def perturb_char(word, charset, prob=0.1, seed=200):\n",
    "    \"\"\"Changes each character with given probability to a random character in the charset\"\"\"\n",
    "    return ''.join(np.random.choice(charset) if np.random.random() < prob else char for char in word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def perturb_word(word, vocabulary, prob=0.1, seed=200):\n",
    "    \"\"\"Changes a word with given probability to a random word in the vocabulary\"\"\"\n",
    "    return np.random.choice(vocabulary) if np.random.random() < prob else word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def perturb(text, charset, vocabulary, prob=0.1, seed=200):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    key = 'c' + str(prob)\n",
    "    text[key] = text.words.apply(lambda word: perturb_char(word, charset, prob, seed))\n",
    "    \n",
    "    key = 'w' + str(prob)\n",
    "    text[key] = text.words.apply(lambda word: perturb_word(word, vocabulary, prob, seed))\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def perturb_text(text):\n",
    "    chars = charset(text)\n",
    "    vocab = vocab_list(text)\n",
    "    \n",
    "    for prob in [0.1, 0.05, 0.01, 0.001, 0.0001, 0.00001]:\n",
    "        perturb(text, chars, vocab, prob=prob, seed=200)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>c0.1</th>\n",
       "      <th>w0.1</th>\n",
       "      <th>c0.05</th>\n",
       "      <th>w0.05</th>\n",
       "      <th>c0.01</th>\n",
       "      <th>w0.01</th>\n",
       "      <th>c0.001</th>\n",
       "      <th>w0.001</th>\n",
       "      <th>c0.0001</th>\n",
       "      <th>w0.0001</th>\n",
       "      <th>c1e-05</th>\n",
       "      <th>w1e-05</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when</td>\n",
       "      <td>when</td>\n",
       "      <td>procure</td>\n",
       "      <td>when</td>\n",
       "      <td>when</td>\n",
       "      <td>when</td>\n",
       "      <td>when</td>\n",
       "      <td>when</td>\n",
       "      <td>when</td>\n",
       "      <td>when</td>\n",
       "      <td>when</td>\n",
       "      <td>when</td>\n",
       "      <td>when</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>on</td>\n",
       "      <td>od</td>\n",
       "      <td>on</td>\n",
       "      <td>od</td>\n",
       "      <td>on</td>\n",
       "      <td>od</td>\n",
       "      <td>on</td>\n",
       "      <td>on</td>\n",
       "      <td>on</td>\n",
       "      <td>on</td>\n",
       "      <td>on</td>\n",
       "      <td>on</td>\n",
       "      <td>on</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>board</td>\n",
       "      <td>board</td>\n",
       "      <td>board</td>\n",
       "      <td>board</td>\n",
       "      <td>board</td>\n",
       "      <td>board</td>\n",
       "      <td>board</td>\n",
       "      <td>board</td>\n",
       "      <td>board</td>\n",
       "      <td>board</td>\n",
       "      <td>board</td>\n",
       "      <td>board</td>\n",
       "      <td>board</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>h</td>\n",
       "      <td>h</td>\n",
       "      <td>h</td>\n",
       "      <td>h</td>\n",
       "      <td>h</td>\n",
       "      <td>h</td>\n",
       "      <td>h</td>\n",
       "      <td>h</td>\n",
       "      <td>h</td>\n",
       "      <td>h</td>\n",
       "      <td>h</td>\n",
       "      <td>h</td>\n",
       "      <td>h</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>m</td>\n",
       "      <td>m</td>\n",
       "      <td>sharks</td>\n",
       "      <td>m</td>\n",
       "      <td>m</td>\n",
       "      <td>m</td>\n",
       "      <td>m</td>\n",
       "      <td>m</td>\n",
       "      <td>m</td>\n",
       "      <td>m</td>\n",
       "      <td>m</td>\n",
       "      <td>m</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>s</td>\n",
       "      <td>s</td>\n",
       "      <td>s</td>\n",
       "      <td>s</td>\n",
       "      <td>s</td>\n",
       "      <td>s</td>\n",
       "      <td>s</td>\n",
       "      <td>s</td>\n",
       "      <td>s</td>\n",
       "      <td>s</td>\n",
       "      <td>s</td>\n",
       "      <td>s</td>\n",
       "      <td>s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>beagle</td>\n",
       "      <td>beagle</td>\n",
       "      <td>beagle</td>\n",
       "      <td>beagle</td>\n",
       "      <td>beagle</td>\n",
       "      <td>beagle</td>\n",
       "      <td>beagle</td>\n",
       "      <td>beagle</td>\n",
       "      <td>beagle</td>\n",
       "      <td>beagle</td>\n",
       "      <td>beagle</td>\n",
       "      <td>beagle</td>\n",
       "      <td>beagle</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    words    c0.1     w0.1   c0.05   w0.05   c0.01   w0.01  c0.001  w0.001  \\\n",
       "0    when    when  procure    when    when    when    when    when    when   \n",
       "1      on      od       on      od      on      od      on      on      on   \n",
       "2   board   board    board   board   board   board   board   board   board   \n",
       "3       h       h        h       h       h       h       h       h       h   \n",
       "4       .       .        .       .       .       .       .       .       .   \n",
       "5       m       m   sharks       m       m       m       m       m       m   \n",
       "6       .       .        .       .       .       .       .       .       .   \n",
       "7       s       s        s       s       s       s       s       s       s   \n",
       "8       .       .        .       .       .       .       .       .       .   \n",
       "9  beagle  beagle   beagle  beagle  beagle  beagle  beagle  beagle  beagle   \n",
       "\n",
       "  c0.0001 w0.0001  c1e-05  w1e-05  \n",
       "0    when    when    when    when  \n",
       "1      on      on      on      on  \n",
       "2   board   board   board   board  \n",
       "3       h       h       h       h  \n",
       "4       .       .       .       .  \n",
       "5       m       m       m       m  \n",
       "6       .       .       .       .  \n",
       "7       s       s       s       s  \n",
       "8       .       .       .       .  \n",
       "9  beagle  beagle  beagle  beagle  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perturb_text(open_text(english))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def english_entropy_perturbed():\n",
    "    text = open_text(english)\n",
    "    models = language_model(text)\n",
    "    \n",
    "    perturb_text(text)\n",
    "    \n",
    "    arr = []\n",
    "    for col in text:\n",
    "        bigrams = bigram_list(text[col])\n",
    "        arr.append((entropy(models, bigrams), perplexity(models, bigrams)))\n",
    "\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(314.09587888960993, 3.566818908057433e+94),\n",
       " (185.48765745234837, 6.876203460900941e+55),\n",
       " (255.05703976648928, 6.023093083165798e+76),\n",
       " (243.63910016166415, 2.201290023108338e+73),\n",
       " (283.88301354564084, 2.8661725788689987e+85),\n",
       " (298.1834302044268, 5.783046728248773e+89),\n",
       " (307.7744255129285, 4.459990951824744e+92),\n",
       " (312.35498029286276, 1.0671325404028293e+94),\n",
       " (313.3189195607709, 2.08157947431838e+94),\n",
       " (313.89959453660543, 3.1131036616944084e+94),\n",
       " (314.05133788216057, 3.458381353349212e+94),\n",
       " (314.0766914800403, 3.519695349917011e+94),\n",
       " (314.0957355271436, 3.566464486273607e+94)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_entropy_perturbed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Questions\n",
    "1. Should we split the data into train/test sets?\n",
    "2. What entropy values should we be seeing so that I know I am on the right track?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
