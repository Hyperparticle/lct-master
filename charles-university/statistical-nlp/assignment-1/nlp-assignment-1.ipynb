{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment #1: PFL067 Statistical NLP\n",
    "\n",
    "## Exploring Entropy and Language Modeling\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Entropy of a Text\n",
    "\n",
    "In this experiment, you will determine the conditional entropy of the word distribution in a text given the previous word. To do this, you will first have to compute P(i,j), which is the probability that at any position in the text you will find the word i followed immediately by the word j, and P(j|i), which is the probability that if word i occurs in the text then word j will follow. Given these probabilities, the conditional entropy of the word distribution in a text given the previous word can then be computed as:\n",
    "\n",
    "$$H(J|I) = -\\sum_{i \\in I, j \\in J} P(i,j) \\log_2 P(j|i)$$\n",
    "\n",
    "The perplexity is then computed simply as\n",
    "\n",
    "$$P_X(P(J|I)) = 2^{H(J|I)}$$\n",
    "\n",
    "Compute this conditional entropy and perplexity for `TEXTEN1.txt`\n",
    "\n",
    "This file has every word on a separate line. (Punctuation is considered a word, as in many other cases.) The i,j above will also span sentence boundaries, where i is the last word of one sentence and j is the first word of the following sentence (but obviously, there will be a fullstop at the end of most sentences).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections as c\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "english = './TEXTEN1.txt'\n",
    "czech = './TEXTCZ1.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_text(filename):\n",
    "    \"\"\"Reads a text line by line, applies light preprocessing, and returns a dataframe of each word\"\"\"\n",
    "    with open(filename) as f:\n",
    "        content = f.readlines()\n",
    "\n",
    "    df = pd.DataFrame(content, columns=['words'])\n",
    "    df['words'] = df['words'].apply(lambda word: word.strip().lower())\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['when', 'on', 'board', 'h', '.', 'm', '.', 's', '.', 'beagle']"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(open_text(english)['words'])[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def language_model(words):\n",
    "    \"\"\"Counts unigrams and bigrams in a dataframe\"\"\"\n",
    "    word_counts = c.Counter(words)\n",
    "    num_words = sum(word_counts.values())\n",
    "    vocabulary = list(set(word_counts.keys()))\n",
    "\n",
    "    bigrams = list(zip(['<s>'] + words, words + ['</s>']))\n",
    "    bigram_counts = c.Counter(bigrams)\n",
    "    num_bigrams = sum(bigram_counts.values())\n",
    "    bigram_vocabulary = list(set(bigram_counts.keys()))\n",
    "    \n",
    "    unigram_model = words, word_counts, num_words, vocabulary\n",
    "    bigram_model = bigrams, bigram_counts, num_bigrams, bigram_vocabulary\n",
    "    \n",
    "    return (unigram_model, bigram_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pword(unigram_model, W='', alpha=0.7):\n",
    "    \"\"\"Calculates the probability a word appears in a sentence\"\"\"\n",
    "    _, word_counts, num_words, vocabulary = unigram_model\n",
    "    return (word_counts[W] + alpha) / (num_words + alpha * len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Pbigram(bigram_model, W='', Wprev='', alpha=0.7):\n",
    "    \"\"\"Calculates the probability a bigram appears in a sentence\"\"\"\n",
    "    _, bigram_counts, num_bigrams, bigram_vocabulary = bigram_model\n",
    "    return (bigram_counts[(Wprev, W)] + alpha) / (num_bigrams + alpha * len(bigram_vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P(A|B) = P(A,B) / P(B)\n",
    "def Pwprev(models, W='', Wprev=''):\n",
    "    \"\"\"Calculates the probability a word W proceeds a word Wprev\"\"\"\n",
    "    unigram_model, bigram_model = models\n",
    "    return Pbigram(bigram_model, W=W, Wprev=Wprev) / Pword(unigram_model, W=Wprev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def entropy(models, bigrams):\n",
    "    \"\"\"Calculates the entropy from a list of bigrams\"\"\"\n",
    "    _, bigram_model = models\n",
    "    return - sum(Pbigram(bigram_model, W=W, Wprev=Wprev) \n",
    "                 * math.log(Pwprev(models, W=W, Wprev=Wprev), 2) for Wprev,W in bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def perplexity(models, bigrams):\n",
    "    \"\"\"Calculates the perplexity from a list of bigrams\"\"\"\n",
    "    return 2 ** entropy(models, bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "def english_entropy():\n",
    "    text = open_text(english)\n",
    "    words = list(text['words'])\n",
    "    models = language_model(words)\n",
    "    _, bigram_model = models\n",
    "    bigrams, *_ = bigram_model\n",
    "\n",
    "    return entropy(models, bigrams), perplexity(models, bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(314.09587888960993, 3.566818908057433e+94)"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_entropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you will mess up the text and measure how this alters the conditional entropy. For every character in the text, mess it up with a likelihood of 10%. If a character is chosen to be messed up, map it into a randomly chosen character from the set of characters that appear in the text. Since there is some randomness to the outcome of the experiment, run the experiment 10 times, each time measuring the conditional entropy of the resulting text, and give the min, max, and average entropy from these experiments. Be sure to use srand to reset the random number generator seed each time you run it. Also, be sure each time you are messing up the original text, and not a previously messed up text. Do the same experiment for mess up likelihoods of 5%, 1%, .1%, .01%, and .001%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "def charset(text):\n",
    "    words = text['words']\n",
    "    return sorted(list(set(char for word in words for char in word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!', '\"', '&', \"'\", '(', ')', ',', '.', '/', '0']"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "charset(open_text(english))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def perturb_char(word, charset, prob=0.1, seed=200):\n",
    "    \"\"\"Changes each character with given probability to a random character in the charset\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    \n",
    "    arr = list(word)\n",
    "    for i,char in enumerate(arr):\n",
    "        if np.random.random() < prob:\n",
    "            arr[i] = np.random.choice(charset)\n",
    "            \n",
    "    return ''.join(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb(text, charset, prob=0.1, seed=200):\n",
    "    key = str(prob)\n",
    "    text[key] = text['words'].apply(lambda word: perturb_char(word, charset, prob, seed))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb_text(text):\n",
    "    for prob in [0.1, 0.05, 0.01, 0.001, 0.0001, 0.00001]:\n",
    "        perturb(text, charset(text), prob=prob)\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>0.1</th>\n",
       "      <th>0.05</th>\n",
       "      <th>0.01</th>\n",
       "      <th>0.001</th>\n",
       "      <th>0.0001</th>\n",
       "      <th>1e-05</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when</td>\n",
       "      <td>when</td>\n",
       "      <td>when</td>\n",
       "      <td>when</td>\n",
       "      <td>when</td>\n",
       "      <td>when</td>\n",
       "      <td>when</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>on</td>\n",
       "      <td>on</td>\n",
       "      <td>on</td>\n",
       "      <td>on</td>\n",
       "      <td>on</td>\n",
       "      <td>on</td>\n",
       "      <td>on</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>board</td>\n",
       "      <td>board</td>\n",
       "      <td>board</td>\n",
       "      <td>board</td>\n",
       "      <td>board</td>\n",
       "      <td>board</td>\n",
       "      <td>board</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>h</td>\n",
       "      <td>h</td>\n",
       "      <td>h</td>\n",
       "      <td>h</td>\n",
       "      <td>h</td>\n",
       "      <td>h</td>\n",
       "      <td>h</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>m</td>\n",
       "      <td>m</td>\n",
       "      <td>m</td>\n",
       "      <td>m</td>\n",
       "      <td>m</td>\n",
       "      <td>m</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>s</td>\n",
       "      <td>s</td>\n",
       "      <td>s</td>\n",
       "      <td>s</td>\n",
       "      <td>s</td>\n",
       "      <td>s</td>\n",
       "      <td>s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>beagle</td>\n",
       "      <td>beagld</td>\n",
       "      <td>beagld</td>\n",
       "      <td>beagld</td>\n",
       "      <td>beagle</td>\n",
       "      <td>beagle</td>\n",
       "      <td>beagle</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    words     0.1    0.05    0.01   0.001  0.0001   1e-05\n",
       "0    when    when    when    when    when    when    when\n",
       "1      on      on      on      on      on      on      on\n",
       "2   board   board   board   board   board   board   board\n",
       "3       h       h       h       h       h       h       h\n",
       "4       .       .       .       .       .       .       .\n",
       "5       m       m       m       m       m       m       m\n",
       "6       .       .       .       .       .       .       .\n",
       "7       s       s       s       s       s       s       s\n",
       "8       .       .       .       .       .       .       .\n",
       "9  beagle  beagld  beagld  beagld  beagle  beagle  beagle"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perturb_text(open_text(english))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "def english_entropy_perturbed():\n",
    "    text = open_text(english)\n",
    "    models = language_model(list(text['words']))\n",
    "    _, bigram_model = models\n",
    "    bigrams, *_ = bigram_model\n",
    "    \n",
    "    perturb_text(text)\n",
    "    \n",
    "    arr = []\n",
    "    ## TODO: isolate bigrams from LM\n",
    "    for words in text:\n",
    "        arr.append((entropy(models, bigrams), perplexity(models, bigrams)))\n",
    "\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(314.09587888960993, 3.566818908057433e+94),\n",
       " (314.09587888960993, 3.566818908057433e+94),\n",
       " (314.09587888960993, 3.566818908057433e+94),\n",
       " (314.09587888960993, 3.566818908057433e+94),\n",
       " (314.09587888960993, 3.566818908057433e+94),\n",
       " (314.09587888960993, 3.566818908057433e+94),\n",
       " (314.09587888960993, 3.566818908057433e+94)]"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_entropy_perturbed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "1. Should we split the data into train/test sets?\n",
    "2. What entropy values should we be seeing so that I know I am on the right track?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
