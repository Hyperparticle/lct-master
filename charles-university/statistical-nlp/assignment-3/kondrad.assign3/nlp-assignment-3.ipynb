{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Assignment #3: NPFL067 Statistical NLP II](http://ufal.mff.cuni.cz/~hajic/courses/npfl067/assign3.html)\n",
    "\n",
    "## Tagging\n",
    "\n",
    "### Author: Dan Kondratyuk\n",
    "\n",
    "### March 28, 2018\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Python notebook compares Brill's Tagger with a trigram HMM tagger.\n",
    "\n",
    "Code and explanation of results is fully viewable within this webpage.\n",
    "\n",
    "## Files\n",
    "\n",
    "- [index.html](./index.html) - Contains all veiwable code and a summary of results\n",
    "- [README.md](./README.md) - Instructions on how to run the code with Python\n",
    "- [nlp-assignment-3.ipynb](./nlp-assignment-3.ipynb) - Jupyter notebook where code can be run\n",
    "- [tag.py](./tag.py) - Contains HMM code for part 2\n",
    "- [requirements.txt](./requirements.txt) - Required python packages for running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Brill's Tagger & Tagger Evaluation\n",
    "\n",
    "> For this whole homework, use data found in `texten2.ptg`, `textcz2.ptg`\n",
    ">\n",
    "> In the following, \"the data\" refers to both English and Czech, as usual.\n",
    ">\n",
    "> Split the data in the following way: use last 40,000 words for testing (data S), and from the remaining data, use the last 20,000 for smoothing (data H, if any). Call the rest \"data T\" (training). \n",
    ">\n",
    "> Download Eric Brill's supervised tagger from [UFAL's course assignment space](http://ufal.mff.cuni.cz/~hajic/courses/npfl067/RULE_BASED_TAGGER_V.1.14.tar.gz). Install it (i.e., uncompress (gunzip), untar, and make).\n",
    ">\n",
    "> You might need to make some changes in his makefile of course (it's and OLD program, in this fast changing world...).\n",
    ">\n",
    "> After installation, get the data, train it on as much data from T as time allows (in the package, there is an extensive documentation on how to train it on new data), and evaluate on data S. Tabulate the results.\n",
    ">\n",
    "> Do cross-validation of the results: split the data into S', [H',] T' such that S' is the first 40,000 words, and T' is the last but the first 20,000 words from the rest. Train Eric Brill's tagger on T' (again, use as much data as time allows) and evaluate on S'. Again, tabulate the results.\n",
    ">\n",
    "> Do three more splits of your data (using the same formula: 40k/20k/the rest) in some way or another (as different as possible), and get another three sets of results. Compute the mean (average) accuracy and the standard deviation of the accuracy. Tabulate all results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.metrics import accuracy_score\n",
    "import itertools\n",
    "import dill as pickle\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm_notebook as tqdm, tnrange as trange\n",
    "\n",
    "from subprocess import call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_text(filename):\n",
    "    \"\"\"Reads a text line by line, applies light preprocessing, and returns an array of words and tags\"\"\"\n",
    "    with open(filename, encoding='iso-8859-2') as f:\n",
    "        content = f.readlines()\n",
    "    \n",
    "    preprocess = lambda word: tuple(word.strip().rsplit('/', 1))\n",
    "    \n",
    "    return [preprocess(word) for word in content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isplit(iterable, splitters):\n",
    "    # https://stackoverflow.com/a/4322780\n",
    "    return [list(g) for k,g in itertools.groupby(iterable, lambda x:x in splitters) if not k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_split(data, token=('###', '###')):\n",
    "    return isplit(data, (None, token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(words, start=0):\n",
    "    train, heldout, test = words[:start] + words[start+60_000:],  words[start+40_000:start+60_000], words[start:start+40_000]\n",
    "    return train, heldout, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_end(words):\n",
    "    train, heldout, test = words[:-60_000],  words[-60_000:-40_000], words[-40_000:]\n",
    "    return train, heldout, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_all(words):\n",
    "    return [\n",
    "        split_data_end(words),\n",
    "        split_data(words, start=40_000 * 0),\n",
    "        split_data(words, start=40_000 * 1),\n",
    "        split_data(words, start=40_000 * 2),\n",
    "        split_data(words, start=40_000 * 3)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from https://github.com/nltk/nltk/blob/a84b28ca26ea3ee53da4eaafc2bbf037847779bd/nltk/tbl/demo.py\n",
    "REGEXP_TAGGER = nltk.tag.RegexpTagger(\n",
    "    [(r'^-?[0-9]+(.[0-9]+)?$', 'CD'),   # cardinal numbers\n",
    "     (r'(The|the|A|a|An|an)$', 'AT'),   # articles\n",
    "     (r'.*able$', 'JJ'),                # adjectives\n",
    "     (r'.*ness$', 'NN'),                # nouns formed from adjectives\n",
    "     (r'.*ly$', 'RB'),                  # adverbs\n",
    "     (r'.*s$', 'NNS'),                  # plural nouns\n",
    "     (r'.*ing$', 'VBG'),                # gerunds\n",
    "     (r'.*ed$', 'VBD'),                 # past tense verbs\n",
    "     (r'.*', 'NN')                      # nouns (default)\n",
    "])\n",
    "templates = nltk.tag.brill.brill24()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brill_tagger(train, heldout, baseline_backoff_tagger=REGEXP_TAGGER, templates=templates, trace=0, \n",
    "                 ruleformat='str', max_rules=300, min_score=3, min_acc=None):\n",
    "    baseline_tagger = nltk.tag.UnigramTagger(heldout, backoff=baseline_backoff_tagger)\n",
    "    trainer = nltk.tag.BrillTaggerTrainer(baseline_tagger, templates, trace=trace, ruleformat=ruleformat)\n",
    "    tagger = trainer.train(train, max_rules, min_score, min_acc)\n",
    "    return tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_brill(split, i=0, lang='', load=False):\n",
    "    train, heldout, test = split\n",
    "    \n",
    "    filename = 'data/brill_tagger_{}_{}.pkl'.format(lang, i)\n",
    "    \n",
    "    print('Evaluating Brill Tagger {} [{}]'.format(lang, i))\n",
    "    if load:\n",
    "        with open(filename, 'rb') as f:\n",
    "            tagger = pickle.load(f)\n",
    "    else:\n",
    "        tagger = brill_tagger([train], [heldout])\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(tagger, f)\n",
    "    \n",
    "    return tagger.evaluate([test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(tagger_type, eval_func, langs=('en', 'cz')):\n",
    "    lang_d = {'en': ('English', splits_en), 'cz': ('Czech', splits_cz)}\n",
    "    \n",
    "    rows = []\n",
    "    for lang in langs:\n",
    "        language, splits = lang_d[lang]\n",
    "        accuracies = [eval_func(split, i, lang) for i,split in enumerate(splits)]\n",
    "        acc_str = ' '.join(['{0:0.1f}'.format(i * 100) for i in accuracies])\n",
    "        row = [tagger_type, language, acc_str, np.mean(accuracies) * 100, np.std(accuracies) * 100]\n",
    "        rows.append(row)\n",
    "\n",
    "    columns = ['type', 'language', 'accuracies', 'mean', 'standard_deviation']\n",
    "    results = pd.DataFrame(rows, columns=columns)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the texts into memory\n",
    "english = './data/texten2.ptg'\n",
    "czech = './data/textcz2.ptg'\n",
    "\n",
    "words_en = open_text(english)\n",
    "words_cz = open_text(czech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_en = split_all(words_en)\n",
    "splits_cz = split_all(words_cz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brill Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Brill Tagger en [0]\n",
      "Evaluating Brill Tagger en [1]\n",
      "Evaluating Brill Tagger en [2]\n",
      "Evaluating Brill Tagger en [3]\n",
      "Evaluating Brill Tagger en [4]\n",
      "Evaluating Brill Tagger cz [0]\n",
      "Evaluating Brill Tagger cz [1]\n",
      "Evaluating Brill Tagger cz [2]\n",
      "Evaluating Brill Tagger cz [3]\n",
      "Evaluating Brill Tagger cz [4]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>language</th>\n",
       "      <th>accuracies</th>\n",
       "      <th>mean</th>\n",
       "      <th>standard_deviation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Brill</td>\n",
       "      <td>English</td>\n",
       "      <td>90.4 90.7 90.6 90.2 87.6</td>\n",
       "      <td>89.924328</td>\n",
       "      <td>0.011614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Brill</td>\n",
       "      <td>Czech</td>\n",
       "      <td>61.9 70.2 64.7 64.0 65.7</td>\n",
       "      <td>65.288820</td>\n",
       "      <td>0.027449</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    type language                accuracies       mean  standard_deviation\n",
       "0  Brill  English  90.4 90.7 90.6 90.2 87.6  89.924328            0.011614\n",
       "1  Brill    Czech  61.9 70.2 64.7 64.0 65.7  65.288820            0.027449"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brill_results = evaluate('Brill', evaluate_brill)\n",
    "brill_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Unsupervised Learning: HMM Tagging\n",
    "\n",
    "> Use the datasets T, H, and S. Estimate the parameters of an HMM tagger using supervised learning off the T data (trigram and lower models for tags). Smooth (both the trigram tag model as well as the lexical model) in the same way as in Homework No. 1 (use data H). Evaluate your tagger on S, using the Viterbi algorithm.\n",
    ">\n",
    "> Now use only the first 10,000 words of T to estimate the initial (raw) parameters of the HMM tagging model. Strip off the tags from the remaining data T. Use the Baum-Welch algorithm to improve on the initial parameters. Smooth as usual. Evaluate your unsupervised HMM tagger and compare the results to the supervised HMM tagger.\n",
    ">\n",
    "> Tabulate and compare the results of the HMM tagger vs. the Brill's tagger. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tag import HMMTagger # See tag.py for implementation details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_hmm(split, i=0, lang='', unsupervised=False, load=False):\n",
    "    train, heldout, test = split\n",
    "    \n",
    "    name = 'unsupervised' if unsupervised else 'supervised'\n",
    "    filename = 'data/hmm_{}_tagger_{}_{}.pkl'.format(name, lang, i)\n",
    "    \n",
    "    if unsupervised:\n",
    "        labeled = sentence_split(train[:10_000])\n",
    "        unlabeled = [list(zip(*sentence))[0] for sentence in sentence_split(train[10_000:])]\n",
    "    else:\n",
    "        labeled = sentence_split(train)\n",
    "\n",
    "    words, tags = list(zip(*(train + heldout + test)))\n",
    "#     tag_set, word_set = list(set(tags)), list(set(words))\n",
    "    tag_set, word_set = set(nltk.bigrams(tags, pad_left=True)), set(words)\n",
    "\n",
    "    test = sentence_split(test)\n",
    "\n",
    "    print('Evaluating HMM {} {} [{}]'.format(name, lang, i))\n",
    "    if load:\n",
    "        with open(filename, 'rb') as f:\n",
    "            tagger = pickle.load(f)\n",
    "    else:\n",
    "        tagger = HMMTagger(labeled, tag_set, word_set)\n",
    "        tagger.smooth(heldout)\n",
    "        \n",
    "        if unsupervised:\n",
    "            tagger.train_unsupervised(unlabeled, max_iterations=5)\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(tagger, f)\n",
    "    \n",
    "    return tagger.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isplit(iterable, splitters):\n",
    "    # https://stackoverflow.com/a/4322780\n",
    "    return [list(g) for k,g in itertools.groupby(iterable, lambda x:x in splitters) if not k]\n",
    "\n",
    "def sentence_split(data, token=('###', '###')):\n",
    "    return [[(token[0], token[0])] + g for g in isplit(data, (None, token))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs=['en', 'cz']\n",
    "hmm_supervised_results = evaluate('HMM (supervised)', lambda split, i, lang: evaluate_hmm(split, i, lang, unsupervised=False), langs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs=['en', 'cz']\n",
    "hmm_unsupervised_results = evaluate('HMM (unsupervised)', lambda split, i, lang: evaluate_hmm(split, i, lang, unsupervised=True), langs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HMM Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>language</th>\n",
       "      <th>accuracies</th>\n",
       "      <th>mean</th>\n",
       "      <th>standard_deviation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HMM (supervised)</td>\n",
       "      <td>English</td>\n",
       "      <td>83.8 83.6 84.0 82.7 84.2</td>\n",
       "      <td>83.669745</td>\n",
       "      <td>0.502114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HMM (supervised)</td>\n",
       "      <td>Czech</td>\n",
       "      <td>55.3 60.5 57.9 56.6 56.0</td>\n",
       "      <td>57.269441</td>\n",
       "      <td>1.839133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               type language                accuracies       mean  \\\n",
       "0  HMM (supervised)  English  83.8 83.6 84.0 82.7 84.2  83.669745   \n",
       "1  HMM (supervised)    Czech  55.3 60.5 57.9 56.6 56.0  57.269441   \n",
       "\n",
       "   standard_deviation  \n",
       "0            0.502114  \n",
       "1            1.839133  "
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm_supervised_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>language</th>\n",
       "      <th>accuracies</th>\n",
       "      <th>mean</th>\n",
       "      <th>standard_deviation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HMM (unsupervised)</td>\n",
       "      <td>English</td>\n",
       "      <td>81.1 79.9 80.6 80.1 80.9</td>\n",
       "      <td>80.515292</td>\n",
       "      <td>0.460171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HMM (unsupervised)</td>\n",
       "      <td>Czech</td>\n",
       "      <td>47.7 62.9 54.0 50.6 48.6</td>\n",
       "      <td>52.788909</td>\n",
       "      <td>5.517085</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 type language                accuracies       mean  \\\n",
       "0  HMM (unsupervised)  English  81.1 79.9 80.6 80.1 80.9  80.515292   \n",
       "1  HMM (unsupervised)    Czech  47.7 62.9 54.0 50.6 48.6  52.788909   \n",
       "\n",
       "   standard_deviation  \n",
       "0            0.460171  \n",
       "1            5.517085  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm_unsupervised_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the final results of all taggers evaluated in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>language</th>\n",
       "      <th>accuracies</th>\n",
       "      <th>mean</th>\n",
       "      <th>standard_deviation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Brill</td>\n",
       "      <td>English</td>\n",
       "      <td>90.4 90.7 90.6 90.2 87.6</td>\n",
       "      <td>89.924328</td>\n",
       "      <td>0.011614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Brill</td>\n",
       "      <td>Czech</td>\n",
       "      <td>61.9 70.2 64.7 64.0 65.7</td>\n",
       "      <td>65.288820</td>\n",
       "      <td>0.027449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HMM (supervised)</td>\n",
       "      <td>English</td>\n",
       "      <td>83.8 83.6 84.0 82.7 84.2</td>\n",
       "      <td>83.669745</td>\n",
       "      <td>0.502114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HMM (supervised)</td>\n",
       "      <td>Czech</td>\n",
       "      <td>55.3 60.5 57.9 56.6 56.0</td>\n",
       "      <td>57.269441</td>\n",
       "      <td>1.839133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HMM (unsupervised)</td>\n",
       "      <td>English</td>\n",
       "      <td>81.1 79.9 80.6 80.1 80.9</td>\n",
       "      <td>80.515292</td>\n",
       "      <td>0.460171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HMM (unsupervised)</td>\n",
       "      <td>Czech</td>\n",
       "      <td>47.7 62.9 54.0 50.6 48.6</td>\n",
       "      <td>52.788909</td>\n",
       "      <td>5.517085</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 type language                accuracies       mean  \\\n",
       "0  Brill  English  90.4 90.7 90.6 90.2 87.6  89.924328            0.011614\n",
       "1  Brill    Czech  61.9 70.2 64.7 64.0 65.7  65.288820            0.027449\n",
       "2  HMM (supervised)  English  83.8 83.6 84.0 82.7 84.2  83.669745   \n",
       "3  HMM (supervised)    Czech  55.3 60.5 57.9 56.6 56.0  57.269441   \n",
       "4  HMM (unsupervised)  English  81.1 79.9 80.6 80.1 80.9  80.515292   \n",
       "5  HMM (unsupervised)    Czech  47.7 62.9 54.0 50.6 48.6  52.788909   \n",
       "\n",
       "   standard_deviation  \n",
       "0            0.460171  \n",
       "1            5.517085  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat(brill_results, hmm_supervised_results, hmm_unsupervised_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all cases, Czech POS tagging performs much worse than English POS tagging. The standard deviation among tag accuracies is higher as well. This can be attributed to two primary causes:\n",
    "\n",
    "1. Czech has many more types of tags (>1500) versus English (<50). This means the potential to get an incorrect answer is much higher.\n",
    "2. OOV issues prevent many words from being observed in the training data. Czech has rich morphology encoded as inflections in each word, which in turn expand the size of the vocabulary exponentially. This means that the Czech tagger is much more likely to encounter words it has never seen before, thereby making it difficult to choose the correct tag for the word.\n",
    "\n",
    "We see that, overall, Brill's tagger performs the best on both English and Czech, with a sizable lead on the HMM tagger.\n",
    "\n",
    "The supervised HMM comes in 2nd, while the unsupervised HMM comes last. This is as expected, since the supervised HMM was trained with labeled words on the entire training set, while the unsupervised was trained on just the first 10,000 labeled examples and used Baum-Welch to train on the remaining unlabeled set. While Baum-Welch can clue in on the distribution of observed words and update its internal model accordingly, this provides less information to the model than if both observed words and labels are avalable. \n",
    "\n",
    "More surprisingly however, the difference between supervised and unsupervised HMM approaches not very large. Despite not observing most of the hidden states in the training set, the unsupervised HMM can still model the distribution quite well, suffering only a couple percentage points in English. This drop is more significant in Czech, as there are likely more unobserved tags in Czech due to its rich morphlogical tags."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
