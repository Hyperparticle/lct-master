{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Assignment #3: NPFL067 Statistical NLP II](http://ufal.mff.cuni.cz/~hajic/courses/npfl067/assign3.html)\n",
    "\n",
    "## Tagging\n",
    "\n",
    "### Author: Dan Kondratyuk\n",
    "\n",
    "### March 2, 2018\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code and explanation of results is fully viewable within this webpage.\n",
    "\n",
    "## Files\n",
    "\n",
    "- [index.html](./index.html) - Contains all veiwable code and a summary of results\n",
    "- [README.md](./README.md) - Instructions on how to run the code with Python\n",
    "- [nlp-assignment-3.ipynb](./nlp-assignment-1.ipynb) - Jupyter notebook where code can be run\n",
    "- [requirements.txt](./requirements.txt) - Required python packages for running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Brill's Tagger & Tagger Evaluation\n",
    "\n",
    ">  For this whole homework, use data found in `texten2.ptg`, `textcz2.ptg`\n",
    "\n",
    "> In the following, \"the data\" refers to both English and Czech, as usual.\n",
    "\n",
    "> Split the data in the following way: use last 40,000 words for testing (data S), and from the remaining data, use the last 20,000 for smoothing (data H, if any). Call the rest \"data T\" (training). \n",
    "\n",
    "> Download Eric Brill's supervised tagger from [UFAL's course assignment space](http://ufal.mff.cuni.cz/~hajic/courses/npfl067/RULE_BASED_TAGGER_V.1.14.tar.gz). Install it (i.e., uncompress (gunzip), untar, and make).\n",
    "\n",
    "> You might need to make some changes in his makefile of course (it's and OLD program, in this fast changing world...).\n",
    "\n",
    "> After installation, get the data, train it on as much data from T as time allows (in the package, there is an extensive documentation on how to train it on new data), and evaluate on data S. Tabulate the results.\n",
    "\n",
    "> Do cross-validation of the results: split the data into S', [H',] T' such that S' is the first 40,000 words, and T' is the last but the first 20,000 words from the rest. Train Eric Brill's tagger on T' (again, use as much data as time allows) and evaluate on S'. Again, tabulate the results.\n",
    "\n",
    "> Do three more splits of your data (using the same formula: 40k/20k/the rest) in some way or another (as different as possible), and get another three sets of results. Compute the mean (average) accuracy and the standard deviation of the accuracy. Tabulate all results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections as c\n",
    "import nltk\n",
    "from nltk.tag import BrillTaggerTrainer, RegexpTagger, UnigramTagger\n",
    "from nltk.tag.brill import brill24\n",
    "from nltk.tag import hmm\n",
    "from sklearn.metrics import accuracy_score\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isplit(iterable,splitters):\n",
    "    return [list(g) for k,g in itertools.groupby(iterable,lambda x:x in splitters) if not k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_text(filename):\n",
    "    \"\"\"Reads a text line by line, applies light preprocessing, and returns an array of words and tags\"\"\"\n",
    "    with open(filename, encoding='iso-8859-2') as f:\n",
    "        content = f.readlines()\n",
    "        \n",
    "    preprocess = (word.strip().rsplit('/', 1) for word in content)\n",
    "    \n",
    "    return isplit(preprocess, (None, '###/###'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_tags(words):\n",
    "    return [tuple(word.rsplit('/', 1)) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_tags(words):\n",
    "    return [word.rsplit('/', 1)[0] for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the texts into memory\n",
    "english = './data/texten2.ptg'\n",
    "czech = './data/textcz2.ptg'\n",
    "\n",
    "words_en = open_text(english)\n",
    "words_cz = open_text(czech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(words, start=0):\n",
    "    test, heldout, train = words[:start+40_000],  words[start+40_000:start+60_000], words[start+60_000:]\n",
    "    return train, heldout, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_end(words):\n",
    "    test, heldout, train = words[40_000:],  words[40_000:60_000], words[:60_000]\n",
    "    return train, heldout, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_all(words):\n",
    "    return [\n",
    "        split_data_end(words),\n",
    "        split_data(words, start=60_000 * 0),\n",
    "        split_data(words, start=60_000 * 1),\n",
    "        split_data(words, start=60_000 * 2),\n",
    "        split_data(words, start=60_000 * 3)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from https://github.com/nltk/nltk/blob/a84b28ca26ea3ee53da4eaafc2bbf037847779bd/nltk/tbl/demo.py\n",
    "REGEXP_TAGGER = RegexpTagger(\n",
    "    [(r'^-?[0-9]+(.[0-9]+)?$', 'CD'),   # cardinal numbers\n",
    "     (r'(The|the|A|a|An|an)$', 'AT'),   # articles\n",
    "     (r'.*able$', 'JJ'),                # adjectives\n",
    "     (r'.*ness$', 'NN'),                # nouns formed from adjectives\n",
    "     (r'.*ly$', 'RB'),                  # adverbs\n",
    "     (r'.*s$', 'NNS'),                  # plural nouns\n",
    "     (r'.*ing$', 'VBG'),                # gerunds\n",
    "     (r'.*ed$', 'VBD'),                 # past tense verbs\n",
    "     (r'.*', 'NN')                      # nouns (default)\n",
    "])\n",
    "templates = brill24()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brill_tagger(train, heldout, baseline_backoff_tagger=REGEXP_TAGGER, templates=templates, trace=0, \n",
    "                 ruleformat='str', max_rules=300, min_score=3, min_acc=None):\n",
    "    baseline_tagger = UnigramTagger(heldout, backoff=baseline_backoff_tagger)\n",
    "    trainer = BrillTaggerTrainer(baseline_tagger, templates, trace=trace, ruleformat=ruleformat)\n",
    "    tagger = trainer.train(train, max_rules, min_score, min_acc)\n",
    "    return tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(split, i=0):\n",
    "    train, heldout, test = [split_tags(s) for s in split]\n",
    "    \n",
    "    print('Evaluating Brill Tagger [{}]'.format(i))\n",
    "    tagger = brill_tagger([train], [heldout])\n",
    "    \n",
    "    test_words = [w for w,_ in test]\n",
    "    predicted_tags = [t for _,t in tagger.tag(test_words)]\n",
    "    true_tags = [t for _,t in test]\n",
    "    return accuracy_score(true_tags, predicted_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_en = split_all(words_en)\n",
    "splits_cz = split_all(words_cz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English\n",
      "Evaluating Brill Tagger [0]\n",
      "Evaluating Brill Tagger [1]\n",
      "Evaluating Brill Tagger [2]\n",
      "Evaluating Brill Tagger [3]\n",
      "Evaluating Brill Tagger [4]\n",
      "Czech\n",
      "Evaluating Brill Tagger [0]\n",
      "Evaluating Brill Tagger [1]\n",
      "Evaluating Brill Tagger [2]\n",
      "Evaluating Brill Tagger [3]\n",
      "Evaluating Brill Tagger [4]\n"
     ]
    }
   ],
   "source": [
    "print('English')\n",
    "accuracies_en = [evaluate(split, i) for i,split in enumerate(splits_en)]\n",
    "print('Czech')\n",
    "accuracies_cz = [evaluate(split, i) for i,split in enumerate(splits_cz)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language</th>\n",
       "      <th>Accuracies</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Standard Deviation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>English</td>\n",
       "      <td>90.7 92.8 92.0 90.6 87.6</td>\n",
       "      <td>90.769112</td>\n",
       "      <td>0.017659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Czech</td>\n",
       "      <td>64.7 80.4 77.5 76.3 75.0</td>\n",
       "      <td>74.796807</td>\n",
       "      <td>0.053395</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Language                Accuracies       Mean  Standard Deviation\n",
       "0  English  90.7 92.8 92.0 90.6 87.6  90.769112            0.017659\n",
       "1    Czech  64.7 80.4 77.5 76.3 75.0  74.796807            0.053395"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_str_en = ' '.join(['{0:0.1f}'.format(i * 100) for i in accuracies_en])\n",
    "acc_str_cz = ' '.join(['{0:0.1f}'.format(i * 100) for i in accuracies_cz])\n",
    "\n",
    "row_en = ['English', acc_str_en, np.mean(accuracies_en) * 100, np.std(accuracies_en)]\n",
    "row_cz = ['Czech',   acc_str_cz, np.mean(accuracies_cz) * 100, np.std(accuracies_cz)]\n",
    "\n",
    "columns = ['Language', 'Accuracies', 'Mean', 'Standard Deviation']\n",
    "brill_results = pd.DataFrame([row_en, row_cz], columns=columns)\n",
    "brill_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_brill(split):\n",
    "#     train, heldout, test = split\n",
    "    \n",
    "#     tagged_all = ' '.join(train + heldout)\n",
    "#     tagged_1 = ' '.join(train)\n",
    "#     tagged_2 = ' '.join(heldout)\n",
    "    \n",
    "#     untagged_1 = ' '.join(strip_tags(train))\n",
    "#     untagged_2 = ' '.join(strip_tags(heldout))\n",
    "    \n",
    "#     with open('./data/TAGGED-CORPUS-ENTIRE', 'w', encoding='iso-8859-2') as f: f.write(tagged_all)\n",
    "#     with open('./data/TAGGED-CORPUS', 'w', encoding='iso-8859-2') as f: f.write(tagged_1)\n",
    "#     with open('./data/TAGGED-CORPUS-2', 'w', encoding='iso-8859-2') as f: f.write(tagged_2)\n",
    "#     with open('./data/UNTAGGED-CORPUS', 'w', encoding='iso-8859-2') as f: f.write(untagged_1)\n",
    "#     with open('./data/UNTAGGED-CORPUS-2', 'w', encoding='iso-8859-2') as f: f.write(untagged_2)\n",
    "\n",
    "# save_brill(splits_en[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Unsupervised Learning: HMM Tagging\n",
    "\n",
    "> Use the datasets T, H, and S. Estimate the parameters of an HMM tagger using supervised learning off the T data (trigram and lower models for tags). Smooth (both the trigram tag model as well as the lexical model) in the same way as in Homework No. 1 (use data H). Evaluate your tagger on S, using the Viterbi algorithm.\n",
    "\n",
    "> Now use only the first 10,000 words of T to estimate the initial (raw) parameters of the HMM tagging model. Strip off the tags from the remaining data T. Use the Baum-Welch algorithm to improve on the initial parameters. Smooth as usual. Evaluate your unsupervised HMM tagger and compare the results to the supervised HMM tagger.\n",
    "\n",
    "> Tabulate and compare the results of the HMM tagger vs. the Brill's tagger. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(split, i=0):\n",
    "    train, heldout, test = [split_tags(s) for s in split]\n",
    "    \n",
    "    print('Evaluating HMM [{}]'.format(i))\n",
    "    trainer = hmm.HiddenMarkovModelTrainer()\n",
    "    tagger = trainer.train_supervised([train])\n",
    "    \n",
    "    return tagger.evaluate([test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_en = split_all(words_en)\n",
    "splits_cz = split_all(words_cz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, heldout, test = [split_tags(s) for s in splits_en[0]]\n",
    "\n",
    "trainer = hmm.HiddenMarkovModelTrainer()\n",
    "tagger = trainer.train_supervised([train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ConditionalProbDist' object has no attribute 'freqdist'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-804f70cdea8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfreqdist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'ConditionalProbDist' object has no attribute 'freqdist'"
     ]
    }
   ],
   "source": [
    "tagger._outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = test[:1000]\n",
    "\n",
    "test_words = [w for w,_ in data]\n",
    "predicted_tags = [t for _,t in tagger.tag(test_words)]\n",
    "true_tags = [t for _,t in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['on',\n",
       " 'behalf',\n",
       " 'of',\n",
       " 'the',\n",
       " 'syndicates',\n",
       " '.',\n",
       " '###',\n",
       " 'It',\n",
       " 'could',\n",
       " 'take',\n",
       " 'six',\n",
       " 'months',\n",
       " 'for',\n",
       " 'a',\n",
       " 'claim',\n",
       " 'to',\n",
       " 'be',\n",
       " 'paid',\n",
       " '.',\n",
       " '###',\n",
       " '``',\n",
       " 'The',\n",
       " 'system',\n",
       " ',',\n",
       " \"''\",\n",
       " 'says',\n",
       " 'Nicholas',\n",
       " 'Samengo-Turner',\n",
       " ',',\n",
       " 'a',\n",
       " 'Lloyd',\n",
       " \"'s\",\n",
       " 'broker',\n",
       " 'who',\n",
       " 'left',\n",
       " 'the',\n",
       " 'exchange',\n",
       " 'in',\n",
       " '1985',\n",
       " ',',\n",
       " '``',\n",
       " 'is',\n",
       " 'so',\n",
       " 'ludicrously',\n",
       " 'unprofessional',\n",
       " 'it',\n",
       " 'drives',\n",
       " 'you',\n",
       " 'mad',\n",
       " '.',\n",
       " \"''\",\n",
       " '###',\n",
       " 'Some',\n",
       " 'maintain',\n",
       " 'underwriters',\n",
       " 'also',\n",
       " 'have',\n",
       " 'been',\n",
       " 'inept',\n",
       " '.',\n",
       " '###',\n",
       " 'John',\n",
       " 'Wetherell',\n",
       " ',',\n",
       " 'a',\n",
       " 'Lloyd',\n",
       " \"'s\",\n",
       " 'underwriter',\n",
       " ',',\n",
       " 'says',\n",
       " 'he',\n",
       " 'and',\n",
       " 'his',\n",
       " 'fellow',\n",
       " 'underwriters',\n",
       " 'underestimated',\n",
       " 'by',\n",
       " 'as',\n",
       " 'much',\n",
       " 'as',\n",
       " '50',\n",
       " '%',\n",
       " 'the',\n",
       " 'premiums',\n",
       " 'they',\n",
       " 'should',\n",
       " 'have',\n",
       " 'charged',\n",
       " 'for',\n",
       " 'property',\n",
       " 'risks',\n",
       " 'from',\n",
       " '1980',\n",
       " 'to',\n",
       " '1985',\n",
       " '.',\n",
       " '###',\n",
       " '``',\n",
       " 'How',\n",
       " 'unprofessional',\n",
       " 'we',\n",
       " 'must',\n",
       " 'have',\n",
       " 'appeared',\n",
       " 'to',\n",
       " 'the',\n",
       " 'outside',\n",
       " 'world',\n",
       " '--',\n",
       " 'how',\n",
       " 'incompetent',\n",
       " 'at',\n",
       " 'risk',\n",
       " 'assessment',\n",
       " 'and',\n",
       " 'evaluation',\n",
       " ',',\n",
       " \"''\",\n",
       " 'he',\n",
       " 'says',\n",
       " '.',\n",
       " '###',\n",
       " 'Lloyd',\n",
       " \"'s\",\n",
       " 'officials',\n",
       " 'decline',\n",
       " 'to',\n",
       " 'comment',\n",
       " 'on',\n",
       " 'the',\n",
       " 'matter',\n",
       " '.',\n",
       " '###',\n",
       " 'More',\n",
       " 'recently',\n",
       " ',',\n",
       " 'property',\n",
       " 'rates',\n",
       " 'have',\n",
       " 'increased',\n",
       " '.',\n",
       " '###',\n",
       " 'Many',\n",
       " 'at',\n",
       " 'Lloyd',\n",
       " \"'s\",\n",
       " 'expect',\n",
       " 'the',\n",
       " 'San',\n",
       " 'Francisco',\n",
       " 'earthquake',\n",
       " 'will',\n",
       " 'cause',\n",
       " 'the',\n",
       " 'industry',\n",
       " 'to',\n",
       " 'boost',\n",
       " 'rates',\n",
       " 'even',\n",
       " 'further',\n",
       " '.',\n",
       " '###',\n",
       " 'But',\n",
       " 'it',\n",
       " 'will',\n",
       " 'be',\n",
       " 'years',\n",
       " 'before',\n",
       " 'it',\n",
       " 'is',\n",
       " 'clear',\n",
       " 'whether',\n",
       " 'higher',\n",
       " 'rates',\n",
       " 'will',\n",
       " 'offset',\n",
       " 'the',\n",
       " 'payouts',\n",
       " 'for',\n",
       " 'such',\n",
       " 'disasters',\n",
       " '.',\n",
       " '###',\n",
       " 'The',\n",
       " 'magnitude',\n",
       " 'of',\n",
       " 'the',\n",
       " 'exchange',\n",
       " \"'s\",\n",
       " 'problems',\n",
       " 'may',\n",
       " 'not',\n",
       " 'become',\n",
       " 'known',\n",
       " 'for',\n",
       " 'some',\n",
       " 'time',\n",
       " 'because',\n",
       " 'of',\n",
       " 'Lloyd',\n",
       " \"'s\",\n",
       " 'practice',\n",
       " 'of',\n",
       " 'leaving',\n",
       " 'the',\n",
       " 'books',\n",
       " 'open',\n",
       " 'for',\n",
       " 'three',\n",
       " 'years',\n",
       " 'to',\n",
       " 'allow',\n",
       " 'for',\n",
       " 'the',\n",
       " 'settlement',\n",
       " 'of',\n",
       " 'claims',\n",
       " '.',\n",
       " '###',\n",
       " 'Lloyd',\n",
       " \"'s\",\n",
       " 'only',\n",
       " 'recently',\n",
       " 'reported',\n",
       " 'its',\n",
       " 'financial',\n",
       " 'results',\n",
       " 'for',\n",
       " '1986',\n",
       " '.',\n",
       " '###',\n",
       " 'That',\n",
       " 'year',\n",
       " ',',\n",
       " 'it',\n",
       " 'posted',\n",
       " 'record',\n",
       " 'pretax',\n",
       " 'profit',\n",
       " 'of',\n",
       " '#',\n",
       " '650',\n",
       " 'million',\n",
       " ',',\n",
       " 'a',\n",
       " 'gain',\n",
       " 'it',\n",
       " 'attributes',\n",
       " 'to',\n",
       " 'higher',\n",
       " 'rates',\n",
       " 'and',\n",
       " 'fewer',\n",
       " 'claims',\n",
       " '.',\n",
       " '###',\n",
       " 'But',\n",
       " 'Mr.',\n",
       " 'Lawrence',\n",
       " 'says',\n",
       " 'reported',\n",
       " 'profit',\n",
       " 'will',\n",
       " 'be',\n",
       " 'down',\n",
       " 'in',\n",
       " '1987',\n",
       " ',',\n",
       " '1988',\n",
       " 'and',\n",
       " '1989',\n",
       " ',',\n",
       " 'though',\n",
       " 'he',\n",
       " 'declines',\n",
       " 'to',\n",
       " 'specify',\n",
       " 'how',\n",
       " 'steep',\n",
       " 'the',\n",
       " 'decline',\n",
       " 'will',\n",
       " 'be',\n",
       " '.',\n",
       " '###',\n",
       " 'Insurance',\n",
       " 'analysts',\n",
       " 'say',\n",
       " 'the',\n",
       " 'exchange',\n",
       " \"'s\",\n",
       " 'downturn',\n",
       " 'in',\n",
       " 'profitability',\n",
       " 'is',\n",
       " 'likely',\n",
       " 'to',\n",
       " 'be',\n",
       " 'aggravated',\n",
       " 'by',\n",
       " 'more',\n",
       " 'than',\n",
       " '$',\n",
       " '600',\n",
       " 'million',\n",
       " 'in',\n",
       " 'aviation',\n",
       " 'losses',\n",
       " '-LRB-',\n",
       " 'including',\n",
       " 'the',\n",
       " '1988',\n",
       " 'Pan',\n",
       " 'Am',\n",
       " 'airline',\n",
       " 'disaster',\n",
       " 'over',\n",
       " 'Lockerbie',\n",
       " ',',\n",
       " 'Scotland',\n",
       " '-RRB-',\n",
       " 'and',\n",
       " 'a',\n",
       " 'still-uncalculated',\n",
       " 'chunk',\n",
       " 'of',\n",
       " 'claims',\n",
       " 'from',\n",
       " 'September',\n",
       " \"'s\",\n",
       " 'Hurricane',\n",
       " 'Hugo',\n",
       " '.',\n",
       " '###',\n",
       " 'Lloyd',\n",
       " \"'s\",\n",
       " 'says',\n",
       " 'the',\n",
       " 'departures',\n",
       " 'of',\n",
       " 'names',\n",
       " 'is',\n",
       " \"n't\",\n",
       " 'likely',\n",
       " 'to',\n",
       " 'hurt',\n",
       " 'its',\n",
       " 'underwriting',\n",
       " 'capacity',\n",
       " ',',\n",
       " 'currently',\n",
       " 'about',\n",
       " '#',\n",
       " '11',\n",
       " 'billion',\n",
       " '.',\n",
       " '###',\n",
       " 'Mr.',\n",
       " 'Lawrence',\n",
       " 'says',\n",
       " 'the',\n",
       " 'drain',\n",
       " 'of',\n",
       " 'funds',\n",
       " 'has',\n",
       " 'been',\n",
       " 'offset',\n",
       " 'by',\n",
       " 'an',\n",
       " 'increase',\n",
       " 'in',\n",
       " 'investments',\n",
       " 'by',\n",
       " 'the',\n",
       " 'remaining',\n",
       " 'names',\n",
       " '.',\n",
       " '###',\n",
       " 'Meanwhile',\n",
       " ',',\n",
       " 'the',\n",
       " 'exchange',\n",
       " 'has',\n",
       " 'been',\n",
       " 'trying',\n",
       " 'to',\n",
       " 'lower',\n",
       " 'costs',\n",
       " '.',\n",
       " '###',\n",
       " 'It',\n",
       " 'recently',\n",
       " 'cut',\n",
       " 'its',\n",
       " 'work',\n",
       " 'force',\n",
       " 'by',\n",
       " '9',\n",
       " '%',\n",
       " ',',\n",
       " 'or',\n",
       " '213',\n",
       " '.',\n",
       " '-RRB-',\n",
       " '###',\n",
       " 'But',\n",
       " 'Lloyd',\n",
       " \"'s\",\n",
       " 'is',\n",
       " 'hampered',\n",
       " 'in',\n",
       " 'its',\n",
       " 'efforts',\n",
       " 'to',\n",
       " 'overhaul',\n",
       " 'operations',\n",
       " 'by',\n",
       " 'its',\n",
       " 'reluctance',\n",
       " 'to',\n",
       " 'embrace',\n",
       " 'modern',\n",
       " 'technology',\n",
       " '.',\n",
       " '###',\n",
       " 'Mr.',\n",
       " 'Wetherell',\n",
       " ',',\n",
       " 'the',\n",
       " 'underwriter',\n",
       " ',',\n",
       " 'reckons',\n",
       " 'half',\n",
       " 'of',\n",
       " 'his',\n",
       " 'business',\n",
       " 'could',\n",
       " 'be',\n",
       " 'transacted',\n",
       " 'by',\n",
       " 'computer',\n",
       " ',',\n",
       " 'cutting',\n",
       " 'costs',\n",
       " 'at',\n",
       " 'least',\n",
       " '10',\n",
       " '%',\n",
       " '.',\n",
       " '###',\n",
       " 'Though',\n",
       " 'Lloyd',\n",
       " \"'s\",\n",
       " 'has',\n",
       " 'talked',\n",
       " 'for',\n",
       " 'years',\n",
       " 'about',\n",
       " 'computerizing',\n",
       " 'underwriting',\n",
       " 'transactions',\n",
       " ',',\n",
       " 'the',\n",
       " 'effort',\n",
       " 'has',\n",
       " \"n't\",\n",
       " 'gotten',\n",
       " 'very',\n",
       " 'far',\n",
       " '.',\n",
       " '###',\n",
       " 'Competition',\n",
       " 'among',\n",
       " 'underwriters',\n",
       " 'and',\n",
       " 'brokers',\n",
       " 'makes',\n",
       " 'them',\n",
       " 'loath',\n",
       " 'to',\n",
       " 'centralize',\n",
       " 'price',\n",
       " 'and',\n",
       " 'policy',\n",
       " 'information',\n",
       " '.',\n",
       " '###',\n",
       " 'Both',\n",
       " 'groups',\n",
       " 'cling',\n",
       " 'to',\n",
       " 'traditional',\n",
       " 'face-to-face',\n",
       " 'dealings',\n",
       " ',',\n",
       " 'even',\n",
       " 'for',\n",
       " 'routine',\n",
       " 'policies',\n",
       " '.',\n",
       " '###',\n",
       " 'Lloyd',\n",
       " \"'s\",\n",
       " 'overblown',\n",
       " 'bureaucracy',\n",
       " 'also',\n",
       " 'hampers',\n",
       " 'efforts',\n",
       " 'to',\n",
       " 'update',\n",
       " 'marketing',\n",
       " 'strategies',\n",
       " '.',\n",
       " '###',\n",
       " 'Some',\n",
       " 'underwriters',\n",
       " 'have',\n",
       " 'been',\n",
       " 'pressing',\n",
       " 'for',\n",
       " 'years',\n",
       " 'to',\n",
       " 'tap',\n",
       " 'the',\n",
       " 'low-margin',\n",
       " 'business',\n",
       " 'by',\n",
       " 'selling',\n",
       " 'some',\n",
       " 'policies',\n",
       " 'directly',\n",
       " 'to',\n",
       " 'consumers',\n",
       " '.',\n",
       " '###',\n",
       " 'Lloyd',\n",
       " \"'s\",\n",
       " 'presently',\n",
       " 'sells',\n",
       " 'only',\n",
       " 'auto',\n",
       " 'insurance',\n",
       " 'directly',\n",
       " 'to',\n",
       " 'the',\n",
       " 'public',\n",
       " ',',\n",
       " 'and',\n",
       " 'such',\n",
       " 'policies',\n",
       " 'are',\n",
       " 'sold',\n",
       " 'only',\n",
       " 'in',\n",
       " 'limited',\n",
       " 'markets',\n",
       " 'such',\n",
       " 'as',\n",
       " 'the',\n",
       " 'U.K.',\n",
       " 'and',\n",
       " 'Canada',\n",
       " '.',\n",
       " '###',\n",
       " 'But',\n",
       " 'such',\n",
       " 'changes',\n",
       " 'must',\n",
       " 'be',\n",
       " 'cleared',\n",
       " 'by',\n",
       " 'four',\n",
       " 'internal',\n",
       " 'committees',\n",
       " 'and',\n",
       " 'dozens',\n",
       " 'of',\n",
       " 'underwriters',\n",
       " ',',\n",
       " 'brokers',\n",
       " 'and',\n",
       " 'administrators',\n",
       " 'before',\n",
       " 'being',\n",
       " 'implemented',\n",
       " '.',\n",
       " '###',\n",
       " 'The',\n",
       " 'proposal',\n",
       " 'to',\n",
       " 'sell',\n",
       " 'directly',\n",
       " 'to',\n",
       " 'the',\n",
       " 'public',\n",
       " 'remains',\n",
       " 'mired',\n",
       " 'in',\n",
       " 'bureaucratic',\n",
       " 'quicksand',\n",
       " '.',\n",
       " '###',\n",
       " 'Lloyd',\n",
       " \"'s\",\n",
       " 'is',\n",
       " 'moving',\n",
       " 'forward',\n",
       " 'on',\n",
       " 'some',\n",
       " 'fronts',\n",
       " ',',\n",
       " 'though',\n",
       " '.',\n",
       " '###',\n",
       " 'Mr.',\n",
       " 'Lawrence',\n",
       " 'says',\n",
       " 'the',\n",
       " 'exchange',\n",
       " 'is',\n",
       " 'updating',\n",
       " 'some',\n",
       " 'procedures',\n",
       " 'to',\n",
       " 'make',\n",
       " 'speedier',\n",
       " 'payments',\n",
       " 'on',\n",
       " 'claims',\n",
       " '.',\n",
       " '###',\n",
       " 'By',\n",
       " 'next',\n",
       " 'year',\n",
       " ',',\n",
       " 'all',\n",
       " 'underwriters',\n",
       " 'will',\n",
       " 'be',\n",
       " 'linked',\n",
       " 'to',\n",
       " 'a',\n",
       " 'communications',\n",
       " 'network',\n",
       " 'that',\n",
       " 'could',\n",
       " 'reduce',\n",
       " 'paper',\n",
       " 'work',\n",
       " 'on',\n",
       " 'claims',\n",
       " '.',\n",
       " '###',\n",
       " 'Japan',\n",
       " \"'s\",\n",
       " 'Daiwa',\n",
       " 'Securities',\n",
       " 'Co.',\n",
       " 'named',\n",
       " 'Masahiro',\n",
       " 'Dozen',\n",
       " 'president',\n",
       " '.',\n",
       " '###',\n",
       " 'Mr.',\n",
       " 'Dozen',\n",
       " 'succeeds',\n",
       " 'Sadakane',\n",
       " 'Doi',\n",
       " ',',\n",
       " 'who',\n",
       " 'will',\n",
       " 'become',\n",
       " 'vice',\n",
       " 'chairman',\n",
       " '.',\n",
       " '###',\n",
       " 'Yoshitoki',\n",
       " 'Chino',\n",
       " 'retains',\n",
       " 'his',\n",
       " 'title',\n",
       " 'of',\n",
       " 'chairman',\n",
       " 'of',\n",
       " 'Daiwa',\n",
       " ',',\n",
       " 'Japan',\n",
       " \"'s\",\n",
       " 'second-largest',\n",
       " 'securities',\n",
       " 'firm',\n",
       " '.',\n",
       " '###',\n",
       " 'In',\n",
       " 'Japanese',\n",
       " 'firms',\n",
       " ',',\n",
       " 'the',\n",
       " 'president',\n",
       " 'usually',\n",
       " 'is',\n",
       " 'in',\n",
       " 'charge',\n",
       " 'of',\n",
       " 'day-to-day',\n",
       " 'operations',\n",
       " ',',\n",
       " 'while',\n",
       " 'the',\n",
       " 'chairman',\n",
       " \"'s\",\n",
       " 'role',\n",
       " 'is',\n",
       " 'more',\n",
       " 'a',\n",
       " 'ceremonial',\n",
       " 'one',\n",
       " '.',\n",
       " '###',\n",
       " 'The',\n",
       " 'title',\n",
       " 'of',\n",
       " 'chief',\n",
       " 'executive',\n",
       " 'officer',\n",
       " 'is',\n",
       " \"n't\",\n",
       " 'used',\n",
       " '.',\n",
       " '###',\n",
       " 'While',\n",
       " 'people',\n",
       " 'within',\n",
       " 'Daiwa',\n",
       " ',',\n",
       " 'particularly',\n",
       " 'internationalists',\n",
       " ',',\n",
       " 'expected',\n",
       " 'that',\n",
       " 'Mr.',\n",
       " 'Dozen',\n",
       " ',',\n",
       " '52',\n",
       " ',',\n",
       " 'would',\n",
       " 'eventually',\n",
       " 'become',\n",
       " 'Daiwa',\n",
       " \"'s\",\n",
       " 'president',\n",
       " ',',\n",
       " 'the',\n",
       " 'speed',\n",
       " 'of',\n",
       " 'his',\n",
       " 'promotion',\n",
       " 'surprised',\n",
       " 'many',\n",
       " '.',\n",
       " '###',\n",
       " 'It',\n",
       " 'was',\n",
       " 'only',\n",
       " 'earlier',\n",
       " 'this',\n",
       " 'year',\n",
       " 'that',\n",
       " 'the',\n",
       " 'jovial',\n",
       " ',',\n",
       " 'easygoing',\n",
       " 'executive',\n",
       " '--',\n",
       " 'he',\n",
       " 'likes',\n",
       " 'to',\n",
       " 'joke',\n",
       " 'with',\n",
       " 'Americans',\n",
       " 'about',\n",
       " 'how',\n",
       " 'his',\n",
       " 'name',\n",
       " 'is',\n",
       " 'synonymous',\n",
       " 'with',\n",
       " 'twelve',\n",
       " '--',\n",
       " 'was',\n",
       " 'appointed',\n",
       " 'deputy',\n",
       " 'president',\n",
       " '.',\n",
       " '###',\n",
       " 'Mr.',\n",
       " 'Dozen',\n",
       " 'is',\n",
       " 'taking',\n",
       " 'over',\n",
       " 'the',\n",
       " 'reins',\n",
       " 'of',\n",
       " 'a',\n",
       " 'securities',\n",
       " 'company',\n",
       " 'that',\n",
       " 'does',\n",
       " 'very',\n",
       " 'well',\n",
       " 'in',\n",
       " 'its',\n",
       " 'domestic',\n",
       " 'market',\n",
       " 'but',\n",
       " 'that',\n",
       " 'is',\n",
       " 'still',\n",
       " 'seeking',\n",
       " 'to',\n",
       " 'realize',\n",
       " 'its',\n",
       " 'potential',\n",
       " 'in',\n",
       " 'global',\n",
       " 'investment',\n",
       " 'banking',\n",
       " 'and',\n",
       " 'securities',\n",
       " 'dealing',\n",
       " '.',\n",
       " '###',\n",
       " 'Daiwa',\n",
       " 'is',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'world',\n",
       " \"'s\",\n",
       " 'largest',\n",
       " 'securities',\n",
       " 'firms',\n",
       " '.',\n",
       " '###',\n",
       " 'As',\n",
       " 'of',\n",
       " 'March',\n",
       " '31',\n",
       " ',',\n",
       " 'the',\n",
       " 'Daiwa',\n",
       " 'group',\n",
       " 'had',\n",
       " 'shareholder',\n",
       " 'equity',\n",
       " 'of',\n",
       " '801.21',\n",
       " 'billion',\n",
       " 'yen',\n",
       " '-LRB-',\n",
       " '$',\n",
       " '5.64',\n",
       " 'billion',\n",
       " '-RRB-',\n",
       " '.',\n",
       " '###',\n",
       " 'For',\n",
       " 'the',\n",
       " 'six',\n",
       " 'months',\n",
       " 'ended',\n",
       " 'Sept.',\n",
       " '30',\n",
       " ',',\n",
       " 'Daiwa',\n",
       " 'reported',\n",
       " 'unconsolidated',\n",
       " '-LRB-',\n",
       " 'parent',\n",
       " 'company',\n",
       " '-RRB-',\n",
       " 'net',\n",
       " 'income',\n",
       " 'of',\n",
       " '79.03',\n",
       " 'billion',\n",
       " 'yen',\n",
       " '-LRB-',\n",
       " '$',\n",
       " '556.5',\n",
       " 'million',\n",
       " '-RRB-',\n",
       " 'on',\n",
       " 'revenue',\n",
       " 'of',\n",
       " '332.38',\n",
       " 'billion',\n",
       " 'yen',\n",
       " '-LRB-',\n",
       " '$',\n",
       " '2.34',\n",
       " 'billion',\n",
       " '-RRB-',\n",
       " '.',\n",
       " '###',\n",
       " 'Both',\n",
       " 'figures',\n",
       " 'were',\n",
       " 'record',\n",
       " 'highs',\n",
       " '.',\n",
       " '###',\n",
       " 'Several',\n",
       " 'observers',\n",
       " 'interpreted',\n",
       " 'Mr.',\n",
       " 'Dozen',\n",
       " \"'s\",\n",
       " 'appointment',\n",
       " 'as',\n",
       " 'an',\n",
       " 'attempt',\n",
       " 'by',\n",
       " 'Daiwa',\n",
       " 'to',\n",
       " 'make',\n",
       " 'its',\n",
       " 'international',\n",
       " 'operations',\n",
       " 'more',\n",
       " 'profitable',\n",
       " 'while',\n",
       " 'preparing',\n",
       " 'the',\n",
       " 'firm',\n",
       " 'for',\n",
       " 'the',\n",
       " 'effects',\n",
       " 'of',\n",
       " 'the',\n",
       " 'continuing',\n",
       " 'deregulation',\n",
       " 'of',\n",
       " 'Japan',\n",
       " \"'s\",\n",
       " 'domestic',\n",
       " 'markets',\n",
       " ',',\n",
       " 'which',\n",
       " 'should',\n",
       " 'mean',\n",
       " 'increased',\n",
       " 'competition',\n",
       " '.',\n",
       " '###',\n",
       " 'All',\n",
       " 'of',\n",
       " 'Japan',\n",
       " \"'s\",\n",
       " 'so-called',\n",
       " 'Big',\n",
       " 'Four',\n",
       " 'securities',\n",
       " 'firms',\n",
       " '--',\n",
       " 'Nomura',\n",
       " 'Securities',\n",
       " 'Co.',\n",
       " 'Ltd.',\n",
       " ',',\n",
       " 'the',\n",
       " 'world',\n",
       " \"'s\",\n",
       " 'largest',\n",
       " ',',\n",
       " 'Nikko',\n",
       " 'Securities',\n",
       " 'Co.',\n",
       " 'Ltd.',\n",
       " ',',\n",
       " 'Yamaichi',\n",
       " 'Securities',\n",
       " 'Co.',\n",
       " 'Ltd.',\n",
       " 'and',\n",
       " 'Daiwa',\n",
       " '--',\n",
       " 'have',\n",
       " 'suffered',\n",
       " 'setbacks',\n",
       " 'in',\n",
       " 'their',\n",
       " 'attempts',\n",
       " 'to',\n",
       " 'break',\n",
       " 'into',\n",
       " 'foreign',\n",
       " 'markets',\n",
       " '.']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English\n",
      "Evaluating HMM [0]\n"
     ]
    }
   ],
   "source": [
    "print('English')\n",
    "accuracies_en = [evaluate(split, i) for i,split in enumerate(splits_en)]\n",
    "print('Czech')\n",
    "accuracies_cz = [evaluate(split, i) for i,split in enumerate(splits_cz)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language</th>\n",
       "      <th>Accuracies</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Standard Deviation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>English</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.032697</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>English</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.032697</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Language Accuracies      Mean  Standard Deviation\n",
       "0  English        4.0  4.032697                 0.0\n",
       "1  English        4.0  4.032697                 0.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_str_en = ' '.join(['{0:0.1f}'.format(i * 100) for i in accuracies_en])\n",
    "acc_str_cz = ' '.join(['{0:0.1f}'.format(i * 100) for i in accuracies_cz])\n",
    "\n",
    "row_en = ['English', acc_str_en, np.mean(accuracies_en) * 100, np.std(accuracies_en)]\n",
    "row_cz = ['Czech',   acc_str_cz, np.mean(accuracies_cz) * 100, np.std(accuracies_cz)]\n",
    "\n",
    "columns = ['Language', 'Accuracies', 'Mean', 'Standard Deviation']\n",
    "hmm_results = pd.DataFrame([row_en, row_cz], columns=columns)\n",
    "hmm_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel:\n",
    "    \"\"\"Counts words and calculates probabilities (up to trigrams)\"\"\"\n",
    "    \n",
    "    def __init__(self, words):\n",
    "        # Prepend two tokens to avoid beginning-of-data problems\n",
    "        words = np.array(['<ss>', '<s>'] + list(words))\n",
    "        \n",
    "        # Unigrams\n",
    "        self.unigrams = words\n",
    "        self.unigram_set = list(set(self.unigrams))\n",
    "        self.unigram_count = len(self.unigram_set)\n",
    "        self.total_unigram_count = len(self.unigrams)\n",
    "        self.unigram_dist = c.Counter(self.unigrams)\n",
    "        \n",
    "        # Bigrams\n",
    "        self.bigrams = list(nltk.bigrams(words))\n",
    "        self.bigram_set = list(set(self.bigrams))\n",
    "        self.bigram_count = len(self.bigram_set)\n",
    "        self.total_bigram_count = len(self.bigrams)\n",
    "        self.bigram_dist = c.Counter(self.bigrams)\n",
    "        \n",
    "        # Trigrams\n",
    "        self.trigrams = list(nltk.trigrams(words))\n",
    "        self.trigram_set = list(set(self.trigrams))\n",
    "        self.trigram_count = len(self.trigram_set)\n",
    "        self.total_trigram_count = len(self.trigrams)\n",
    "        self.trigram_dist = c.Counter(self.trigrams)\n",
    "    \n",
    "    def count(ngrams):\n",
    "        ngram_set = list(set(ngrams))\n",
    "        ngram_count = len(ngram_set)\n",
    "        total_ngram_count = len(ngrams)\n",
    "        ngram_dist = c.Counter(ngrams)\n",
    "        return ngram_set, ngram_count, total_ngram_count, ngram_dist\n",
    "        \n",
    "    def p_uniform(self):\n",
    "        \"\"\"Calculates the probability of choosing a word uniformly at random\"\"\"\n",
    "        return self.div(1, self.unigram_count)\n",
    "    \n",
    "    def p_unigram(self, w):\n",
    "        \"\"\"Calculates the probability a unigram appears in the distribution\"\"\"\n",
    "        return self.div(self.unigram_dist[w], self.total_unigram_count)\n",
    "    \n",
    "    def p_bigram_cond(self, wprev, w):\n",
    "        \"\"\"Calculates the probability a word appears in the distribution given the previous word\"\"\"\n",
    "        # If neither ngram has been seen, use the uniform distribution for smoothing purposes\n",
    "        if ((self.bigram_dist[wprev, w], self.unigram_dist[wprev]) == (0,0)):\n",
    "            return self.p_uniform()\n",
    "        \n",
    "        return self.div(self.bigram_dist[wprev, w], self.unigram_dist[wprev])\n",
    "    \n",
    "    def p_trigram_cond(self, wprev2, wprev, w):\n",
    "        \"\"\"Calculates the probability a word appears in the distribution given the previous word\"\"\"\n",
    "        # If neither ngram has been seen, use the uniform distribution for smoothing purposes\n",
    "        if ((self.trigram_dist[wprev2, wprev, w], self.bigram_dist[wprev2, wprev]) == (0,0)):\n",
    "            return self.p_uniform()\n",
    "        \n",
    "        return self.div(self.trigram_dist[wprev2, wprev, w], self.bigram_dist[wprev2, wprev])\n",
    "    \n",
    "    def div(self, a, b):\n",
    "        \"\"\"Divides a and b safely\"\"\"\n",
    "        return a / b if b != 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_lambdas(n=3):\n",
    "    \"\"\"Initializes a list of lambdas for an ngram language model with uniform probabilities\"\"\"\n",
    "    return np.array([1 / (n + 1)] * (n + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_smoothed(lm, lambdas, wprev2, wprev, w):\n",
    "    \"\"\"Calculate the smoothed trigram probability using the weighted product of lambdas\"\"\"\n",
    "    return np.multiply(lambdas, [\n",
    "        lm.p_uniform(),\n",
    "        lm.p_unigram(w),\n",
    "        lm.p_bigram_cond(wprev, w),\n",
    "        lm.p_trigram_cond(wprev2, wprev, w)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_counts(lm, lambdas, heldout):\n",
    "    \"\"\"Computes the expected counts by smoothing across all trigrams and summing them all together\"\"\"\n",
    "    smoothed_probs = (p_smoothed(lm, lambdas, *trigram) for trigram in heldout) # Multiply lambdas by probabilities\n",
    "    return np.sum(smoothed / np.sum(smoothed) for smoothed in smoothed_probs) # Element-wise sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_lambda(lm, lambdas, heldout):\n",
    "    \"\"\"Computes the next lambda from the current lambdas by normalizing the expected counts\"\"\"\n",
    "    expected = expected_counts(lm, lambdas, heldout)\n",
    "    return expected / np.sum(expected) # Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def em_algorithm(train, heldout, stop_tolerance=1e-4):\n",
    "    \"\"\"Computes the EM algorithm for linear interpolation smoothing\"\"\"\n",
    "    lambdas = init_lambdas(3)\n",
    "    \n",
    "    lm = LanguageModel(train)\n",
    "    heldout_trigrams = LanguageModel(heldout).trigrams\n",
    "    \n",
    "    print('Lambdas:')\n",
    "    \n",
    "    next_l = next_lambda(lm, lambdas, heldout_trigrams)\n",
    "    while not np.all([diff < stop_tolerance for diff in np.abs(lambdas - next_l)]):\n",
    "        print(next_l)\n",
    "        lambdas = next_l\n",
    "        next_l = next_lambda(lm, lambdas, heldout_trigrams)\n",
    "\n",
    "    lambdas = next_l\n",
    "    return lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, heldout, test = splits_en[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_en = LanguageModel(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambdas:\n",
      "[0.00171272 0.01427302 0.23094838 0.75306588]\n",
      "[6.09494990e-06 4.42538007e-04 1.01261852e-01 8.98289515e-01]\n",
      "[3.49390171e-08 6.14055420e-05 4.23362189e-02 9.57602341e-01]\n",
      "[7.19392903e-10 5.15530093e-05 1.74922896e-02 9.82456157e-01]\n",
      "[1.71849267e-11 5.12945905e-05 7.20653209e-03 9.92742173e-01]\n",
      "[4.12379908e-13 5.12845010e-05 2.97099414e-03 9.96977721e-01]\n",
      "[9.90029950e-15 5.12829547e-05 1.22672529e-03 9.98721992e-01]\n",
      "[2.37739409e-16 5.12824581e-05 5.07102930e-04 9.99441615e-01]\n",
      "[5.70960107e-18 5.12822773e-05 2.09756728e-04 9.99738961e-01]\n",
      "[1.37130442e-19 5.12822076e-05 8.67883404e-05 9.99861929e-01]\n"
     ]
    }
   ],
   "source": [
    "lambdas_en = em_algorithm(train, heldout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
