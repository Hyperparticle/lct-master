{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Assignment #3: NPFL067 Statistical NLP II](http://ufal.mff.cuni.cz/~hajic/courses/npfl067/assign3.html)\n",
    "\n",
    "## Tagging\n",
    "\n",
    "### Author: Dan Kondratyuk\n",
    "\n",
    "### March 28, 2018\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Python notebook compares Brill's Tagger with a trigram HMM tagger.\n",
    "\n",
    "Code and explanation of results is fully viewable within this webpage.\n",
    "\n",
    "## Files\n",
    "\n",
    "- [index.html](./index.html) - Contains all veiwable code and a summary of results\n",
    "- [README.md](./README.md) - Instructions on how to run the code with Python\n",
    "- [nlp-assignment-3.ipynb](./nlp-assignment-3.ipynb) - Jupyter notebook where code can be run\n",
    "- [requirements.txt](./requirements.txt) - Required python packages for running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Brill's Tagger & Tagger Evaluation\n",
    "\n",
    "> For this whole homework, use data found in `texten2.ptg`, `textcz2.ptg`\n",
    ">\n",
    "> In the following, \"the data\" refers to both English and Czech, as usual.\n",
    ">\n",
    "> Split the data in the following way: use last 40,000 words for testing (data S), and from the remaining data, use the last 20,000 for smoothing (data H, if any). Call the rest \"data T\" (training). \n",
    ">\n",
    "> Download Eric Brill's supervised tagger from [UFAL's course assignment space](http://ufal.mff.cuni.cz/~hajic/courses/npfl067/RULE_BASED_TAGGER_V.1.14.tar.gz). Install it (i.e., uncompress (gunzip), untar, and make).\n",
    ">\n",
    "> You might need to make some changes in his makefile of course (it's and OLD program, in this fast changing world...).\n",
    ">\n",
    "> After installation, get the data, train it on as much data from T as time allows (in the package, there is an extensive documentation on how to train it on new data), and evaluate on data S. Tabulate the results.\n",
    ">\n",
    "> Do cross-validation of the results: split the data into S', [H',] T' such that S' is the first 40,000 words, and T' is the last but the first 20,000 words from the rest. Train Eric Brill's tagger on T' (again, use as much data as time allows) and evaluate on S'. Again, tabulate the results.\n",
    ">\n",
    "> Do three more splits of your data (using the same formula: 40k/20k/the rest) in some way or another (as different as possible), and get another three sets of results. Compute the mean (average) accuracy and the standard deviation of the accuracy. Tabulate all results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections as c\n",
    "import nltk\n",
    "from sklearn.metrics import accuracy_score\n",
    "import itertools\n",
    "import dill as pickle\n",
    "\n",
    "from subprocess import call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_text(filename):\n",
    "    \"\"\"Reads a text line by line, applies light preprocessing, and returns an array of words and tags\"\"\"\n",
    "    with open(filename, encoding='iso-8859-2') as f:\n",
    "        content = f.readlines()\n",
    "    \n",
    "    preprocess = lambda word: tuple(word.strip().rsplit('/', 1))\n",
    "    \n",
    "    return [preprocess(word) for word in content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isplit(iterable, splitters):\n",
    "    # https://stackoverflow.com/a/4322780\n",
    "    return [list(g) for k,g in itertools.groupby(iterable, lambda x:x in splitters) if not k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_split(data, token=('###', '###')):\n",
    "    return isplit(data, (None, token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(words, start=0):\n",
    "    train, heldout, test = words[:start] + words[start+60_000:],  words[start+40_000:start+60_000], words[start:start+40_000]\n",
    "    return train, heldout, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_end(words):\n",
    "    train, heldout, test = words[:-60_000],  words[-60_000:-40_000], words[-40_000:]\n",
    "    return train, heldout, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_all(words):\n",
    "    return [\n",
    "        split_data_end(words),\n",
    "        split_data(words, start=40_000 * 0),\n",
    "        split_data(words, start=40_000 * 1),\n",
    "        split_data(words, start=40_000 * 2),\n",
    "        split_data(words, start=40_000 * 3)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from https://github.com/nltk/nltk/blob/a84b28ca26ea3ee53da4eaafc2bbf037847779bd/nltk/tbl/demo.py\n",
    "REGEXP_TAGGER = nltk.tag.RegexpTagger(\n",
    "    [(r'^-?[0-9]+(.[0-9]+)?$', 'CD'),   # cardinal numbers\n",
    "     (r'(The|the|A|a|An|an)$', 'AT'),   # articles\n",
    "     (r'.*able$', 'JJ'),                # adjectives\n",
    "     (r'.*ness$', 'NN'),                # nouns formed from adjectives\n",
    "     (r'.*ly$', 'RB'),                  # adverbs\n",
    "     (r'.*s$', 'NNS'),                  # plural nouns\n",
    "     (r'.*ing$', 'VBG'),                # gerunds\n",
    "     (r'.*ed$', 'VBD'),                 # past tense verbs\n",
    "     (r'.*', 'NN')                      # nouns (default)\n",
    "])\n",
    "templates = nltk.tag.brill.brill24()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brill_tagger(train, heldout, baseline_backoff_tagger=REGEXP_TAGGER, templates=templates, trace=0, \n",
    "                 ruleformat='str', max_rules=300, min_score=3, min_acc=None):\n",
    "    baseline_tagger = nltk.tag.UnigramTagger(heldout, backoff=baseline_backoff_tagger)\n",
    "    trainer = nltk.tag.BrillTaggerTrainer(baseline_tagger, templates, trace=trace, ruleformat=ruleformat)\n",
    "    tagger = trainer.train(train, max_rules, min_score, min_acc)\n",
    "    return tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_brill(split, i=0, lang='', load=False):\n",
    "    train, heldout, test = split\n",
    "    \n",
    "    filename = 'data/brill_tagger_{}_{}.pkl'.format(lang, i)\n",
    "    \n",
    "    print('Evaluating Brill Tagger {} [{}]'.format(lang, i))\n",
    "    if load:\n",
    "        with open(filename, 'rb') as f:\n",
    "            tagger = pickle.load(f)\n",
    "    else:\n",
    "        tagger = brill_tagger([train], [heldout])\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(tagger, f)\n",
    "    \n",
    "    return tagger.evaluate([test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(tagger_type, eval_func, langs=('en', 'cz')):\n",
    "    lang_d = {'en': ('English', splits_en), 'cz': ('Czech', splits_cz)}\n",
    "    \n",
    "    rows = []\n",
    "    for lang in langs:\n",
    "        language, splits = lang_d[lang]\n",
    "        accuracies = [eval_func(split, i, lang) for i,split in enumerate(splits)]\n",
    "        acc_str = ' '.join(['{0:0.1f}'.format(i * 100) for i in accuracies])\n",
    "        row = [tagger_type, language, acc_str, np.mean(accuracies) * 100, np.std(accuracies) * 100]\n",
    "        rows.append(row)\n",
    "\n",
    "    columns = ['type', 'language', 'accuracies', 'mean', 'standard_deviation']\n",
    "    results = pd.DataFrame(rows, columns=columns)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the texts into memory\n",
    "english = './data/texten2.ptg'\n",
    "czech = './data/textcz2.ptg'\n",
    "\n",
    "words_en = open_text(english)\n",
    "words_cz = open_text(czech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_en = split_all(words_en)\n",
    "splits_cz = split_all(words_cz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Brill Tagger en [0]\n",
      "Evaluating Brill Tagger en [1]\n",
      "Evaluating Brill Tagger en [2]\n",
      "Evaluating Brill Tagger en [3]\n",
      "Evaluating Brill Tagger en [4]\n",
      "Evaluating Brill Tagger cz [0]\n",
      "Evaluating Brill Tagger cz [1]\n",
      "Evaluating Brill Tagger cz [2]\n",
      "Evaluating Brill Tagger cz [3]\n",
      "Evaluating Brill Tagger cz [4]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>language</th>\n",
       "      <th>accuracies</th>\n",
       "      <th>mean</th>\n",
       "      <th>standard_deviation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Brill</td>\n",
       "      <td>English</td>\n",
       "      <td>90.4 90.7 90.6 90.2 87.6</td>\n",
       "      <td>89.924328</td>\n",
       "      <td>0.011614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Brill</td>\n",
       "      <td>Czech</td>\n",
       "      <td>61.9 70.2 64.7 64.0 65.7</td>\n",
       "      <td>65.288820</td>\n",
       "      <td>0.027449</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    type language                accuracies       mean  standard_deviation\n",
       "0  Brill  English  90.4 90.7 90.6 90.2 87.6  89.924328            0.011614\n",
       "1  Brill    Czech  61.9 70.2 64.7 64.0 65.7  65.288820            0.027449"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brill_results = evaluate('Brill', evaluate_brill)\n",
    "brill_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Unsupervised Learning: HMM Tagging\n",
    "\n",
    "> Use the datasets T, H, and S. Estimate the parameters of an HMM tagger using supervised learning off the T data (trigram and lower models for tags). Smooth (both the trigram tag model as well as the lexical model) in the same way as in Homework No. 1 (use data H). Evaluate your tagger on S, using the Viterbi algorithm.\n",
    ">\n",
    "> Now use only the first 10,000 words of T to estimate the initial (raw) parameters of the HMM tagging model. Strip off the tags from the remaining data T. Use the Baum-Welch algorithm to improve on the initial parameters. Smooth as usual. Evaluate your unsupervised HMM tagger and compare the results to the supervised HMM tagger.\n",
    ">\n",
    "> Tabulate and compare the results of the HMM tagger vs. the Brill's tagger. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_hmm(split, i=0, lang='', unsupervised=False, load=False):\n",
    "    train, heldout, test = split\n",
    "    \n",
    "    name = 'unsupervised' if unsupervised else 'supervised'\n",
    "    filename = 'data/hmm_{}_tagger_{}_{}.pkl'.format(name, lang, i)\n",
    "    \n",
    "    if unsupervised:\n",
    "        labeled = sentence_split(train[:10_000])\n",
    "        unlabeled = sentence_split(train[10_000:])\n",
    "    else:\n",
    "        labeled = sentence_split(train)\n",
    "\n",
    "    words, tags = list(zip(*(train + heldout + test)))\n",
    "    states, symbols = list(set(tags)), list(set(words))\n",
    "\n",
    "    test = sentence_split(test)\n",
    "\n",
    "    trainer = nltk.hmm.HiddenMarkovModelTrainer(states, symbols)\n",
    "    \n",
    "    print('Evaluating HMM {} {} [{}]'.format(name, lang, i))\n",
    "    if load:\n",
    "        with open(filename, 'rb') as f:\n",
    "            tagger = pickle.load(f)\n",
    "    else:\n",
    "        tagger = trainer.train_supervised(labeled, estimator=lambda fd, bins: nltk.probability.LidstoneProbDist(fd, 0.1, bins))\n",
    "        if unsupervised:\n",
    "            tagger = trainer.train_unsupervised(unlabeled, model=tagger, max_iterations=5)\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(tagger, f)\n",
    "    \n",
    "    return tagger.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs = ['en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating HMM supervised en [0]\n",
      "Evaluating HMM supervised en [1]\n",
      "Evaluating HMM supervised en [2]\n",
      "Evaluating HMM supervised en [3]\n",
      "Evaluating HMM supervised en [4]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>language</th>\n",
       "      <th>accuracies</th>\n",
       "      <th>mean</th>\n",
       "      <th>standard_deviation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HMM (supervised)</td>\n",
       "      <td>English</td>\n",
       "      <td>91.1 90.7 91.7 89.8 91.2</td>\n",
       "      <td>90.908268</td>\n",
       "      <td>0.641836</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               type language                accuracies       mean  \\\n",
       "0  HMM (supervised)  English  91.1 90.7 91.7 89.8 91.2  90.908268   \n",
       "\n",
       "   standard_deviation  \n",
       "0            0.641836  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm_supervised_results = evaluate('HMM (supervised)', lambda split, i, lang: evaluate_hmm(split, i, lang, unsupervised=False), langs)\n",
    "hmm_supervised_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call(['spd-say', 'hmm supervised done'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(187479, 20000, 40000)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, heldout, test = splits_en[0]\n",
    "len(train), len(heldout), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7380410022779044\n",
      "iteration 0 logprob -1921749.2676943757\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7095671981776766"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, heldout, test = splits_en[0]\n",
    "\n",
    "words, tags = list(zip(*(train + heldout + test)))\n",
    "states, symbols = list(set(tags)), list(set(words))\n",
    "\n",
    "test = sentence_split(test)\n",
    "\n",
    "trainer = nltk.hmm.HiddenMarkovModelTrainer(states, symbols)\n",
    "\n",
    "labeled = sentence_split(train[:10_000])\n",
    "unlabeled = sentence_split(train[10_000:])\n",
    "\n",
    "tagger = trainer.train_supervised(labeled, estimator=lambda fd, bins: nltk.probability.LidstoneProbDist(fd, 0.1, bins))\n",
    "\n",
    "print(tagger.evaluate(test))\n",
    "\n",
    "tagger = trainer.train_unsupervised(unlabeled, model=tagger, max_iterations=3, update_outputs=False)\n",
    "\n",
    "tagger.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('two', 'CD'),\n",
       " ('administrations', 'NNPS'),\n",
       " ('in', 'NNPS'),\n",
       " ('a', 'NNPS'),\n",
       " ('row', 'NNPS'),\n",
       " ('have', 'NNPS'),\n",
       " ('been', 'NNPS'),\n",
       " ('unwilling', 'NNPS'),\n",
       " ('and', 'NNPS'),\n",
       " ('unable', 'NNPS'),\n",
       " ('to', 'NNPS'),\n",
       " ('develop', 'NNPS'),\n",
       " ('any', 'NNPS'),\n",
       " ('plan', 'NNPS'),\n",
       " (',', 'NNPS'),\n",
       " ('military', 'NNPS'),\n",
       " ('or', 'NNPS'),\n",
       " ('economic', 'NNPS'),\n",
       " (',', 'NNPS'),\n",
       " ('for', 'NNPS'),\n",
       " ('supporting', 'NNPS'),\n",
       " ('the', 'NNPS'),\n",
       " ('Panamanian', 'NNPS'),\n",
       " ('people', 'NNPS'),\n",
       " ('in', 'NNPS'),\n",
       " ('their', 'NNPS'),\n",
       " ('attempts', 'NNPS'),\n",
       " ('to', 'NNPS'),\n",
       " ('restore', 'NNPS'),\n",
       " ('democracy', 'NNPS'),\n",
       " ('.', 'NNPS')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.tag([w for w,t in test[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm_supervised_results = evaluate('HMM (unsupervised)', lambda split, i, lang: evaluate_hmm(split, i, lang, unsupervised=True), langs)\n",
    "hmm_supervised_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "call(['spd-say', 'hmm unsupervised done'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmoothProbDist(nltk.probability.ProbDistI):\n",
    "    def __init__(self, probdist):\n",
    "        self._probdist = probdist\n",
    "\n",
    "    def prob(self, sample):\n",
    "        return (self._probdist.prob(sample) + 0.1) / 1.1\n",
    "\n",
    "    def max(self):\n",
    "        return self._probdist.max()\n",
    "\n",
    "    def samples(self):\n",
    "        return self._probdist.samples()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '<SmoothProbDist>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(tagger):\n",
    "    transitions = tagger._transitions\n",
    "    outputs = tagger._outputs\n",
    "\n",
    "    for k in transitions:\n",
    "        transitions[k] = SmoothProbDist(transitions[k])\n",
    "    for k in outputs:\n",
    "        outputs[k] = SmoothProbDist(outputs[k])\n",
    "\n",
    "    return nltk.tag.hmm.HiddenMarkovModelTagger(tagger._symbols, tagger._states, transitions, outputs, tagger._priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel:\n",
    "    \"\"\"Counts words and calculates probabilities (up to trigrams)\"\"\"\n",
    "    \n",
    "    def __init__(self, words):\n",
    "        # Prepend two tokens to avoid beginning-of-data problems\n",
    "        words = np.array(['<ss>', '<s>'] + list(words))\n",
    "        \n",
    "        # Unigrams\n",
    "        self.unigrams = words\n",
    "        self.unigram_set = list(set(self.unigrams))\n",
    "        self.unigram_count = len(self.unigram_set)\n",
    "        self.total_unigram_count = len(self.unigrams)\n",
    "        self.unigram_dist = c.Counter(self.unigrams)\n",
    "        \n",
    "        # Bigrams\n",
    "        self.bigrams = list(nltk.bigrams(words))\n",
    "        self.bigram_set = list(set(self.bigrams))\n",
    "        self.bigram_count = len(self.bigram_set)\n",
    "        self.total_bigram_count = len(self.bigrams)\n",
    "        self.bigram_dist = c.Counter(self.bigrams)\n",
    "        \n",
    "        # Trigrams\n",
    "        self.trigrams = list(nltk.trigrams(words))\n",
    "        self.trigram_set = list(set(self.trigrams))\n",
    "        self.trigram_count = len(self.trigram_set)\n",
    "        self.total_trigram_count = len(self.trigrams)\n",
    "        self.trigram_dist = c.Counter(self.trigrams)\n",
    "    \n",
    "    def count(ngrams):\n",
    "        ngram_set = list(set(ngrams))\n",
    "        ngram_count = len(ngram_set)\n",
    "        total_ngram_count = len(ngrams)\n",
    "        ngram_dist = c.Counter(ngrams)\n",
    "        return ngram_set, ngram_count, total_ngram_count, ngram_dist\n",
    "        \n",
    "    def p_uniform(self):\n",
    "        \"\"\"Calculates the probability of choosing a word uniformly at random\"\"\"\n",
    "        return self.div(1, self.unigram_count)\n",
    "    \n",
    "    def p_unigram(self, w):\n",
    "        \"\"\"Calculates the probability a unigram appears in the distribution\"\"\"\n",
    "        return self.div(self.unigram_dist[w], self.total_unigram_count)\n",
    "    \n",
    "    def p_bigram_cond(self, wprev, w):\n",
    "        \"\"\"Calculates the probability a word appears in the distribution given the previous word\"\"\"\n",
    "        # If neither ngram has been seen, use the uniform distribution for smoothing purposes\n",
    "        if ((self.bigram_dist[wprev, w], self.unigram_dist[wprev]) == (0,0)):\n",
    "            return self.p_uniform()\n",
    "        \n",
    "        return self.div(self.bigram_dist[wprev, w], self.unigram_dist[wprev])\n",
    "    \n",
    "    def p_trigram_cond(self, wprev2, wprev, w):\n",
    "        \"\"\"Calculates the probability a word appears in the distribution given the previous word\"\"\"\n",
    "        # If neither ngram has been seen, use the uniform distribution for smoothing purposes\n",
    "        if ((self.trigram_dist[wprev2, wprev, w], self.bigram_dist[wprev2, wprev]) == (0,0)):\n",
    "            return self.p_uniform()\n",
    "        \n",
    "        return self.div(self.trigram_dist[wprev2, wprev, w], self.bigram_dist[wprev2, wprev])\n",
    "    \n",
    "    def div(self, a, b):\n",
    "        \"\"\"Divides a and b safely\"\"\"\n",
    "        return a / b if b != 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_lambdas(n=3):\n",
    "    \"\"\"Initializes a list of lambdas for an ngram language model with uniform probabilities\"\"\"\n",
    "    return np.array([1 / (n + 1)] * (n + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_smoothed(lm, lambdas, wprev2, wprev, w):\n",
    "    \"\"\"Calculate the smoothed trigram probability using the weighted product of lambdas\"\"\"\n",
    "    return np.multiply(lambdas, [\n",
    "        lm.p_uniform(),\n",
    "        lm.p_unigram(w),\n",
    "        lm.p_bigram_cond(wprev, w),\n",
    "        lm.p_trigram_cond(wprev2, wprev, w)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_counts(lm, lambdas, heldout):\n",
    "    \"\"\"Computes the expected counts by smoothing across all trigrams and summing them all together\"\"\"\n",
    "    smoothed_probs = (p_smoothed(lm, lambdas, *trigram) for trigram in heldout) # Multiply lambdas by probabilities\n",
    "    return np.sum(smoothed / np.sum(smoothed) for smoothed in smoothed_probs) # Element-wise sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_lambda(lm, lambdas, heldout):\n",
    "    \"\"\"Computes the next lambda from the current lambdas by normalizing the expected counts\"\"\"\n",
    "    expected = expected_counts(lm, lambdas, heldout)\n",
    "    return expected / np.sum(expected) # Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def em_algorithm(train, heldout, stop_tolerance=1e-4):\n",
    "    \"\"\"Computes the EM algorithm for linear interpolation smoothing\"\"\"\n",
    "    lambdas = init_lambdas(3)\n",
    "    \n",
    "    lm = LanguageModel(train)\n",
    "    heldout_trigrams = LanguageModel(heldout).trigrams\n",
    "    \n",
    "    print('Lambdas:')\n",
    "    \n",
    "    next_l = next_lambda(lm, lambdas, heldout_trigrams)\n",
    "    while not np.all([diff < stop_tolerance for diff in np.abs(lambdas - next_l)]):\n",
    "        print(next_l)\n",
    "        lambdas = next_l\n",
    "        next_l = next_lambda(lm, lambdas, heldout_trigrams)\n",
    "\n",
    "    lambdas = next_l\n",
    "    return lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, heldout, test = splits_en[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_en = LanguageModel(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambdas:\n",
      "[0.00171272 0.01427302 0.23094838 0.75306588]\n",
      "[6.09494990e-06 4.42538007e-04 1.01261852e-01 8.98289515e-01]\n",
      "[3.49390171e-08 6.14055420e-05 4.23362189e-02 9.57602341e-01]\n",
      "[7.19392903e-10 5.15530093e-05 1.74922896e-02 9.82456157e-01]\n",
      "[1.71849267e-11 5.12945905e-05 7.20653209e-03 9.92742173e-01]\n",
      "[4.12379908e-13 5.12845010e-05 2.97099414e-03 9.96977721e-01]\n",
      "[9.90029950e-15 5.12829547e-05 1.22672529e-03 9.98721992e-01]\n",
      "[2.37739409e-16 5.12824581e-05 5.07102930e-04 9.99441615e-01]\n",
      "[5.70960107e-18 5.12822773e-05 2.09756728e-04 9.99738961e-01]\n",
      "[1.37130442e-19 5.12822076e-05 8.67883404e-05 9.99861929e-01]\n"
     ]
    }
   ],
   "source": [
    "lambdas_en = em_algorithm(train, heldout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
