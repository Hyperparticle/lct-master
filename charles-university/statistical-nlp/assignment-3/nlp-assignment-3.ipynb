{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Assignment #3: NPFL067 Statistical NLP II](http://ufal.mff.cuni.cz/~hajic/courses/npfl067/assign3.html)\n",
    "\n",
    "## Tagging\n",
    "\n",
    "### Author: Dan Kondratyuk\n",
    "\n",
    "### March 28, 2018\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Python notebook compares Brill's Tagger with a trigram HMM tagger.\n",
    "\n",
    "Code and explanation of results is fully viewable within this webpage.\n",
    "\n",
    "## Files\n",
    "\n",
    "- [index.html](./index.html) - Contains all veiwable code and a summary of results\n",
    "- [README.md](./README.md) - Instructions on how to run the code with Python\n",
    "- [nlp-assignment-3.ipynb](./nlp-assignment-3.ipynb) - Jupyter notebook where code can be run\n",
    "- [requirements.txt](./requirements.txt) - Required python packages for running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Brill's Tagger & Tagger Evaluation\n",
    "\n",
    "> For this whole homework, use data found in `texten2.ptg`, `textcz2.ptg`\n",
    ">\n",
    "> In the following, \"the data\" refers to both English and Czech, as usual.\n",
    ">\n",
    "> Split the data in the following way: use last 40,000 words for testing (data S), and from the remaining data, use the last 20,000 for smoothing (data H, if any). Call the rest \"data T\" (training). \n",
    ">\n",
    "> Download Eric Brill's supervised tagger from [UFAL's course assignment space](http://ufal.mff.cuni.cz/~hajic/courses/npfl067/RULE_BASED_TAGGER_V.1.14.tar.gz). Install it (i.e., uncompress (gunzip), untar, and make).\n",
    ">\n",
    "> You might need to make some changes in his makefile of course (it's and OLD program, in this fast changing world...).\n",
    ">\n",
    "> After installation, get the data, train it on as much data from T as time allows (in the package, there is an extensive documentation on how to train it on new data), and evaluate on data S. Tabulate the results.\n",
    ">\n",
    "> Do cross-validation of the results: split the data into S', [H',] T' such that S' is the first 40,000 words, and T' is the last but the first 20,000 words from the rest. Train Eric Brill's tagger on T' (again, use as much data as time allows) and evaluate on S'. Again, tabulate the results.\n",
    ">\n",
    "> Do three more splits of your data (using the same formula: 40k/20k/the rest) in some way or another (as different as possible), and get another three sets of results. Compute the mean (average) accuracy and the standard deviation of the accuracy. Tabulate all results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.metrics import accuracy_score\n",
    "import itertools\n",
    "import dill as pickle\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from subprocess import call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_text(filename):\n",
    "    \"\"\"Reads a text line by line, applies light preprocessing, and returns an array of words and tags\"\"\"\n",
    "    with open(filename, encoding='iso-8859-2') as f:\n",
    "        content = f.readlines()\n",
    "    \n",
    "    preprocess = lambda word: tuple(word.strip().rsplit('/', 1))\n",
    "    \n",
    "    return [preprocess(word) for word in content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isplit(iterable, splitters):\n",
    "    # https://stackoverflow.com/a/4322780\n",
    "    return [list(g) for k,g in itertools.groupby(iterable, lambda x:x in splitters) if not k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_split(data, token=('###', '###')):\n",
    "    return isplit(data, (None, token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(words, start=0):\n",
    "    train, heldout, test = words[:start] + words[start+60_000:],  words[start+40_000:start+60_000], words[start:start+40_000]\n",
    "    return train, heldout, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_end(words):\n",
    "    train, heldout, test = words[:-60_000],  words[-60_000:-40_000], words[-40_000:]\n",
    "    return train, heldout, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_all(words):\n",
    "    return [\n",
    "        split_data_end(words),\n",
    "        split_data(words, start=40_000 * 0),\n",
    "        split_data(words, start=40_000 * 1),\n",
    "        split_data(words, start=40_000 * 2),\n",
    "        split_data(words, start=40_000 * 3)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from https://github.com/nltk/nltk/blob/a84b28ca26ea3ee53da4eaafc2bbf037847779bd/nltk/tbl/demo.py\n",
    "REGEXP_TAGGER = nltk.tag.RegexpTagger(\n",
    "    [(r'^-?[0-9]+(.[0-9]+)?$', 'CD'),   # cardinal numbers\n",
    "     (r'(The|the|A|a|An|an)$', 'AT'),   # articles\n",
    "     (r'.*able$', 'JJ'),                # adjectives\n",
    "     (r'.*ness$', 'NN'),                # nouns formed from adjectives\n",
    "     (r'.*ly$', 'RB'),                  # adverbs\n",
    "     (r'.*s$', 'NNS'),                  # plural nouns\n",
    "     (r'.*ing$', 'VBG'),                # gerunds\n",
    "     (r'.*ed$', 'VBD'),                 # past tense verbs\n",
    "     (r'.*', 'NN')                      # nouns (default)\n",
    "])\n",
    "templates = nltk.tag.brill.brill24()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brill_tagger(train, heldout, baseline_backoff_tagger=REGEXP_TAGGER, templates=templates, trace=0, \n",
    "                 ruleformat='str', max_rules=300, min_score=3, min_acc=None):\n",
    "    baseline_tagger = nltk.tag.UnigramTagger(heldout, backoff=baseline_backoff_tagger)\n",
    "    trainer = nltk.tag.BrillTaggerTrainer(baseline_tagger, templates, trace=trace, ruleformat=ruleformat)\n",
    "    tagger = trainer.train(train, max_rules, min_score, min_acc)\n",
    "    return tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_brill(split, i=0, lang='', load=False):\n",
    "    train, heldout, test = split\n",
    "    \n",
    "    filename = 'data/brill_tagger_{}_{}.pkl'.format(lang, i)\n",
    "    \n",
    "    print('Evaluating Brill Tagger {} [{}]'.format(lang, i))\n",
    "    if load:\n",
    "        with open(filename, 'rb') as f:\n",
    "            tagger = pickle.load(f)\n",
    "    else:\n",
    "        tagger = brill_tagger([train], [heldout])\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(tagger, f)\n",
    "    \n",
    "    return tagger.evaluate([test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(tagger_type, eval_func, langs=('en', 'cz')):\n",
    "    lang_d = {'en': ('English', splits_en), 'cz': ('Czech', splits_cz)}\n",
    "    \n",
    "    rows = []\n",
    "    for lang in langs:\n",
    "        language, splits = lang_d[lang]\n",
    "        accuracies = [eval_func(split, i, lang) for i,split in enumerate(splits)]\n",
    "        acc_str = ' '.join(['{0:0.1f}'.format(i * 100) for i in accuracies])\n",
    "        row = [tagger_type, language, acc_str, np.mean(accuracies) * 100, np.std(accuracies) * 100]\n",
    "        rows.append(row)\n",
    "\n",
    "    columns = ['type', 'language', 'accuracies', 'mean', 'standard_deviation']\n",
    "    results = pd.DataFrame(rows, columns=columns)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the texts into memory\n",
    "english = './data/texten2.ptg'\n",
    "czech = './data/textcz2.ptg'\n",
    "\n",
    "words_en = open_text(english)\n",
    "words_cz = open_text(czech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_en = split_all(words_en)\n",
    "splits_cz = split_all(words_cz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Brill Tagger en [0]\n",
      "Evaluating Brill Tagger en [1]\n",
      "Evaluating Brill Tagger en [2]\n",
      "Evaluating Brill Tagger en [3]\n",
      "Evaluating Brill Tagger en [4]\n",
      "Evaluating Brill Tagger cz [0]\n",
      "Evaluating Brill Tagger cz [1]\n",
      "Evaluating Brill Tagger cz [2]\n",
      "Evaluating Brill Tagger cz [3]\n",
      "Evaluating Brill Tagger cz [4]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>language</th>\n",
       "      <th>accuracies</th>\n",
       "      <th>mean</th>\n",
       "      <th>standard_deviation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Brill</td>\n",
       "      <td>English</td>\n",
       "      <td>90.4 90.7 90.6 90.2 87.6</td>\n",
       "      <td>89.924328</td>\n",
       "      <td>0.011614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Brill</td>\n",
       "      <td>Czech</td>\n",
       "      <td>61.9 70.2 64.7 64.0 65.7</td>\n",
       "      <td>65.288820</td>\n",
       "      <td>0.027449</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    type language                accuracies       mean  standard_deviation\n",
       "0  Brill  English  90.4 90.7 90.6 90.2 87.6  89.924328            0.011614\n",
       "1  Brill    Czech  61.9 70.2 64.7 64.0 65.7  65.288820            0.027449"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brill_results = evaluate('Brill', evaluate_brill)\n",
    "brill_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Unsupervised Learning: HMM Tagging\n",
    "\n",
    "> Use the datasets T, H, and S. Estimate the parameters of an HMM tagger using supervised learning off the T data (trigram and lower models for tags). Smooth (both the trigram tag model as well as the lexical model) in the same way as in Homework No. 1 (use data H). Evaluate your tagger on S, using the Viterbi algorithm.\n",
    ">\n",
    "> Now use only the first 10,000 words of T to estimate the initial (raw) parameters of the HMM tagging model. Strip off the tags from the remaining data T. Use the Baum-Welch algorithm to improve on the initial parameters. Smooth as usual. Evaluate your unsupervised HMM tagger and compare the results to the supervised HMM tagger.\n",
    ">\n",
    "> Tabulate and compare the results of the HMM tagger vs. the Brill's tagger. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_hmm(split, i=0, lang='', unsupervised=False, load=False):\n",
    "    train, heldout, test = split\n",
    "    \n",
    "    name = 'unsupervised' if unsupervised else 'supervised'\n",
    "    filename = 'data/hmm_{}_tagger_{}_{}.pkl'.format(name, lang, i)\n",
    "    \n",
    "    if unsupervised:\n",
    "        labeled = sentence_split(train[:10_000])\n",
    "        unlabeled = sentence_split(train[10_000:])\n",
    "    else:\n",
    "        labeled = sentence_split(train)\n",
    "\n",
    "    words, tags = list(zip(*(train + heldout + test)))\n",
    "    states, symbols = list(set(tags)), list(set(words))\n",
    "\n",
    "    test = sentence_split(test)\n",
    "\n",
    "    trainer = nltk.hmm.HiddenMarkovModelTrainer(states, symbols)\n",
    "    \n",
    "    print('Evaluating HMM {} {} [{}]'.format(name, lang, i))\n",
    "    if load:\n",
    "        with open(filename, 'rb') as f:\n",
    "            tagger = pickle.load(f)\n",
    "    else:\n",
    "        tagger = trainer.train_supervised(labeled, estimator=lambda fd, bins: nltk.probability.LidstoneProbDist(fd, 0.1, bins))\n",
    "        if unsupervised:\n",
    "            tagger = trainer.train_unsupervised(unlabeled, model=tagger, max_iterations=5)\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(tagger, f)\n",
    "    \n",
    "    return tagger.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs = ['en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating HMM supervised en [0]\n",
      "Evaluating HMM supervised en [1]\n",
      "Evaluating HMM supervised en [2]\n",
      "Evaluating HMM supervised en [3]\n",
      "Evaluating HMM supervised en [4]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>language</th>\n",
       "      <th>accuracies</th>\n",
       "      <th>mean</th>\n",
       "      <th>standard_deviation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HMM (supervised)</td>\n",
       "      <td>English</td>\n",
       "      <td>91.1 90.7 91.7 89.8 91.2</td>\n",
       "      <td>90.908268</td>\n",
       "      <td>0.641836</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               type language                accuracies       mean  \\\n",
       "0  HMM (supervised)  English  91.1 90.7 91.7 89.8 91.2  90.908268   \n",
       "\n",
       "   standard_deviation  \n",
       "0            0.641836  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm_supervised_results = evaluate('HMM (supervised)', lambda split, i, lang: evaluate_hmm(split, i, lang, unsupervised=False), langs)\n",
    "hmm_supervised_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call(['spd-say', 'hmm supervised done'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7380410022779044\n",
      "iteration 0 logprob -1921749.2676943757\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7095671981776766"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, heldout, test = splits_en[0]\n",
    "\n",
    "words, tags = list(zip(*(train + heldout + test)))\n",
    "states, symbols = list(set(tags)), list(set(words))\n",
    "\n",
    "test = sentence_split(test)\n",
    "\n",
    "trainer = nltk.hmm.HiddenMarkovModelTrainer(states, symbols)\n",
    "\n",
    "labeled = sentence_split(train[:10_000])\n",
    "unlabeled = sentence_split(train[10_000:])\n",
    "\n",
    "tagger = trainer.train_supervised(labeled, estimator=lambda fd, bins: nltk.probability.LidstoneProbDist(fd, 0.1, bins))\n",
    "\n",
    "print(tagger.evaluate(test))\n",
    "\n",
    "tagger = trainer.train_unsupervised(unlabeled, model=tagger, max_iterations=3, update_outputs=False)\n",
    "\n",
    "tagger.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm_supervised_results = evaluate('HMM (unsupervised)', lambda split, i, lang: evaluate_hmm(split, i, lang, unsupervised=True), langs)\n",
    "hmm_supervised_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "call(['spd-say', 'hmm unsupervised done'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LISmoother:\n",
    "    \"\"\"Linear interpolation smoother\"\"\"\n",
    "    \n",
    "    def __init__(self, p_uniform, p_unigram, p_bigram, p_trigram):\n",
    "        self.p_uniform = p_uniform\n",
    "        self.p_unigram = p_unigram\n",
    "        self.p_bigram = p_bigram\n",
    "        self.p_trigram = p_trigram\n",
    "        \n",
    "        self.lambdas = self.init_lambdas(2)\n",
    "    \n",
    "    def init_lambdas(self, n=3):\n",
    "        \"\"\"Initializes a list of lambdas for an ngram language model with uniform probabilities\"\"\"\n",
    "        return np.array([1 / (n + 1)] * (n + 1))\n",
    "    \n",
    "    def smooth(self, heldout_data, stop_tolerance=1e-4):\n",
    "        \"\"\"Computes the EM algorithm for linear interpolation smoothing\"\"\"\n",
    "        \n",
    "        print('Lambdas:')\n",
    "        print(self.lambdas)\n",
    "\n",
    "        next_l = self.next_lambda(self.lambdas, heldout_data)\n",
    "        while not all(diff < stop_tolerance for diff in np.abs(self.lambdas - next_l)):\n",
    "            print(next_l)\n",
    "            self.lambdas = next_l\n",
    "            next_l = self.next_lambda(self.lambdas, heldout_data)\n",
    "\n",
    "        print(next_l)\n",
    "        self.lambdas = next_l\n",
    "\n",
    "    def next_lambda(self, lambdas, heldout):\n",
    "        \"\"\"Computes the next lambda from the current lambdas by normalizing the expected counts\"\"\"\n",
    "        expected = self.expected_counts(lambdas, heldout)\n",
    "        return expected / np.sum(expected)  # Normalize\n",
    "\n",
    "    def expected_counts(self, lambdas, heldout):\n",
    "        \"\"\"Computes the expected counts by smoothing across all trigrams and summing them all together\"\"\"\n",
    "        smoothed_probs = (self.p_smoothed(lambdas, *h) for h in heldout)  # Multiply lambdas by probabilities\n",
    "        return np.sum(smoothed / np.sum(smoothed) for smoothed in smoothed_probs)  # Element-wise sum\n",
    "\n",
    "    def p_smoothed(self, lambdas, tprev, t, w):\n",
    "        \"\"\"Calculate the smoothed trigram probability using the weighted product of lambdas\"\"\"\n",
    "        return np.multiply(lambdas, [\n",
    "            self.p_uniform,\n",
    "            self.p_unigram[w],\n",
    "            self.p_bigram[t, w],\n",
    "#             self.p_trigram[tprev, t, w]\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMMTagger:\n",
    "    def __init__(self, tagged_data, tag_set, word_set):\n",
    "        # Prepend two tokens to avoid beginning-of-data problems\n",
    "\n",
    "        self.states = tag_set\n",
    "        self.symbols = word_set\n",
    "        \n",
    "        self.text_size = len(tagged_data)\n",
    "        \n",
    "        # Transition tables - p(t | tprev2, tprev)\n",
    "        self.transition = defaultdict(float)\n",
    "        self.transition_bigram = defaultdict(float)\n",
    "        self.transition_unigram = defaultdict(float)\n",
    "        self.state_uniform = self.div(1, len(self.states))\n",
    "\n",
    "        # Emission tables - p(w | tprev, t)\n",
    "        self.emission = defaultdict(float)\n",
    "        self.emission_bigram = defaultdict(float)\n",
    "        self.emission_unigram = defaultdict(float)\n",
    "        self.symbol_uniform = self.div(1, len(self.symbols))\n",
    "\n",
    "        unigram_tag_dist = defaultdict(int)\n",
    "        bigram_tag_dist = defaultdict(int)\n",
    "        trigram_tag_dist = defaultdict(int)\n",
    "\n",
    "        unigram_output_dist = defaultdict(int)\n",
    "        bigram_output_dist = defaultdict(int)\n",
    "        trigram_output_dist = defaultdict(int)\n",
    "\n",
    "        tprev, tprev2 = None, None\n",
    "        for w, t in tagged_data:\n",
    "            unigram_tag_dist[t] += 1\n",
    "            bigram_tag_dist[tprev, t] += 1\n",
    "            trigram_tag_dist[tprev2, tprev, t] += 1\n",
    "\n",
    "            unigram_output_dist[w] += 1\n",
    "            bigram_output_dist[t, w] += 1\n",
    "            trigram_output_dist[tprev, t, w] += 1\n",
    "\n",
    "            tprev2 = tprev\n",
    "            tprev = t\n",
    "\n",
    "        # Build transition tables\n",
    "        for tprev2, tprev, t in trigram_tag_dist:\n",
    "            # Use uniform distribution if tags not seen\n",
    "            if (trigram_tag_dist[tprev2, tprev, t], bigram_tag_dist[tprev, t]) == (0, 0):\n",
    "                self.transition[tprev2, tprev, t] = self.state_uniform\n",
    "            self.transition[tprev2, tprev, t] = self.div(trigram_tag_dist[tprev2, tprev, t], bigram_tag_dist[tprev, t])\n",
    "        \n",
    "        for tprev, t in bigram_tag_dist:\n",
    "            # Use uniform distribution if tags not seen\n",
    "            if (bigram_tag_dist[tprev, t], unigram_tag_dist[t]) == (0, 0):\n",
    "                self.transition_bigram[tprev, t] = self.state_uniform\n",
    "            self.transition_bigram[tprev, t] = self.div(bigram_tag_dist[tprev, t], unigram_tag_dist[t])\n",
    "\n",
    "        for t in unigram_tag_dist:\n",
    "            self.transition_unigram[t] = self.div(unigram_tag_dist[t], self.text_size)\n",
    "            \n",
    "        # Build emission tables\n",
    "        for tprev, t, w in trigram_output_dist:\n",
    "            # Use uniform distribution if tags not seen\n",
    "            if (trigram_output_dist[tprev, t, w], bigram_tag_dist[tprev, t]) == (0, 0):\n",
    "                self.emission[tprev, t, w] = self.symbol_uniform\n",
    "            self.emission[tprev, t, w] = self.div(trigram_output_dist[tprev, t, w], bigram_tag_dist[tprev, t])    \n",
    "        \n",
    "        for t, w in bigram_output_dist:\n",
    "            # Use uniform distribution if tags not seen\n",
    "            if (bigram_output_dist[t, w], unigram_tag_dist[t]) == (0, 0):\n",
    "                self.emission_bigram[t, w] = self.symbol_uniform\n",
    "            self.emission_bigram[t, w] = self.div(bigram_output_dist[t, w], unigram_tag_dist[t])\n",
    "\n",
    "        for w in unigram_output_dist:\n",
    "            self.emission_unigram[w] = self.div(unigram_output_dist[w], self.text_size)\n",
    "        \n",
    "        self.transition_smoother = LISmoother(self.state_uniform, self.transition_unigram, \n",
    "                                              self.transition_bigram, self.transition)\n",
    "        self.emission_smoother = LISmoother(self.symbol_uniform, self.emission_unigram, \n",
    "                                            self.emission_bigram, self.emission)\n",
    "    \n",
    "    def smooth(self, heldout_data):\n",
    "        \"\"\"Smooth the transition and emission tables with linear interpolation smoothing\"\"\"\n",
    "        heldout_trigrams = [(tprev, t, w) for (tprev, _), (t, w) in  nltk.bigrams(heldout_data)]\n",
    "        self.transition_smoother.smooth(heldout_trigrams)\n",
    "        self.emission_smoother.smooth(heldout_trigrams)\n",
    "        \n",
    "    def tag(self, words):\n",
    "        T = len(words)\n",
    "        V = defaultdict(float)\n",
    "        B = {}\n",
    "\n",
    "        # Find the starting probabilities for each state\n",
    "        symbol = words[0]\n",
    "        for state in self.states:\n",
    "            V[0, state] = self.p_emission(state, symbol)\n",
    "            B[0, state] = None\n",
    "\n",
    "        # Find the maximum probabilities for reaching each state at time t\n",
    "        for t in range(1, T):\n",
    "            symbol = words[t]\n",
    "            for j in self.states:\n",
    "                sj = j\n",
    "                best = None\n",
    "                for i in self.states:\n",
    "                    si = i\n",
    "                    va = V[t - 1, i] * self.p_transition(sj, si)\n",
    "                    if not best or va > best[0]:\n",
    "                        best = (va, si)\n",
    "                V[t, j] = best[0] * self.p_emission(sj, symbol)\n",
    "                B[t, sj] = best[1]\n",
    "\n",
    "        # Find the highest probability for the final state\n",
    "        best = None\n",
    "        for i in self.states:\n",
    "            val = V[T - 1, i]\n",
    "            if not best or val > best[0]:\n",
    "                best = (val, i)\n",
    "\n",
    "        # traverse the back-pointers B to find the state sequence\n",
    "        current = best[1]\n",
    "        sequence = [current]\n",
    "        for t in range(T - 1, 0, -1):\n",
    "            last = B[t, current]\n",
    "            sequence.append(last)\n",
    "            current = last\n",
    "\n",
    "        sequence.reverse()\n",
    "        return sequence\n",
    "    \n",
    "    def evaluate(self, data):\n",
    "        total, correct = 0, 0\n",
    "        for sentence in tqdm(data):\n",
    "            words, tags = zip(*sentence)\n",
    "            predicted_tags = self.tag(words)\n",
    "            for tag, pred in zip(tags, predicted_tags):\n",
    "                if tag == pred:\n",
    "                    correct +=1\n",
    "                total += 1\n",
    "            \n",
    "        return correct / total\n",
    "    \n",
    "    def p_transition(self, tprev, t):\n",
    "        return self.transition_smoother.lambdas.dot([\n",
    "            self.state_uniform,\n",
    "            self.transition_unigram[t],\n",
    "            self.transition_bigram[tprev, t]\n",
    "        ])\n",
    "\n",
    "    def p_emission(self, t, w):\n",
    "        return self.emission_smoother.lambdas.dot([\n",
    "            self.symbol_uniform,\n",
    "            self.emission_unigram[w],\n",
    "            self.emission_bigram[t, w]\n",
    "        ])\n",
    "    \n",
    "#     def p_transition(self, tprev2, tprev, t):\n",
    "#         return self.transition_smoother.lambdas.dot([\n",
    "#             self.state_uniform,\n",
    "#             self.transition_unigram[t],\n",
    "#             self.transition_bigram[tprev, t],\n",
    "#             self.transition[tprev2, tprev]\n",
    "#         ])\n",
    "\n",
    "#     def p_emission(self, tprev, t, w):\n",
    "#         return self.emission_smoother.lambdas.dot([\n",
    "#             self.symbol_uniform,\n",
    "#             self.emission_unigram[w],\n",
    "#             self.emission_bigram[t, w],\n",
    "#             self.emission[tprev, t, w]\n",
    "#         ])\n",
    "    \n",
    "    def div(self, a, b):\n",
    "        \"\"\"Divides a and b safely\"\"\"\n",
    "        return a / b if b != 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, heldout, test = splits_en[0]\n",
    "\n",
    "words, tags = list(zip(*(train + heldout + test)))\n",
    "tag_set, word_set = list(set(tags)), list(set(words))\n",
    "# tag_set, word_set = list(set(nltk.bigrams(tags, pad_left=True))), list(set(words))\n",
    "\n",
    "# labeled = train[:10_000]\n",
    "labeled = train\n",
    "unlabeled = train[10_000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20712, 62)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_set), len(tag_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79616928, 238328)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Emissions, transitions\n",
    "len(tag_set) ** 2 * len(word_set), len(tag_set) ** 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = HMMTagger(labeled, tag_set, word_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambdas:\n",
      "[0.33333333 0.33333333 0.33333333]\n",
      "[0.27908848 0.71800998 0.00290154]\n",
      "[1.49433792e-01 8.50540016e-01 2.61923790e-05]\n",
      "[8.16056656e-02 9.18394051e-01 2.83275973e-07]\n",
      "[4.65155434e-02 9.53484453e-01 3.61744443e-09]\n",
      "[2.76186980e-02 9.72381302e-01 5.44067005e-11]\n",
      "[1.69518239e-02 9.83048176e-01 9.62574538e-13]\n",
      "[1.06746187e-02 9.89325381e-01 1.98667759e-14]\n",
      "[6.85316967e-03 9.93146830e-01 4.70175271e-16]\n",
      "[4.46351488e-03 9.95536485e-01 1.24700508e-17]\n",
      "[2.93783305e-03 9.97062167e-01 3.61769440e-19]\n",
      "[1.94829958e-03 9.98051700e-01 1.12292411e-20]\n",
      "[1.29898038e-03 9.98701020e-01 3.66162262e-22]\n",
      "[8.69296066e-04 9.99130704e-01 1.23662184e-23]\n",
      "[5.83245418e-04 9.99416755e-01 4.28025760e-25]\n",
      "[3.92014746e-04 9.99607985e-01 1.50690402e-26]\n",
      "[2.63801523e-04 9.99736198e-01 5.36745560e-28]\n",
      "[1.77667677e-04 9.99822332e-01 1.92712570e-29]\n",
      "Lambdas:\n",
      "[0.33333333 0.33333333 0.33333333]\n",
      "[0.79103479 0.06186889 0.14709632]\n",
      "[0.83597754 0.0138263  0.15019616]\n",
      "[0.84368966 0.0042711  0.15203924]\n",
      "[0.84581075 0.00178471 0.15240454]\n",
      "[0.84653216 0.00096946 0.15249838]\n",
      "[8.46842653e-01 6.28280678e-04 1.52529067e-01]\n",
      "[8.47004604e-01 4.53498062e-04 1.52541898e-01]\n",
      "[8.47101187e-01 3.50344082e-04 1.52548469e-01]\n",
      "[8.47164419e-01 2.83234354e-04 1.52552346e-01]\n"
     ]
    }
   ],
   "source": [
    "tagger.smooth(heldout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isplit(iterable, splitters):\n",
    "    # https://stackoverflow.com/a/4322780\n",
    "    return [list(g) for k,g in itertools.groupby(iterable, lambda x:x in splitters) if not k]\n",
    "\n",
    "def sentence_split(data, token=('###', '###')):\n",
    "    return [[token[0]] + g for g in isplit(data, (None, token))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sentence_split(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eb49bc5863544d7abb507dff554aa56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8087804878048781"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.evaluate(sentences[:100]) # TODO: prune states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
